{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Statistics\n",
    "\n",
    "Notes based off Casella and Berger edition 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Theory\n",
    "\n",
    "To Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations and Expectations\n",
    "\n",
    "### Transformation\n",
    "\n",
    "**Theorem 2.1.5:** Let $X$ have pdf $f_X(x)$ and let $Y = g(X)$ where $g$ is a monotone function. Let $X$ and $Y$ be defined by $\\mathbf{X} = \\{ x: f_X(x) > 0 \\}$ and $\\mathbf{Y} = \\{y: y = g(x)$ for some $x \\in \\mathbf{X} \\}$. Suppose that $f_X(x)$ is continuous on $\\mathbf{X}$ and that $g^{-1}(y)$ has a continuous derivative on $\\mathbf{Y}$. Then, the pdf of $Y$ is given by \n",
    "\n",
    "$$f_Y(y) = \\begin{cases}\n",
    "f_X(g^{-1}(y)) |\\frac{d}{dy} g^{-1}(y) | & y \\in \\mathbf{Y}\\\\\n",
    "0 & o.w.\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "**Procedure:**\n",
    "\n",
    "1. Check if $g$ is monotonic\n",
    "\n",
    "2. Split up into regions where monotonic and then evaluate formula above\n",
    "\n",
    "**See:** Example 2.1.6 (monotonic) and Example 2.1.7 (multiple regions)\n",
    "\n",
    "***\n",
    "\n",
    "**Theorem 2.1.10:** $$F^{-1}_X(y) = x \\Leftrightarrow F_X(x) = y$$\n",
    "\n",
    "**See:** Proof on pg. 54\n",
    "\n",
    "***\n",
    "\n",
    "### Expected values\n",
    "\n",
    "$$E g(X) = \\begin{cases}\n",
    "\\int_{-\\infty}^\\infty g(x) f_X(x) dx& \\hbox{ if $X$ is continuous}\\\\\n",
    "\\sum_{x \\in \\mathbf{X}} g(x) f_X(x) = \\sum_{x \\in \\mathbf{X}} g(x) P(X=x)& \\hbox{ if $X$ is discrete}\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "**See:** Example 2.2.2 for continuous and Example 2.2.3 for discrete\n",
    "\n",
    "**Properties:**\n",
    "\n",
    "a. $E(a g_1(X) + b g_2(X) + c) = a E g_1(X) + b E g_2(X) + c$.\n",
    "\n",
    "b. If $g_1(x) \\geq 0$ for all $x$ then $E g_1(X) \\geq 0$.\n",
    "\n",
    "c. If $g_1(x) \\geq g_2(x)$ for all $x$ then $E g_1(X) \\geq E g_2(X)$.\n",
    "\n",
    "d. If $a \\leq g_1(x) \\leq b$ for all $x$ then $a \\leq E g_1(X) \\leq b$.\n",
    "\n",
    "**Example:** Minimize distance\n",
    "\n",
    "$$\\begin{align*}\n",
    "& \\textcolor{red}{\\hbox{Add } \\pm E X}\\\\\n",
    "E(X-b)^2 &= E(X - E X  + E X + b^2)\\\\\n",
    " & \\textcolor{red}{\\hbox{Group terms}}\\\\\n",
    " &= E((X - EX) + (EX - b))^2\\\\\n",
    " &= E(X-EX)^2 + (EX - b)^2 + 2E((X-EX)(EX - b))\\\\\n",
    " & \\textcolor{red}{\\hbox{We know } E((X-EX)(EX - b)) = (EX-b)E(X-EX) = 0}\\\\\n",
    " & \\textcolor{red}{\\hbox{because (EX-b) is constant and } E(X-EX) = EX-EX=0}\\\\\n",
    " &= E(X-EX)^2 + (EX - b)^2\\\\\n",
    "\\min_b E(X-b)^2 &= E(X-EX)^2\\\\\n",
    " & \\textcolor{red}{\\hbox{If choosing } b=EX}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "This result happens to be the definition of variance.\n",
    "\n",
    "***\n",
    "\n",
    "### Moments\n",
    "\n",
    "The $n$th central moment of $X$ is \n",
    "\n",
    "$$\\mu_n = E(X-\\mu)^n$$\n",
    "\n",
    "where $\\mu = E X$. \n",
    "\n",
    "From this, we know the variance is \n",
    "\n",
    "$$Var X = E(X - EX)^2$$\n",
    "\n",
    "**See:** Example 2.3.3 for the variance of a parameter\n",
    "\n",
    "**Properties**:\n",
    "\n",
    "a. $Var(aX + b) = a^2 Var X$\n",
    "\n",
    "b. $Var X = E(X - EX)^2 = EX^2 - (EX)^2$\n",
    "\n",
    "***\n",
    "\n",
    "### Moment Generating Function (mgf)\n",
    "\n",
    "Let $X$ be a R.V. with cdf $F_X$. The mgf of $X$ (or $F_X$) is\n",
    "\n",
    "$$M_X(t) = E e^{tX}$$\n",
    "\n",
    "With our knowledge of expected values, \n",
    "\n",
    "$$M_X(t) = \\begin{cases}\n",
    "\\int_{-\\infty}^\\infty e^{tX} f_X(x) dx & \\hbox{if } X \\hbox{ is continuous}\\\\\n",
    "\\sum_{x} e^{tx} P(X=x) & \\hbox{if } X \\hbox{ is discrete}\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "**Theorem:** The $n$th moment is equal to the $n$th derivative of $M_X(t)$ evaluated at $t=0$.\n",
    "\n",
    "$$M_X^{(n)}(0) = \\frac{d^n}{dt^n} M_X(t) \\rvert_{t=0}$$\n",
    "\n",
    "Assuming we can differentiate under the integral sign (see Leibnitz Rule below), \n",
    "\n",
    "$$\\frac{d}{dt} M_X(t) = E X e^{tX}$$\n",
    "\n",
    "Evaluating this at $t=0$, we have\n",
    "\n",
    "$$\\frac{d^n}{dt^n} M_X(t)\\rvert_{t=0} = E X^N e^{tX} \\rvert_{t=0} = E X^n$$\n",
    "\n",
    "**See:** Example 2.3.8 for continuous case and Example 2.3.9 for discrete.\n",
    "\n",
    "**Properties:**\n",
    "\n",
    "a. $M_{aX + b}(t) = e^{bt} M_X(at)$\n",
    "\n",
    "***\n",
    "\n",
    "### Convergence of mgfs\n",
    "\n",
    "Suppose $\\{X_i, i = 1, 2, ... \\}$ is a sequence of RVs, each with mgf $M_{X_i}(t)$. Furthermore suppose that $$\\lim_{i\\xrightarrow{} \\infty} M_{X_i}(t) = M_X(t)$$ for all $t$ in a neighborhood of 0 and $M_X(t)$ is an mgf. Then, there is a unique cdf $F_X$ whose moments are determined by $M_X(t)$ and, for all $x$ where $F_X(x)$ is continuous we have $$\\lim_{t\\xrightarrow{}\\infty}F_{X_i}(x) = F_X(x)$$. That is, convergence, for $|t| < h$, of mgfs to an mgf implies convergence of cdfs.\n",
    "\n",
    "This relies on Laplace transforms, which defines\n",
    "\n",
    "$$M_X(t) = \\int_{-\\infty}^\\infty e^{tX} f_X(x) dx$$\n",
    "\n",
    "***\n",
    "\n",
    "**Proof:** Poisson approximation of Binomial\n",
    "\n",
    "We know that the poisson approximation is valid when $n$ is large and $np$ is small.\n",
    "\n",
    "Recall that the moment of binomial is $M_X(t) = [p e^t + (1 - p)]^n$.\n",
    "\n",
    "From the rule above (and just from txtbk), the MGF of poisson is $M_Y(t) = e^{\\lambda (e^t - 1)}$.\n",
    "\n",
    "If we define $p = \\lambda / n$ then $M_X(t) \\xrightarrow{} M_Y(t)$ as $n \\xrightarrow{} \\infty$.\n",
    "\n",
    "***\n",
    "\n",
    "**Lemma:** If $\\lim_{n\\xrightarrow{} \\infty} a_n = a$, then $$\\lim_{n\\xrightarrow{} \\infty} (1 + \\frac{a_n}{n})^n = e^a$$\n",
    "\n",
    "***\n",
    "\n",
    "### Leibnitz Rule\n",
    "\n",
    "If $f(x,\\theta)$, $a(\\theta)$, and $b(\\theta)$ are differentiable with respect to $\\theta$, then \n",
    "\n",
    "See page 69.\n",
    "\n",
    "If $a(\\theta)$ and $b(\\theta)$ are constant, then \n",
    "\n",
    "$$\\frac{d}{d\\theta} \\int_a^b f(x,\\theta) dx = \\int_a^b \\frac{\\delta}{\\delta \\theta} f(x,\\theta) dx$$\n",
    "\n",
    "### Lebesgue's Dominated Convergence Theorem\n",
    "\n",
    "See page 69 & 70. Basically, if the integral is not too badly behaved, then we can say it's good enough to bring a limit inside an integral.\n",
    "\n",
    "### Lipschitz Continuous\n",
    "\n",
    "Impose smoothness on a function by bounding its first derivative by a function with finite integral. It leads to interchangeability of integration and differentiation.\n",
    "\n",
    "**See:** Theorem 2.4.3 (pg 70), Corollary 2.4.4 and Examples 2.4.5 and 2.4.6\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Families of Distributions\n",
    "\n",
    "\n",
    "### Check if pdf part of exponential family\n",
    "\n",
    "$$f(x|\\mathbf{\\theta}) = h(x) c(\\mathbf{\\theta}) \\exp (\\sum_{i=1}^k w_i(\\mathbf{\\theta}) t_i(x))$$\n",
    "\n",
    "where $h(x) \\geq 0$, $t_i(x)$ are real-valued functions of $x$, $c(\\mathbf{\\theta}) \\geq 0$ and $w_i(\\mathbf{\\theta})$ are real-valued functions of the possibly vector-valued parameter $\\mathbf{\\theta}$ which is independent of $x$.\n",
    "\n",
    "Here are some common exponential families:\n",
    "\n",
    "a. Continuous: normal, gamma, beta\n",
    "\n",
    "b. Discrete: binomial, poisson, negative binomial\n",
    "\n",
    "A distribution which is a member of the exponential family has nice properties. For instance,\n",
    "\n",
    "### Expectations and Variance of exponential family pdf\n",
    "\n",
    "**Theorem:** If $X$ is a RV with pdf or pmf which is member of exponential family, \n",
    "\n",
    "$$E(\\sum_{i=1}^k \\frac{\\delta w_i(\\mathbf{\\theta})}{d \\theta_j} t_i(X)) = - \\frac{\\delta}{\\delta \\theta_j} \\log c(\\mathbf{\\theta})$$\n",
    "\n",
    "$$Var(\\sum_{i=1}^k \\frac{\\delta w_i(\\mathbf{\\theta})}{d \\theta_j} t_i(X)) = - \\frac{\\delta^2}{\\delta \\theta_j^2} \\log c(\\mathbf{\\theta}) - E(\\sum_{i=1}^k \\frac{\\delta^2 w_i(\\mathbf{\\theta})}{d \\theta^2_j} t_i(X))$$\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** The indicator function of a set $A$ \n",
    "\n",
    "$$I_A(x) = \\begin{cases}\n",
    "1 & x \\in A\\\\\n",
    "0 & x \\not\\in A\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "So, we can write the normal pdf (example 3.4.4) as \n",
    "\n",
    "$$f(x|\\mu, \\sigma^2) = h(x) c(\\mu, \\sigma) \\exp [w_1(\\mu, \\sigma) t_1(x) + w_2(\\mu, \\sigma) t_2(x)] I_{(-\\infty, \\infty)}(x)$$\n",
    "\n",
    "Since the indicator function is only a function of $x$, it can be incorporated into the function $h(x)$, showing that this pdf is of the exponential family form. \n",
    "\n",
    "Another example is of $f(x|\\theta) = \\theta^{-1} \\exp(1 - \\frac{x}{\\theta})$ on $0 < \\theta < x < \\infty$. Although this expression can fit the exponential family definition, the indicator function is dependent, $I_{[\\theta, \\infty)}(x)$.\n",
    "\n",
    "***\n",
    "\n",
    "### Chebychev's inequality\n",
    "\n",
    "Let $X$ be a RV and let $g(x)$ be a nonnegative function. Then, for any $r>0$,\n",
    "\n",
    "$$P(g(X) \\geq r) \\leq \\frac{E g(X)}{r}$$\n",
    "\n",
    "We usually set $r = t^2$.\n",
    "\n",
    "**See:** Example 3.6.2 and 3.6.3\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Random Variables\n",
    "\n",
    "Joint probability $f_{X,Y}(x,y)$\n",
    "\n",
    "**Definition:** Discrete\n",
    "\n",
    "Marginal probability is $f_X(x) = \\sum_{y \\in \\mathbf{R}} f_{X,Y}(x,y)$\n",
    "\n",
    "**Definition:** Continuous \n",
    "\n",
    "$$Eg(X,Y) = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty g(x,y) f(x,y) dx dy$$\n",
    "\n",
    "$$f_X(x) \\int_{-\\infty}^\\infty f(x,y)dy$$\n",
    "\n",
    "where $-\\infty <x < \\infty$\n",
    "\n",
    "$$\\frac{\\delta^2 F(x,y)}{\\delta x \\delta y} = f(x,y)$$\n",
    "\n",
    "**Definition:** Conditional\n",
    "\n",
    "$$f(y|x) = P(Y=y|X=x) = \\frac{f(x,y)}{f_X(x)}$$\n",
    "\n",
    "where $\\sum_y f(y|x) = 1$.\n",
    "\n",
    "$$E(g(Y) | x) = \\int_{-\\infty}^\\infty g(y) f(y|x) dy$$\n",
    "\n",
    "***\n",
    "\n",
    "### Independence properties\n",
    "\n",
    "$$f(x,y) = f_X(x) f_Y(y)$$\n",
    "\n",
    "$$f(y|x) = \\frac{f(x,y)}{f_X(x)} = \\frac{f_X(x) f_Y(y)}{f_X(x)} = f_Y(y)$$\n",
    "\n",
    "$$E(g(X)h(Y)) = (Eg(X))(Eh(Y))$$\n",
    "\n",
    "$$M_Z(t) = M_X(t) M_Y(t)$$\n",
    "\n",
    "With $U=g(X)$ and $V=h(Y)$ where $X$ and $Y$ are independent and $A_u = \\{ x: g(x) \\leq u\\}$ and $B_v = \\{ y: h(y) \\leq v\\}. Then, \n",
    "\n",
    "$$f_{U,V}(u, v) = \\frac{\\delta^2}{\\delta u \\delta v} F_{U,V}(u,v) = (\\frac{d}{du} P(X \\in A_u)) (\\frac{d}{dv} P(Y \\in B_v))$$\n",
    "\n",
    "***\n",
    "\n",
    "### Conditional expectation $EX = E(E(X|Y))$\n",
    "\n",
    "Rewritten, we say $E_X X = E_Y (E_{X|Y} (X|Y))$ because $E(X|Y)$ is a rv (random in $Y$), \n",
    "\n",
    "$$E(X|Y=y) = \\int x f_{X|Y}(x|Y=y) dx$$\n",
    "is a constant and $$E_Y E(X|Y=y) = \\int \\{ \\int x f_{X|Y}(x|y)dx \\} f_Y(y) dy$$\n",
    "\n",
    "**See:** Example 4.4.5\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Conditional variance identity\n",
    "\n",
    "For any two rv $X$ and $Y$,\n",
    "$$Var X = E(Var(X|Y)) + Var(E(X|Y))$$\n",
    "\n",
    "**See:** Example 4.4.8\n",
    "\n",
    "***\n",
    "\n",
    "### Covariance and correlation\n",
    "\n",
    "Covariance and correlation measure the strength of a relationship between two rv.\n",
    "\n",
    "Covariance of $X$ and $Y$ is $$Cov(X,Y) = E((X - \\mu_X)(Y - \\mu_Y)) = EXY - \\mu_X \\mu_Y$$\n",
    "\n",
    "This gives information regarding the relationship of $X$ and $Y$. Large positive values mean $X$ and $Y$ both go up together or down together. This value, however, struggles because, by itself, it is domain-specific. We can normalize by the variance to ensure the range of the metric... this is what correlation does.\n",
    "\n",
    "Correlation of $Y$ and $Y$ is $$\\rho_{XY} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}$$\n",
    "\n",
    "$\\rho_{XY}$ is also known as the *correlation coefficient*.\n",
    "\n",
    "If $X$ and $Y$ are independent, then $EXY = (EX)(EY)$ and therefore, \n",
    "\n",
    "$$Cov(X,Y) = EXY - (EX)(EY) = 0$$\n",
    "\n",
    "$$p_{XY} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y} = \\frac{0}{\\sigma_X \\sigma_Y} = 0$$\n",
    "\n",
    "**Note:** It is invalid to say because $Cov(X,Y)=0$, $X$ and $Y$ are independent. For example, if $X \\sim f(x-\\theta)$ and $Y$ is an indicator function $Y = I(|X-\\theta|<2)$, then $Y$ and $X$ are not independent but $E(XY)$ ends up equaling $EXEY$ so $Cov(X,Y) = 0$.\n",
    "\n",
    "**Properties:**\n",
    "\n",
    "For any rv $X$ and $Y$, \n",
    "\n",
    "a. $-1 \\leq \\rho_{XY} \\leq 1$\n",
    "\n",
    "b. $|\\rho_{XY}| = 1$ iff there exists $a \\neq 0$ and $b$ st $P(Y=aX+b) = 1$. If $\\rho_{XY} = 1$, then $a > 0$, and if $\\rho_{XY} = -1$, then $a<0$.\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Multivariate variance\n",
    "\n",
    "If $X$ and $Y$ are any two rv and $a$ and $b$ are any two constants, then\n",
    "\n",
    "$$Var(aX + bY) = a^2 Var X + b^2 Var Y + 2ab Cov(X,Y)$$\n",
    "\n",
    "**Note:** if $X$ and $Y$ are independent rv then \n",
    "\n",
    "$$Var(aX + bY) = a^2 Var X + b^2 Var Y$$\n",
    "\n",
    "***\n",
    "\n",
    "### Multivariate distributions\n",
    "\n",
    "With $\\mathbf{X} = (X_1, ..., X_n)$ representing a sample space that is a subset of $\\mathbf{R}^n$\n",
    "\n",
    "$$P(\\mathbf{X} \\in A) = \\int ... \\int_A f(\\mathbf{x}) d\\mathbf{x}$$\n",
    "\n",
    "and its expectation\n",
    "\n",
    "$$E g(\\mathbf{X}) = \\int_{-\\infty}^\\infty ... \\int_{-\\infty}^\\infty g(\\mathbf{x}) f(\\mathbf{x}) d\\mathbf{x}$$\n",
    "\n",
    "The marginal pdf of any subset of the coordinates of $(X_1, ..., X_n)$ can be computed by integrating the joint pdf over all possible values of the coordinates\n",
    "\n",
    "***\n",
    "\n",
    "### Multinomial distribution\n",
    "\n",
    "Let $n$ and $m$ be positive integers and let $p_1,..., p_n$ be numbers satisfying $0 \\leq p_i \\leq 1$, $i=1, ..., n$ and $\\sum_{i=1}^n p_i = 1$. Then the rv (X_1, ..., X_n)$ has a multinomial distribution with $m$ trials and cell probabilities $p_1, ..., p_n$ if the join pmf of $(X_1, ..., X_n)$ is \n",
    "\n",
    "$$f(x_1, ..., x_n) = \\frac{m!}{x_1! \\times ... \\times x_n!} p_1^{x_1} \\times ... \\times p_n^{x_n} = m! \\prod_{i=1}^n \\frac{p_i^{x_i}}{x_i!}$$\n",
    "\n",
    "on the set of $(x_1, ..., x_n)$ st $x_i$ is a nonnegative integer and $\\sum_{i=1}^n x_i = m$.\n",
    "\n",
    "This follows the following experiment: the experiment consists of $m$ independent trials. each trial results in one of $n$ distinct possible outcomes. The probability of $i$th outcome is $p_i$ on every trial. And, $X_i$ is the count of the number of times the $i$th outcome occurred in the $m$ trials. For $n=2$, this is just the binomial experiment in which each trial has $n=2$ possible oucomes and $X_i$ counts the number of \"successes\" and $X_2 = m - X_1$ counts the number of fials in $m$ tirlas. In a general multinomial experiment, there are $n$ possible outcomes to count.\n",
    "\n",
    "***\n",
    "\n",
    "### Multinomial properties (similar to univariate)\n",
    "\n",
    "$$Cov(X_i, X_j) = E[(X_i - p_i)(X_j - p_j)] = -m p_i p_j$$\n",
    "\n",
    "$$E(g_1(X_1) \\times ... \\times g_n(X_n)) = (g_1(X_1))\\times ... \\times (g_n(X_n))$$\n",
    "\n",
    "$$M_Z(t) = M_{X_1}(t) \\times ... \\times M_{X_n}(t)$$\n",
    "\n",
    "In particular, if $X_1, ..., X_n$ share the same distribution with mgf $M_X(t)$, then \n",
    "\n",
    "$$M_Z(t) = (M_X(t))^n$$\n",
    "\n",
    "***\n",
    "\n",
    "**Corollary:** Linear combination of independent distributions form the same (but multivariate) distribution\n",
    "\n",
    "Let $X_1, ..., X_n$ be mutually independent rv with mgfs $M_{X_1}(t), ..., M_{X_n}(t)$. Let $a_1, ..., a_n$ and $b_1, ..., b_n$ be fixed constants. Let $Z = (a_1X_1 +b_1) + ... + (a_n X_n + b_n)$. Then, the mgf of $Z$ is \n",
    "\n",
    "$$\\begin{align*}\n",
    "M_Z(t) &= E e^{tZ}\\\\\n",
    "&= Ee^{t\\sum (a_i X_i + b_i)}\\\\\n",
    "&= (e^{t(\\sum b_i)}) E(e^{t(\\sum a_i)} \\times ... \\times e^{t(\\sum a_n)})\\\\\n",
    "&= (e^{t(\\sum b_i)}) M_{X_1}(a_1 t) \\times ... \\times M_{X_n}(a_n t)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "From this, we can conclude (for instance) that a linear combination of independent (say) normal rv is normally distributed.\n",
    "\n",
    "$$Z = \\sum_{i=1}^n (a_i X_i + b_i) \\sim Normal(\\sum_{i=1}^n (a_i \\mu_i + b_i), \\sum_{i=1}^n a_i^2 \\sigma_i^2)$$\n",
    "\n",
    "***\n",
    "\n",
    "### Inequalities\n",
    "\n",
    "Per *Holder's Inequality*, if $\\frac{1}{p} + \\frac{1}{q} = 1$, then $|EXY| \\leq E|XY| \\leq (E|X|^p)^{1/p}(E|X|^q)^{1/q}$\n",
    "\n",
    "*Cauchy-Schwarz's* inequality is a special case of Holder's Inequality where $p=q=2$: \n",
    "\n",
    "$$|EXY| \\leq E|XY| \\leq (E|X|^2)^{1/2}(E|X|^2)^{1/2}$$\n",
    "\n",
    "The *covariance inequality* states that if $X$ and $Y$ have means $\\mu_X$ and $\\mu_Y$ and variances $\\sigma_X^2$ and $\\sigma_Y^2$, we can apply Cauch-Schwarz's Inequality to get \n",
    "\n",
    "$$E|(X-\\mu_X)(Y-\\mu_Y)| \\leq \\{ E(X-\\mu_X)^2 \\}^{1/2} \\{ E(Y-\\mu_Y)^2 \\}^{1/2}$$\n",
    "\n",
    "By squaring both sides, we get a useful property:\n",
    "\n",
    "$$(Cov(X,Y))^2 \\leq \\sigma_X^2 \\sigma_Y^2$$\n",
    "\n",
    "This can be modified (by setting $Y \\equiv 1$) to state $E|X| \\leq \\{ E|X|^p \\}^{1/p}$ (with $1<p<\\infty$).\n",
    "\n",
    "Additionally, *Liapounov's Inequality* takes this a step further by, for $1<r<p$, if we replace $|X|$ by $|X|^r$ we obtain\n",
    "\n",
    "$$E|X|^r \\leq \\{ E(|X|^{pr})^{1/p} \\}$$\n",
    "\n",
    "and then we set $s=pr$ (where $s>r$) and rearrange:\n",
    "\n",
    "$$\\{E|X|^r\\}^{1/r} \\leq \\{ E(|X|^{s})^{1/s} \\}$$\n",
    "\n",
    "Also, *Minkowski's Inequality* states that for two rvs $X$ and $Y$ and for $1 \\leq p < \\infty$, \n",
    "\n",
    "$$[E|X+Y|^p]^{1/p} \\leq [E|X|^p]^{1/p} + [E|Y|^p]^{1/p}$$\n",
    "\n",
    "This just uses the normal triangle inequality property.\n",
    "\n",
    "Lastly, *Jensen's Inequality* says that for a rv $X$, if $g(x)$ is a convex function, then $$Eg(X) \\geq g(EX)$$\n",
    "\n",
    "Note: This only holds true iff, for every line $a+bx$ that is tangent to $g(x)$ at $x=EX$, $P(g(X) = a+bX)=1$.\n",
    "\n",
    "***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Random Sample\n",
    "\n",
    "### Statistics \n",
    "\n",
    "**Definition:** A statistic $Y = T(X_1, ..., X_n)$ cannot be a function of a parameter of the distribution.\n",
    "\n",
    "The *sample mean* is $$\\bar{X} = \\frac{X_1 + ... + X_n}{n} = \\frac{1}{n} \\sum_{i=1}^n X_i$$\n",
    "\n",
    "The *sample variance* is $$S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2$$\n",
    "\n",
    "**Theorem:** With the definition defined like above, we know a few things:\n",
    "\n",
    "a. $\\min_a \\sum_{i=1}^n (x_i - a)^2 = \\sum_{i=1}^n (x_i - \\bar{x})^2$\n",
    "\n",
    "b. $(n-1)s^2 = \\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i=1}^n x_i^2 - n \\bar{x}^2$\n",
    "\n",
    "***\n",
    "\n",
    "**Lemma:** Let $X_1, ..., X_n$ be a random sample from a population and let $g(x)$ be a function such that $Eg(X_1)$ and $\\hbox{Var} g(X_1)$ exist. Then,\n",
    "\n",
    "$$E(\\sum_{i=1}^n g(X_i)) = n(E g(X_1))$$\n",
    "\n",
    "$$\\hbox{Var}(\\sum_{i=1}^n g(X_i)) = n(\\hbox{Var} g(X_1))$$\n",
    "\n",
    "***\n",
    "\n",
    "**Theorem:** With $X_1, ..., X_n$ as a random sample and population with mean $\\mu$ and variance $\\sigma^2<\\infty$,\n",
    "\n",
    "a. $E \\bar{X} = \\mu$\n",
    "\n",
    "b. $\\hbox{Var} \\bar{X} = \\frac{\\sigma^2}{n}$\n",
    "\n",
    "c. $E S^2 = \\sigma^2$\n",
    "\n",
    "In relationship (c), we see why $S^2$ requires a $\\frac{1}{n-1}$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "ES^2 &= E(\\frac{1}{n-1} [\\sum_{i=1}^n X_i^2 - n \\bar{X}^2])\\\\\n",
    "&= \\frac{1}{n-1} (n E X_1^2 - n E \\bar{X}^2)\\\\\n",
    "&= \\frac{1}{n-1} (n(\\sigma^2 + \\mu^2) - n(\\frac{\\sigma^2}{n} + \\mu^2))\\\\\n",
    "&= \\sigma^2\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "### Distribution of statistics\n",
    "\n",
    "**Example:** since $\\bar{X} = \\frac{1}{n}(X_1 + ... + X_n)$, if $f(y)$ is the pdf of $Y = (X_1 + ... + X_n)$, then $f_{\\bar{X}}(x) = nf(nx)$ is the pdf of $\\bar{X}$.\n",
    "We can prove this using the transformation equation from chapter 4.3.2:\n",
    "\n",
    "We say that $\\bar{X} = g(Y) = (1/n)Y$. Therefore, $g^{-1}(\\bar{X}) = n\\bar{X}$\n",
    "\n",
    "$f_{\\bar{X}}(x) = f_{Y}(n\\bar{X}) |n|$\n",
    "\n",
    "Additionally, this can be conducted for mgfs:\n",
    "\n",
    "$$M_{\\bar{X}}(t) = Ee^{t\\bar{X}} = Ee^{t(X_1 + ... + X_n)/n} = Ee^{(t/n)Y} = M_Y(t/n)$$\n",
    "\n",
    "Since $X_1, ..., X_n$ are iid, then this is true. \n",
    "\n",
    "**Theorem:** However, if they are just from a random sample from a population with mgf $M_X(t)$, then the mgf of the sample mean is:\n",
    "\n",
    "$$M_{\\bar{X}}(t) = [M_X(t/n)]^n$$\n",
    "\n",
    "This can be useful in cases where $M_{\\bar{X}}(t)$ is a familiar mgf, for instance:\n",
    "\n",
    "**Example:** Distribution of the mean\n",
    "\n",
    "Let $X_1, ..., X_n$ be a random sample from a $n(\\mu, \\sigma^2)$ population. Then, the mgf of the sample mean is\n",
    "\n",
    "$$\\begin{align*}\n",
    "M_{\\bar{X}}(t) &= [ \\exp(\\mu \\frac{t}{n} + \\frac{\\sigma^2(t/n)^2}{2}) ]^n\\\\\n",
    "&= \\exp(n(\\mu\\frac{t}{n} + \\frac{\\sigma^2(t/n)^2}{2}))\\\\\n",
    "&= \\exp(\\mu t + \\frac{(\\sigma^2/n)t^2}{2})\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "This also works for a random sample of $\\gamma(\\alpha,\\beta)$ (see Example 5.2.8)\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Check if pdf is member of exponential family\n",
    "\n",
    "Suppose $X_1, ..., X_n$ is a random sample from a pdf or pmf $f(x|\\theta)$, where\n",
    "\n",
    "$$f(x|\\theta) = h(x)c(\\theta) \\exp(\\sum_{i=1}^k w_i(\\theta) t_i(x))$$\n",
    "\n",
    "is a member of an exponential family. Define statistics $T_1, ..., T_k$ by\n",
    "\n",
    "$$T_i(X_1, ..., X_n) = \\sum_{j=1}^n t_i(X_j)$$\n",
    "\n",
    "where $i=1,...,k$.\n",
    "\n",
    "If the set $\\{(w_1(\\theta) w_2(\\theta), ..., w_k(\\theta)), \\theta \\in \\Theta\\}$ contains an open subset of $\\mathbf{R}^k$, then the distribution of $(T_1,...,T_k)$ is an exponential family of the form\n",
    "\n",
    "$$f_{T}(u_1,...,u_k|\\theta) = H(u_1,...,u_k)[c(\\theta)]^n \\exp(\\sum_{i=1}^k w_i(\\theta)u_i)$$\n",
    "\n",
    "The open set condition eliminates a density such as the $n(\\theta, \\theta^2)$ and, in general, eliminates curved exponential families.\n",
    "\n",
    "**Example:** Sum of bernuolli rvs\n",
    "\n",
    "Suppose $X_1, ..., X_n$ is a random sample from a $Bernuolli(p)$. We know that\n",
    "\n",
    "$$\\begin{align*}\n",
    "f(x|p) &= {n \\choose x} p^x (1-p)^{n-x}\\\\\n",
    "&= {n \\choose x} (1 - p)^n (\\frac{p}{1-p})^x\\\\\n",
    "&= {n \\choose x} (1 - p)^n \\exp(\\log(\\frac{p}{1-p})x)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "We can see that this is a exponential family where (if $n=1$, because $Bernuolli(p) \\sim Binomial(1,p)$)\n",
    "\n",
    "$$h(x) =\\begin{cases}\n",
    "{n \\choose x} & x=0,...,n\\\\\n",
    "0 & \\hbox{o.w.}\n",
    "\\end{cases}$$\n",
    "\n",
    "$c(p) = (1-p)^n, 0<p<1$\n",
    "\n",
    "$w_1(p) = \\log(\\frac{p}{1-p}), 0<p<1$\n",
    "\n",
    "$t_1(x) = x$\n",
    "\n",
    "Thus, from the previous theorem, $T_1(X_1,...,X_n) = X_1 + ... + X_n$. From the definition of a binomial distribution, we know that $T_1$ has a $binomial(n,p)$ distribution, which we have already shown is an exponential family. This verifies the theorem shown above.\n",
    "\n",
    "***\n",
    "\n",
    "**Properties:** of sample mean and variance\n",
    "\n",
    "a. $\\bar{X}$ and $S^2$ are independent random variables\n",
    "\n",
    "b. $\\bar{X}$ has a $n(\\mu, \\sigma^2/n)$ distribution\n",
    "\n",
    "c. $(n-1)S^2/\\sigma^2 \\sim \\chi_{n-1}^2$\n",
    "\n",
    "### Some distribution facts\n",
    "\n",
    "**Facts:** about $\\chi_p^2$ distribution with $p$ dof\n",
    "\n",
    "a. If $Z$ is a $n(0,1)$ rv, then $Z^2 \\sim \\chi_1^2$\n",
    "\n",
    "b. If $X_1, ..., X_n$ are independent and $X_i \\sim \\chi_{p_i}^2$ then $X_1 + ... X_n \\sim \\chi^2_{p_1 + ... p_n}$; so, independent chi-squared variables add to a chi-squared variable AND the dof also add.\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Student's t-distribution\n",
    "\n",
    "Instead of looking at the $n(\\mu, \\sigma^2)$,\n",
    "\n",
    "$$\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}}$$\n",
    "\n",
    "where we can use our knowledge of $\\sigma$ and our measurement of $\\bar{X}$ as a basis to determine $\\mu$,\n",
    "we can look at a distribution where $\\mu$ and $\\sigma$ are unknown:\n",
    "\n",
    "$$\\frac{\\bar{X} - \\mu}{S /\\sqrt{n}}$$\n",
    "\n",
    "**Properties:** of t-distribution\n",
    "\n",
    "a. Has no mgf becasue it does not have moments of all orders. If it has $p$ degrees of freedom, it only has $p-1$ moments. For instance, $t_1$ has no mean and $t_2$ has no variance.\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** F-distribution\n",
    "\n",
    "Built by a ratio of variances. See Definition 5.3.6\n",
    "\n",
    "This distribution has a few important corollaries:\n",
    "\n",
    "a. If $X \\sim F_{p,q}$, then $\\frac{1}{X} \\sim F_{q,p}$\n",
    "\n",
    "b. If $X \\sim t_q$, then $X^2 \\sim F_{1,q}$\n",
    "\n",
    "c. If $X \\sim F_{p,q}$, then $\\frac{p}{q} \\frac{X}{1 + (p/q)X} \\sim beta(p/2, q/2)$\n",
    "\n",
    "***\n",
    "\n",
    "### Order statistics\n",
    "\n",
    "Organize the random variables by size: $X_{(1)} \\leq ... \\leq X_{(n)}$\n",
    "\n",
    "We know the pdf of $X_{(j)}$ of a random sample $X_1, ..., X_n$ from a continuous population with cdf $F_X(x)$ and pdf $f_X(x)$ is\n",
    "\n",
    "$$f_{X_{(j)}}(x) = \\frac{n!}{(j-1)!(n-j)!} f_X(x) [F_X(x)]^{j-1} [1 - F_X(x)]^{n-j}$$\n",
    "\n",
    "and the joint pdf of $X_{(i)}$ and $X_{(j)}$, $1 \\leq i \\leq j \\leq n$ is \n",
    "\n",
    "$$f_{X_{(i)}, X_{(j)}}(u, v) = \\frac{n!}{(i-1)!(j-1-i)!(n-j)!} f_X(u) f_X(v) [F_X(u)]^{i-1} \\times [F_X(v) - F_X(u)]^{j-1-i} [1 - F_X(v)]^{n-j}$$\n",
    "\n",
    "***\n",
    "### Convergence\n",
    "\n",
    "**Definition:** Convergence in probability\n",
    "\n",
    "Weak law of large numbers (WLLN) says that $\\lim_{n\\xrightarrow[]{}\\infty}P(|\\bar{X}_n - \\mu| < \\epsilon) = 1$. That is, $\\bar{X}_n$ converges in probability to $\\mu$. So, \\textbf{convergence in probability} is $\\lim_{n\\xrightarrow[]{}\\infty} P(w \\in S |X_n(w) - X(w)| \\geq \\epsilon) = 0$ where $w$ is all the solutions in the set. I.e. for $\\Sigma X_i = 4$, the set could be $\\{(1,1,1,1), (2,2), \\hbox{etc.}\\}$\n",
    "\n",
    "**Definition:** Convergence almost surely\n",
    "\n",
    "A stronger definition of convergence (yet, it does not need to converge on a set with probability 0) says that a sequence of RVs \\textbf{converges almost surely} to a random variable X if $P(\\lim_{n\\xrightarrow[]{}\\infty}|X_n - X| < \\epsilon) = 1$. Moving the limit inside gives it a more strict definition.\n",
    "\n",
    "**Definition:** Convergence in distribution\n",
    "\n",
    "A sequence of RVs *converge in distribution* to a random variable X if $\\lim_{n\\xrightarrow[]{}\\infty}F_{X_n}(x) = F_X(x)$. The CDFs converge. \n",
    "\n",
    "***\n",
    "\n",
    "### Proving Tools\n",
    "\n",
    "\n",
    "**Definition:** Slutsky's Theorem\n",
    "\n",
    "If $X_n \\xrightarrow{} X$ in distribution and $Y_n \\xrightarrow{} a$, a constant, in probability, then \n",
    "\n",
    "a. $Y_n X_n \\xrightarrow{} a X$ in distribution\n",
    "\n",
    "b. $X_n + Y_n \\xrightarrow{} X + a$ in distribution\n",
    "\n",
    "**Definition:** Delta method\n",
    "\n",
    "Let $Y_n$ be a sequence of rvs that satisfies $\\sqrt{n} (Y_n - \\theta) \\xrightarrow{} n(0, \\sigma^2)$ in distribution. For a given function $g$ and a specific value of $\\theta$, suppose that $g^\\prime(\\theta)$ exists and is not 0. Then, $\\sqrt{n}[g(Y_n) - g(\\theta)] \\xrightarrow{} n(0, \\sigma^2[g^\\prime(\\theta)]^2)$ in distribution.\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principles of Data Reduction\n",
    "\n",
    "We are interested in methods of data reduction that do not discard important information about the unknown parameter $\\theta$ and methods that successfully discard information that is irrelevant as far as gaining knowledge about $\\theta$ is concerned.\n",
    "\n",
    "* *Sufficiency*: data reduction that does not discard information about $\\theta$ while achieving some summarization of the data\n",
    "\n",
    "* *Likelihood*: a function of the parameter, obtained by the observed sample, that contains all the information about $\\theta$ that is available from the sample\n",
    "\n",
    "* *Equivariance*: preserve important features of the model\n",
    "\n",
    "***\n",
    "\n",
    "### Sufficiency\n",
    "\n",
    "If $T(\\mathbf{X})$ is a sufficient statistic for $\\theta$ then any inference about $\\theta$ should depend on the sample $\\mathbf{X}$ only through the value $T(\\mathbf{X})$. That is, if $\\mathbf{x}$ and $\\mathbf{y}$ are two sample points such that $T(\\mathbf{x}) = T(\\mathbf{y})$ then the inference about $\\theta$ shoud be the ame whether $\\mathbf{X}=\\mathbf{x}$ or $\\mathbf{X}=\\mathbf{y}$ is observed.\n",
    "\n",
    "A statistic $T(\\mathbf{X})$ is a sufficient statistic for $\\theta$\n",
    "\n",
    "a. if the conditional distribution of the sample $\\mathbf{X}$ given the value of $T(\\mathbf{X})$ does not depend on $\\theta$\n",
    "\n",
    "b. if $p(\\mathbf{x}|\\theta)$ is the joint pdf/pmf of $\\mathbf{X}$ and $q(t|\\theta)$ is the pdf/pmf of $T(\\mathbf{X})$ and if, for every $\\mathbf{x}$ in the sample space, the ratio $p(\\mathbf{x}|\\theta)/q(T(\\mathbf{x})|\\theta)$ is constant as a function of $\\theta$ (aka, does not depend on $\\theta$).\n",
    "\n",
    "**Definition:** Determine if sufficient statistic\n",
    "\n",
    "**Factorization Theorem: (no prereq)**\n",
    "\n",
    "$$f(\\mathbf{x}|\\theta) = g(T(\\mathbf{x})|\\theta) h(\\mathbf{x})$$\n",
    "\n",
    "**For exponential family pdfs:**\n",
    "\n",
    "$$f(x|\\mathbf{\\theta}) = h(x) c(\\theta) \\exp(\\sum_{i=1}^k w_i(\\mathbf{\\theta}) t_i(x))$$\n",
    "\n",
    "where $\\mathbf{\\theta} = (\\theta_1, ..., \\theta_d), d \\leq k$. Then,\n",
    "\n",
    "$$T(\\mathbf{X}) = ( \\sum_{j=1}^n t_1(X_j), ..., \\sum_{j=1}^n t_k(X_j))$$\n",
    "\n",
    "is a sufficient statistic for $\\mathbf{\\theta}$.\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Minimal sufficient statistic\n",
    "\n",
    "**General: (no prereq)**\n",
    "\n",
    "A sufficient statistic $T(\\mathbf{X})$ is a minimal sufficient statistic if, for any other sufficient statistic $T^\\prime(\\mathbf{X})$, $T(\\mathbf{x})$ is a function $T^\\prime(\\mathbf{x})$.\n",
    "\n",
    "**General: (no prereq)**\n",
    "\n",
    "Let $f(\\mathbf{x}|\\theta)$ be the pmf or pdf of a sample $\\mathbf{X}$. Suppose there exists a function $T(\\mathbf{x})$ st, for every two sample points $\\mathbf{x}$ and $\\mathbf{y}$, the ratio $f(\\mathbf{x}|\\theta) / f(\\mathbf{y}|\\theta)$ is constant as a function of $\\theta$ iff $T(\\mathbf{x}) = T(\\mathbf{y})$. Then, $T(\\mathbf{X})$ is a minimal sufficient statistic for $\\theta$.\n",
    "\n",
    "Therefore, show $\\frac{f(\\textbf{x}|\\theta)}{f(\\textbf{y}|\\theta)} = \\frac{g^\\prime(T^\\prime(\\textbf{x})|\\theta)h^\\prime(\\textbf{x})}{g^\\prime(T^\\prime(\\textbf{y})|\\theta)h^\\prime(\\textbf{y})} = \\frac{h^\\prime(\\textbf{x})}{h^\\prime(\\textbf{y})}$ does not depend on $\\theta$. Therefore, $T(\\textbf{x}) = T(\\textbf{y})$. Thus, $T(\\textbf{x})$ is a function of $T^\\prime(\\textbf{x})$ and $T(\\textbf{x}) (6.2.13)$\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "#### Other properties\n",
    "\n",
    "**Definition:** Ancillary statistics\n",
    "\n",
    "A statistic $S(\\mathbf{X})$ whose distribution does not depend on the parameter $\\theta$ is called an ancillary statistic.\n",
    "\n",
    "Prove that the statistic does not depend on $\\theta$. Derive $f(T(X)|\\theta)$ and check whether $\\theta$ is in it. Using *Basu's theorem*, if $T(\\textbf{X})$ is a complete and minimal sufficient statistic, then $T(\\textbf{X})$ is independent of every ancillary statistic.\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Complete statistic\n",
    "\n",
    "Let $f(t|\\theta)$ be a family of pdfs or pmfs for a statistic $T(\\mathbf{X})$. The family of probability distributions is called *complete* if $E_\\theta g(T) = 0$ for all $\\theta$ implies $P_\\theta(g(T) = 0) = 1$ for all $\\theta$. Equivalently, $T(\\mathbf{X})$ is called a complete statistic.\n",
    "\n",
    "**General: (no prereq)**\n",
    "\n",
    "Basu's theorem \n",
    "\n",
    "**For exponential pdfs:**\n",
    "\n",
    "$$f(x|\\mathbf{\\theta}) = h(x) c(\\mathbf{\\theta}) \\exp(\\sum_{i=1}^k w(\\theta_j) t_j(x))$$\n",
    "\n",
    "where $\\mathbf{\\theta} = (\\theta_1, ..., \\theta_k). Then the statistic\n",
    "\n",
    "$$T(\\mathbf{X}) = ( \\sum_{i=1}^n t_1(X_i), ..., \\sum_{i=1}^n t_k(X_i))$$\n",
    "\n",
    "is complete as long as the parameter space $\\Theta$ contains an open set in $\\mathbf{R}^k$.\n",
    "\n",
    "***\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "Let $f(\\mathbf{x}|\\theta)$ denote the joint pdf/pmf of the sample $\\mathbf{X} = (X_1, ..., X_n)$. Then, given that $\\mathbf{X} = \\mathbf{x}$ is observed the function of $\\theta$ is defined by $$L(\\theta|\\mathbf{x}) = f(\\mathbf{x}|\\theta)$$ is called the likelihood function.\n",
    "\n",
    "**Definition:** Likelihood principle\n",
    "\n",
    "If $\\mathbf{x}$ and $\\mathbf{y}$ are two sample points st $L(\\theta|\\mathbf{x})$ is proportional to $L(\\theta|\\mathbf{y})$, that is, there exists a constant $C(\\mathbf{x},\\mathbf{y})$ st\n",
    "\n",
    "$$L(\\theta|\\mathbf{x}) = C(\\mathbf{x},\\mathbf{y}) L(\\theta|\\mathbf{y})$$\n",
    "\n",
    "for all $\\theta$, then the conclusions drawn from $\\mathbf{x}$ and $\\mathbf{y}$ should be identical.\n",
    "\n",
    "If $C(\\mathbf{x},\\mathbf{y}) = 1$, then the likelihood principle states that if two sample points result in the same likelihood function, then they contain the same information about $\\theta$. But, this can be taken further: the principle states that even if two sample points have only proportional likelihoods, then they contain equivalent information about $\\theta$. The plausibility can be observed by the proportion. For instance, if $L(\\theta_2|\\mathbf{x}) = 2L(\\theta_1|\\mathbf{x})$ then $\\theta_2$ is said to be twice as plausible as $\\theta_1$.\n",
    "\n",
    "The *fiducial inference* sometime interprets likelihoods as probabilities for $\\theta$. That is, $L(\\theta|\\mathbf{x})$ is multiplied by $M(\\mathbf{x}) = (\\int_{-\\infty}^\\infty L(\\theta|\\mathbf{x})d\\theta)^{-1}$ and then $M(\\mathbf{x})L(\\theta|\\mathbf{x})$ is interpreted as a pdf for $\\theta$ (if $M(\\mathbf{x})$ is finite).\n",
    "\n",
    "***\n",
    "\n",
    "### Equivariance\n",
    "\n",
    "If $\\mathbf{Y} = g(\\mathbf{X})$ is a change of measurement scale st the model for $\\mathbf{Y}$ has the same formal structure as the model for $\\mathbf{X}$, then an inference procedure should be both measurement equivariant and formally equivariant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Estimation\n",
    "\n",
    "A point estimator is any function $W(X_1, ..., X_n)$ of a sample; that is, any statistic is a point estimator.\n",
    "\n",
    "There exist three ways of finding estimators: 1) Method of Moments, 2) Maximum Likelihood Estimation, and 3) Bayes'\n",
    "\n",
    "### Method of moments (MOM)\n",
    "\n",
    "Let $X_1, ..., X_n$ be a sample from a population with pdf/pmf $f(x|\\theta_1,...,\\theta_k)$. MOM estimators are found by equating the first $k$ sample moments to the corresponding $k$ population moments, and solving the resulting system of simultaneous equations. \n",
    "\n",
    "$$\\begin{cases}\n",
    "m_1 = \\frac{1}{n} \\sum_{i=1}^n X_i^1, & \\mu_1^\\prime = EX^1\\\\\n",
    "m_2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2, & \\mu_2^\\prime = EX^2\\\\\n",
    "\\vdots\\\\\n",
    "m_k = \\frac{1}{n} \\sum_{i=1}^n X_i^k, & \\mu_k^\\prime = EX^k\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "### Maximum likelihood estimation (MLE)\n",
    "\n",
    "Get $L(\\theta | \\mathbf{x})$, then $\\ln L(\\theta | \\mathbf{x})$, then observe $\\frac{d}{d\\theta} \\ln L(\\theta | \\mathbf{x})$ domain and find the MLE. Check endpoints. \n",
    "\n",
    "## Bayes\n",
    "\n",
    "$\\hbox{posterior} = \\pi(\\theta|\\mathbf{x}) = f(\\mathbf{x}|\\theta)\\pi(\\theta) / m(\\mathbf{x})$\n",
    "\n",
    "where $f(\\mathbf{x}|\\theta)\\pi(\\theta) = f(\\mathbf{x},\\theta)$, the joint PDF and the marginal PDF $m(\\mathbf{x}) = \\int f(x|\\theta) \\pi(\\theta) d\\theta$. $\\pi(\\theta)$ is your prior distribution.\n",
    "\n",
    "**Definition:** Let $\\mathbf{F}$ denote the class of pdfs or pmfs $f(x|\\theta)$ (indexed by $\\theta$). A class $\\Pi$ of prior distributions is a conjugate family for $\\mathbf{F}$ if the posterior distribution is in the class $\\Pi$ for all $f \\in \\mathbf{F}$, all priors in $\\Pi$ and all $x \\in \\mathbf{X}$.\n",
    "\n",
    "For instance, the beta family is conjugate for the binomial family. Thus, if we start with a beta prior, we will end up with a beta posterior. \n",
    "\n",
    "***\n",
    "\n",
    "### Examples Finding Estimators\n",
    "\n",
    "**Example:** Normal distribution\n",
    "\n",
    "**MOM**\n",
    "\n",
    "If $X_1, ..., X_n$ are iid $n(\\theta,\\sigma^2)$, then $\\theta_1 = \\theta$ and $\\theta_2 = \\sigma^2$. We have $m_1 = \\bar{X}, m_2 = \\frac{1}{n} \\sum X_i^2, \\mu_1^\\prime = \\theta, \\mu_2^\\prime = \\theta^2 + \\sigma^2$, and hence we must solve\n",
    "\n",
    "\n",
    "$$\\bar{X} = \\theta$$\n",
    "\n",
    "$$\\frac{1}{n} \\sum X_i^2 = \\theta^2 + \\sigma^2$$\n",
    "\n",
    "Solving for $\\theta$ and $\\sigma^2$ yields the MOM estimators:\n",
    "\n",
    "$$\\vec{\\theta} = \\bar{X}$$\n",
    "\n",
    "$$\\vec{\\sigma}^2 = \\frac{1}{n} \\sum X_i^2 - \\bar{X}^2 = \\frac{1}{n} \\sum(X_i - \\bar{X})^2$$\n",
    "\n",
    "**Bayes**\n",
    "\n",
    "(Example 7.2.16) Let $X \\sim n(\\theta, \\sigma^2)$ and suppose that the prior distribution on $\\theta$ is $n(\\mu, \\tau^2)$. Here, we assume all the parameters are known. The posterior distribution of $\\theta$ is also normal with mean and variance given by\n",
    "\n",
    "$$\\pi(\\theta) = \\frac{1}{\\sqrt{2\\pi}\\tau} \\exp(-\\frac{1}{2\\tau^2} (\\theta - \\mu)^2)$$\n",
    "\n",
    "$$f(x|\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp(-\\frac{1}{2\\sigma^2} (x - \\theta)^2)$$\n",
    "\n",
    "Step 1: evaluate the posterior $\\pi(\\theta|\\vec{x}) = \\frac{f(x|\\theta) \\pi(\\theta)}{m(x)}$. Or, since $m(x)$ is not dependent on $\\theta$, we can just evaluate the joint distribution $f(x,\\theta) = f(x|\\theta) \\pi(\\theta)$.\n",
    "\n",
    "$$\\begin{align*}\n",
    "f(x,\\theta) &= [\\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp(-\\frac{1}{2\\sigma^2} (x - \\theta)^2)] [\\frac{1}{\\sqrt{2\\pi}\\tau} \\exp(-\\frac{1}{2\\tau^2} (\\theta - \\mu)^2)]\\\\\n",
    "&= \\frac{1}{2\\pi\\sigma\\tau} \\exp(-\\frac{1}{2\\sigma^2} (x - \\theta)^2 - \\frac{1}{2\\tau^2} (\\theta - \\mu)^2)\\\\\n",
    "&= \\frac{1}{2\\pi\\sigma\\tau} \\exp(-\\frac{1}{2\\sigma^2} (x^2 - 2x\\theta + \\theta^2) - \\frac{1}{2\\tau^2} (\\theta^2 - 2\\theta\\mu + \\mu^2))\\\\\n",
    "&= \\frac{1}{2\\pi\\sigma\\tau} \\exp(-\\frac{1}{2\\sigma^2} (x^2 - 2x\\theta + \\theta^2) - \\frac{1}{2\\tau^2} (\\theta^2 - 2\\theta\\mu + \\mu^2))\\\\\n",
    "&= \\frac{1}{2\\pi\\sigma\\tau} \\exp(-\\frac{x^2}{2\\sigma^2} + \\frac{2x\\theta}{2\\sigma^2} - \\frac{\\theta^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2} + \\frac{2\\theta\\mu}{2\\tau^2} - \\frac{\\mu^2}{2\\tau^2} )\\\\\n",
    "&= \\frac{1}{2\\pi\\sigma\\tau} \\exp(\\theta^2(- \\frac{1}{2\\sigma^2} - \\frac{1}{2\\tau^2}) + \\theta(\\frac{x}{\\sigma^2} + \\frac{\\mu}{\\tau^2}) - \\frac{\\mu^2}{2\\tau^2} -\\frac{x^2}{2\\sigma^2} )\\\\\n",
    "&= \\frac{1}{2\\pi\\sigma\\tau} \\exp(-\\theta^2(\\frac{1}{2\\sigma^2} + \\frac{1}{2\\tau^2})) \\times \\exp(\\theta(\\frac{x}{\\sigma^2} + \\frac{\\mu}{\\tau^2})) \\times \\exp(- \\frac{\\mu^2}{2\\tau^2} - \\frac{x^2}{2\\sigma^2} )\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Step 1b: Find $m(x) = \\int f(x|\\theta) \\pi(\\theta) d\\theta = \\int f(x,\\theta) d\\theta$.\n",
    "\n",
    "$$\\begin{align*}\n",
    "m(x) &= \\int f(x,\\theta) d\\theta\\\\\n",
    "&= \\frac{1}{2\\pi\\sigma\\tau} \\exp(-\\theta^2(\\frac{1}{2\\sigma^2} + \\frac{1}{2\\tau^2})) \\times \\exp(\\theta(\\frac{x}{\\sigma^2} + \\frac{\\mu}{\\tau^2})) \\times \\exp(- \\frac{\\mu^2}{2\\tau^2} - \\frac{x^2}{2\\sigma^2} )\\\\\n",
    "& \\textcolor{red}{\\hbox{This is a hard integral which is okay because we don't need it!}}\n",
    "\\end{align*}$$\n",
    "\n",
    "We can continue to boil down the joing $f(x,\\theta)$\n",
    "\n",
    "$$\\begin{align*}\n",
    "f(x,\\theta) &= \\frac{1}{2\\pi\\sigma\\tau} \\exp(\\theta^2(- \\frac{1}{2\\sigma^2} - \\frac{1}{2\\tau^2}) + \\theta(\\frac{x}{\\sigma^2} + \\frac{\\mu}{\\tau^2}) - \\frac{\\mu^2}{2\\tau^2} -\\frac{x^2}{2\\sigma^2} )\\\\\n",
    "&= ...\\\\\n",
    "&\\sim N(\\frac{\\tau^2 x + \\sigma^2 \\mu}{\\tau^2 + \\sigma^2}, \\frac{\\sigma^2 \\tau^2}{\\tau + \\sigma^2}) \\times N(\\mu, \\tau^2 + \\sigma^2)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Therefore, we can find the mean and variance\n",
    "\n",
    "$$E(\\theta|x) = \\frac{\\tau^2}{\\tau^2 + \\sigma^2} x + \\frac{\\sigma^2}{\\sigma^2 + \\tau^2} \\mu$$\n",
    "\n",
    "$$Var(\\theta|x) = \\frac{\\sigma^2 \\tau^2}{\\sigma^2 + \\tau^2}$$\n",
    "\n",
    "When have $tau^2$ near 0, the weight on $\\bar{x}$ is 0 and the weight on $\\mu$ is 1.\n",
    "\n",
    "**The normal family is its own conjugate.**\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "**Example:** Let $X_1, ..., X_n$ be iid $binomial(k,p)$, that is,\n",
    "\n",
    "$$P(X_i=x|k,p) = {k \\choose x} p^x (1-p)^{k-x}$$\n",
    "\n",
    "on $x = 0,1,...,k$.\n",
    "\n",
    "We desire point estimators for both $k$ and $p$. We start with the population yields:\n",
    "\n",
    "$$\\bar{X} = kp$$\n",
    "\n",
    "$$\\frac{1}{n}\\sum X_i^2 = kp(1-p) + k^2 p^2$$\n",
    "\n",
    "Or more simply, \n",
    "\n",
    "$$\\frac{1}{n} \\sum(X_i - \\bar{X})^2 = kp(1-p)$$\n",
    "\n",
    "With these definitions, we can build parameter estimates\n",
    "\n",
    "$$\\vec{p} = 1 - \\frac{\\frac{1}{n} \\sum(X_i - \\bar{X})^2}{\\bar{X}}$$\n",
    "\n",
    "$$\\vec{n} = \\frac{\\bar{X}}{\\vec{p}}$$\n",
    "\n",
    "***\n",
    "\n",
    "### Metrics for evaluating estimators\n",
    "\n",
    "**Definition:** Mean squared error (MSE)\n",
    "\n",
    "The MSE of an estimator $W$ of a parameter $\\theta$ is a function of $\\theta$ defined by $E_\\theta (W - \\theta)^2$.\n",
    "\n",
    "$$E_\\theta (W-\\theta)^2 = Var_\\theta W + (E_\\theta W - \\theta)^2 = Var_\\theta W + (Bias_\\theta W)^2$$\n",
    "\n",
    "This is usually a balancing act.\n",
    "\n",
    "Note: for an unbiased ($Bias_\\theta = 0$) estimator, we have\n",
    "\n",
    "$$E_\\theta(W-\\theta)^2 = Var_\\theta W$$\n",
    "\n",
    "**Example:** Normal MSE\n",
    "\n",
    "Let $X_1, ..., X_n$ be iid $n(\\mu, \\sigma^2)$. The statistics $\\bar{X}$ and $S^2$ are both unbiased estimators since\n",
    "\n",
    "$$E\\bar{X} = \\mu$$\n",
    "\n",
    "$$E S^2 = \\sigma^2$$\n",
    "\n",
    "for all $\\mu$ and $\\sigma^2$. *This is always true!!*\n",
    "\n",
    "This is true without the normality assumption. The MSE of these estimators are\n",
    "\n",
    "$$MSE_\\mu (\\bar{X}_n) = E(\\bar{X} - \\mu)^2 = Var \\bar{X} = \\frac{\\sigma^2}{n}$$\n",
    "\n",
    "This goes to 0 as $n \\xrightarrow{} \\infty$.\n",
    "\n",
    "$$MSE_{\\sigma^2}(S_n^2) = E(S^2 - \\sigma^2)^2 = Var S^2 = \\frac{2 \\sigma^4}{n-1}$$\n",
    "\n",
    "For a non-normal case, this is $MSE_{\\sigma^2}(S_n^2) = \\frac{1}{n}(\\theta_4 - \\frac{n-3}{n-1} \\theta_2^2)$\n",
    "\n",
    "**Example:** $MSE(\\hat{\\sigma}^2) < MSE(S^2)$\n",
    "\n",
    "We know the MLE of $\\hat{\\sigma^2} = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})^2 = \\frac{n-1}{n} \\sigma^2$.\n",
    "\n",
    "$$E \\hat{\\sigma^2} = E(\\frac{n-1}{n} S^2) = \\frac{n-1}{n} \\sigma^2$$\n",
    "\n",
    "So, $\\hat{\\sigma^2}$ is a biased estimator of $\\sigma^2$. The variance of $\\hat{\\sigma^2}$ can be calculated \n",
    "\n",
    "$$Var \\hat{\\sigma^2} = Var(\\frac{n-1}{n} S^2) = (\\frac{n-1}{n})^2 Var S^2 = \\frac{2(n-1)\\sigma^4}{n^2}$$\n",
    "\n",
    "because $Var S^2 = \\frac{1}{n}(\\theta_4 - \\frac{n-3}{n-1} \\theta_2^2)$, from above.\n",
    "\n",
    "Therefore, the $MSE$ is given by\n",
    "\n",
    "$$E(\\hat{\\sigma^2} - \\sigma^2)^2 = Var(\\frac{n-1}{n} S^2) = \\frac{2(n-1)\\sigma^4}{n^2} + (\\frac{n-1}{n} \\sigma^2 - \\sigma^2)^2 = (\\frac{2n-1}{n^2})\\sigma^4$$\n",
    "\n",
    "**Conclusions about MSE:**\n",
    "\n",
    "* Small inccrease in bias can be traded for a large decrease in variance resulting in a smaller MSE.\n",
    "\n",
    "* Because MSE is a function of the parameter, there is often not one *best* esetimator. Often, the MSEs of two estimators will cross each other, showing that each estimator is better (wrt the other) in only a portion of the parameter space.\n",
    "\n",
    "This second bullet point is why we discuss other tactics of finding the best estimator... see next!\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Best unbiased estimator\n",
    "\n",
    "We want to recommend a candidate estimator. Specifically, we consider unbiased estimators. So, if both $W_1$ and $W_2$ are unbiased estimators of a parameter $\\theta$, that is, $E_\\theta W_1 = E_\\theta W_2 = \\theta$, then their MSE are equal to their variances, so we should choose the estimator with the smaller variance. \n",
    "\n",
    "***\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c28e26739430500fec97d508cbac2e5d4a112deb445b412c4e69aa96f605479"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
