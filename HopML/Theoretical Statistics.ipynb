{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Statistics\n",
    "\n",
    "Notes based off Casella and Berger edition 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Theory\n",
    "\n",
    "To Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations and Expectations\n",
    "\n",
    "**Theorem 2.1.5:** Let $X$ have pdf $f_X(x)$ and let $Y = g(X)$ where $g$ is a monotone function. Let $X$ and $Y$ be defined by $\\mathbf{X} = \\{ x: f_X(x) > 0 \\}$ and $\\mathbf{Y} = \\{y: y = g(x)$ for some $x \\in \\mathbf{X} \\}$. Suppose that $f_X(x)$ is continuous on $\\mathbf{X}$ and that $g^{-1}(y)$ has a continuous derivative on $\\mathbf{Y}$. Then, the pdf of $Y$ is given by \n",
    "\n",
    "$$f_Y(y) = \\begin{cases}\n",
    "f_X(g^{-1}(y)) |\\frac{d}{dy} g^{-1}(y) | & y \\in \\mathbf{Y}\\\\\n",
    "0 & o.w.\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "**Procedure:**\n",
    "\n",
    "1. Check if $g$ is monotonic\n",
    "\n",
    "2. Split up into regions where monotonic and then evaluate formula above\n",
    "\n",
    "**See:** Example 2.1.6 (monotonic) and Example 2.1.7 (multiple regions)\n",
    "\n",
    "***\n",
    "\n",
    "**Theorem 2.1.10:** $$F^{-1}_X(y) = x \\Leftrightarrow F_X(x) = y$$\n",
    "\n",
    "**See:** Proof on pg. 54\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Expected values\n",
    "\n",
    "$$E g(X) = \\begin{cases}\n",
    "\\int_{-\\infty}^\\infty g(x) f_X(x) dx& \\hbox{ if $X$ is continuous}\\\\\n",
    "\\sum_{x \\in \\mathbf{X}} g(x) f_X(x) = \\sum_{x \\in \\mathbf{X}} g(x) P(X=x)& \\hbox{ if $X$ is discrete}\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "**See:** Example 2.2.2 for continuous and Example 2.2.3 for discrete\n",
    "\n",
    "**Properties:**\n",
    "\n",
    "a. $E(a g_1(X) + b g_2(X) + c) = a E g_1(X) + b E g_2(X) + c$.\n",
    "\n",
    "b. If $g_1(x) \\geq 0$ for all $x$ then $E g_1(X) \\geq 0$.\n",
    "\n",
    "c. If $g_1(x) \\geq g_2(x)$ for all $x$ then $E g_1(X) \\geq E g_2(X)$.\n",
    "\n",
    "d. If $a \\leq g_1(x) \\leq b$ for all $x$ then $a \\leq E g_1(X) \\leq b$.\n",
    "\n",
    "**Example:** Minimize distance\n",
    "\n",
    "$$\\begin{align*}\n",
    "& \\textcolor{red}{\\hbox{Add } \\pm E X}\\\\\n",
    "E(X-b)^2 &= E(X - E X  + E X + b^2)\\\\\n",
    " & \\textcolor{red}{\\hbox{Group terms}}\\\\\n",
    " &= E((X - EX) + (EX - b))^2\\\\\n",
    " &= E(X-EX)^2 + (EX - b)^2 + 2E((X-EX)(EX - b))\\\\\n",
    " & \\textcolor{red}{\\hbox{We know } E((X-EX)(EX - b)) = (EX-b)E(X-EX) = 0}\\\\\n",
    " & \\textcolor{red}{\\hbox{because (EX-b) is constant and } E(X-EX) = EX-EX=0}\\\\\n",
    " &= E(X-EX)^2 + (EX - b)^2\\\\\n",
    "\\min_b E(X-b)^2 &= E(X-EX)^2\\\\\n",
    " & \\textcolor{red}{\\hbox{If choosing } b=EX}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "This result happens to be the definition of variance.\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Moments\n",
    "\n",
    "The $n$th central moment of $X$ is \n",
    "\n",
    "$$\\mu_n = E(X-\\mu)^n$$\n",
    "\n",
    "where $\\mu = E X$. \n",
    "\n",
    "From this, we know the variance is \n",
    "\n",
    "$$Var X = E(X - EX)^2$$\n",
    "\n",
    "**See:** Example 2.3.3 for the variance of a parameter\n",
    "\n",
    "**Properties**:\n",
    "\n",
    "a. $Var(aX + b) = a^2 Var X$\n",
    "\n",
    "b. $Var X = E(X - EX)^2 = EX^2 - (EX)^2$\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Moment Generating Function (mgf)\n",
    "\n",
    "Let $X$ be a R.V. with cdf $F_X$. The mgf of $X$ (or $F_X$) is\n",
    "\n",
    "$$M_X(t) = E e^{tX}$$\n",
    "\n",
    "With our knowledge of expected values, \n",
    "\n",
    "$$M_X(t) = \\begin{cases}\n",
    "\\int_{-\\infty}^\\infty e^{tX} f_X(x) dx & \\hbox{if } X \\hbox{ is continuous}\\\\\n",
    "\\sum_{x} e^{tx} P(X=x) & \\hbox{if } X \\hbox{ is discrete}\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "**Theorem:** The $n$th moment is equal to the $n$th derivative of $M_X(t)$ evaluated at $t=0$.\n",
    "\n",
    "$$M_X^{(n)}(0) = \\frac{d^n}{dt^n} M_X(t) \\rvert_{t=0}$$\n",
    "\n",
    "Assuming we can differentiate under the integral sign (see Leibnitz Rule below), \n",
    "\n",
    "$$\\frac{d}{dt} M_X(t) = E X e^{tX}$$\n",
    "\n",
    "Evaluating this at $t=0$, we have\n",
    "\n",
    "$$\\frac{d^n}{dt^n} M_X(t)\\rvert_{t=0} = E X^N e^{tX} \\rvert_{t=0} = E X^n$$\n",
    "\n",
    "**See:** Example 2.3.8 for continuous case and Example 2.3.9 for discrete.\n",
    "\n",
    "**Properties:**\n",
    "\n",
    "a. $M_{aX + b}(t) = e^{bt} M_X(at)$\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Convergence of mgfs\n",
    "\n",
    "Suppose $\\{X_i, i = 1, 2, ... \\}$ is a sequence of RVs, each with mgf $M_{X_i}(t)$. Furthermore suppose that $$\\lim_{i\\xrightarrow{} \\infty} M_{X_i}(t) = M_X(t)$$ for all $t$ in a neighborhood of 0 and $M_X(t)$ is an mgf. Then, there is a unique cdf $F_X$ whose moments are determined by $M_X(t)$ and, for all $x$ where $F_X(x)$ is continuous we have $$\\lim_{t\\xrightarrow{}\\infty}F_{X_i}(x) = F_X(x)$$. That is, convergence, for $|t| < h$, of mgfs to an mgf implies convergence of cdfs.\n",
    "\n",
    "This relies on Laplace transforms, which defines\n",
    "\n",
    "$$M_X(t) = \\int_{-\\infty}^\\infty e^{tX} f_X(x) dx$$\n",
    "\n",
    "***\n",
    "\n",
    "**Proof:** Poisson approximation of Binomial\n",
    "\n",
    "We know that the poisson approximation is valid when $n$ is large and $np$ is small.\n",
    "\n",
    "Recall that the moment of binomial is $M_X(t) = [p e^t + (1 - p)]^n$.\n",
    "\n",
    "From the rule above (and just from txtbk), the MGF of poisson is $M_Y(t) = e^{\\lambda (e^t - 1)}$.\n",
    "\n",
    "If we define $p = \\lambda / n$ then $M_X(t) \\xrightarrow{} M_Y(t)$ as $n \\xrightarrow{} \\infty$.\n",
    "\n",
    "***\n",
    "\n",
    "**Lemma:** If $\\lim_{n\\xrightarrow{} \\infty} a_n = a$, then $$\\lim_{n\\xrightarrow{} \\infty} (1 + \\frac{a_n}{n})^n = e^a$$\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Leibnitz Rule\n",
    "\n",
    "If $f(x,\\theta)$, $a(\\theta)$, and $b(\\theta)$ are differentiable with respect to $\\theta$, then \n",
    "\n",
    "See page 69.\n",
    "\n",
    "If $a(\\theta)$ and $b(\\theta)$ are constant, then \n",
    "\n",
    "$$\\frac{d}{d\\theta} \\int_a^b f(x,\\theta) dx = \\int_a^b \\frac{\\delta}{\\delta \\theta} f(x,\\theta) dx$$\n",
    "\n",
    "**Definition:** Lebesgue's Dominated Convergence Theorem\n",
    "\n",
    "See page 69 & 70. Basically, if the integral is not too badly behaved, then we can say it's good enough to bring a limit inside an integral.\n",
    "\n",
    "**Definition:** Lipschitz Continuous\n",
    "\n",
    "Impose smoothness on a function by bounding its first derivative by a function with finite integral. It leads to interchangeability of integration and differentiation.\n",
    "\n",
    "**See:** Theorem 2.4.3 (pg 70), Corollary 2.4.4 and Examples 2.4.5 and 2.4.6\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Families of Distributions\n",
    "\n",
    "**Definition:** Check if pdf is part of exponential family\n",
    "\n",
    "$$f(x|\\mathbf{\\theta}) = h(x) c(\\mathbf{\\theta}) \\exp (\\sum_{i=1}^k w_i(\\mathbf{\\theta}) t_i(x))$$\n",
    "\n",
    "where $h(x) \\geq 0$, $t_i(x)$ are real-valued functions of $x$, $c(\\mathbf{\\theta}) \\geq 0$ and $w_i(\\mathbf{\\theta})$ are real-valued functions of the possibly vector-valued parameter $\\mathbf{\\theta}$ which is independent of $x$.\n",
    "\n",
    "Here are some common exponential families:\n",
    "\n",
    "a. Continuous: normal, gamma, beta\n",
    "\n",
    "b. Discrete: binomial, poisson, negative binomial\n",
    "\n",
    "A distribution which is a member of the exponential family has nice properties. For instance,\n",
    "\n",
    "**Theorem:** If $X$ is a RV with pdf or pmf which is member of exponential family, \n",
    "\n",
    "$$E(\\sum_{i=1}^k \\frac{\\delta w_i(\\mathbf{\\theta})}{d \\theta_j} t_i(X)) = - \\frac{\\delta}{\\delta \\theta_j} \\log c(\\mathbf{\\theta})$$\n",
    "\n",
    "$$Var(\\sum_{i=1}^k \\frac{\\delta w_i(\\mathbf{\\theta})}{d \\theta_j} t_i(X)) = - \\frac{\\delta^2}{\\delta \\theta_j^2} \\log c(\\mathbf{\\theta}) - E(\\sum_{i=1}^k \\frac{\\delta^2 w_i(\\mathbf{\\theta})}{d \\theta^2_j} t_i(X))$$\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** The indicator function of a set $A$ \n",
    "\n",
    "$$I_A(x) = \\begin{cases}\n",
    "1 & x \\in A\\\\\n",
    "0 & x \\not\\in A\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "So, we can write the normal pdf (example 3.4.4) as \n",
    "\n",
    "$$f(x|\\mu, \\sigma^2) = h(x) c(\\mu, \\sigma) \\exp [w_1(\\mu, \\sigma) t_1(x) + w_2(\\mu, \\sigma) t_2(x)] I_{(-\\infty, \\infty)}(x)$$\n",
    "\n",
    "Since the indicator function is only a function of $x$, it can be incorporated into the function $h(x)$, showing that this pdf is of the exponential family form. \n",
    "\n",
    "Another example is of $f(x|\\theta) = \\theta^{-1} \\exp(1 - \\frac{x}{\\theta})$ on $0 < \\theta < x < \\infty$. Although this expression can fit the exponential family definition, the indicator function is dependent, $I_{[\\theta, \\infty)}(x)$.\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Chebychev's inequality\n",
    "\n",
    "Let $X$ be a RV and let $g(x)$ be a nonnegative function. Then, for any $r>0$,\n",
    "\n",
    "$$P(g(X) \\geq r) \\leq \\frac{E g(X)}{r}$$\n",
    "\n",
    "We usually set $r = t^2$.\n",
    "\n",
    "**See:** Example 3.6.2 and 3.6.3\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Random Variables\n",
    "\n",
    "Joint probability $f_{X,Y}(x,y)$\n",
    "\n",
    "**Definition:** Discrete\n",
    "\n",
    "Marginal probability is $f_X(x) = \\sum_{y \\in \\mathbf{R}} f_{X,Y}(x,y)$\n",
    "\n",
    "**Definition:** Continuous \n",
    "\n",
    "$$Eg(X,Y) = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty g(x,y) f(x,y) dx dy$$\n",
    "\n",
    "$$f_X(x) \\int_{-\\infty}^\\infty f(x,y)dy$$\n",
    "\n",
    "where $-\\infty <x < \\infty$\n",
    "\n",
    "$$\\frac{\\delta^2 F(x,y)}{\\delta x \\delta y} = f(x,y)$$\n",
    "\n",
    "**Definition:** Conditional\n",
    "\n",
    "$$f(y|x) = P(Y=y|X=x) = \\frac{f(x,y)}{f_X(x)}$$\n",
    "\n",
    "where $\\sum_y f(y|x) = 1$.\n",
    "\n",
    "$$E(g(Y) | x) = \\int_{-\\infty}^\\infty g(y) f(y|x) dy$$\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Independence properties\n",
    "\n",
    "$$f(x,y) = f_X(x) f_Y(y)$$\n",
    "\n",
    "$$f(y|x) = \\frac{f(x,y)}{f_X(x)} = \\frac{f_X(x) f_Y(y)}{f_X(x)} = f_Y(y)$$\n",
    "\n",
    "$$E(g(X)h(Y)) = (Eg(X))(Eh(Y))$$\n",
    "\n",
    "$$M_Z(t) = M_X(t) M_Y(t)$$\n",
    "\n",
    "With $U=g(X)$ and $V=h(Y)$ where $X$ and $Y$ are independent and $A_u = \\{ x: g(x) \\leq u\\}$ and $B_v = \\{ y: h(y) \\leq v\\}. Then, \n",
    "\n",
    "$$f_{U,V}(u, v) = \\frac{\\delta^2}{\\delta u \\delta v} F_{U,V}(u,v) = (\\frac{d}{du} P(X \\in A_u)) (\\frac{d}{dv} P(Y \\in B_v))$$\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Conditional expectation $EX = E(E(X|Y))$\n",
    "\n",
    "Rewritten, we say $E_X X = E_Y (E_{X|Y} (X|Y))$ because $E(X|Y)$ is a rv (random in $Y$), \n",
    "\n",
    "$$E(X|Y=y) = \\int x f_{X|Y}(x|Y=y) dx$$\n",
    "is a constant and $$E_Y E(X|Y=y) = \\int \\{ \\int x f_{X|Y}(x|y)dx \\} f_Y(y) dy$$\n",
    "\n",
    "**See:** Example 4.4.5\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Conditional variance identity\n",
    "\n",
    "For any two rv $X$ and $Y$,\n",
    "$$Var X = E(Var(X|Y)) + Var(E(X|Y))$$\n",
    "\n",
    "**See:** Example 4.4.8\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Covariance and correlation\n",
    "\n",
    "Covariance and correlation measure the strength of a relationship between two rv.\n",
    "\n",
    "Covariance of $X$ and $Y$ is $$Cov(X,Y) = E((X - \\mu_X)(Y - \\mu_Y)) = EXY - \\mu_X \\mu_Y$$\n",
    "\n",
    "This gives information regarding the relationship of $X$ and $Y$. Large positive values mean $X$ and $Y$ both go up together or down together. This value, however, struggles because, by itself, it is domain-specific. We can normalize by the variance to ensure the range of the metric... this is what correlation does.\n",
    "\n",
    "Correlation of $Y$ and $Y$ is $$\\rho_{XY} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}$$\n",
    "\n",
    "$\\rho_{XY}$ is also known as the *correlation coefficient*.\n",
    "\n",
    "If $X$ and $Y$ are independent, then $EXY = (EX)(EY)$ and therefore, \n",
    "\n",
    "$$Cov(X,Y) = EXY - (EX)(EY) = 0$$\n",
    "\n",
    "$$p_{XY} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y} = \\frac{0}{\\sigma_X \\sigma_Y} = 0$$\n",
    "\n",
    "**Note:** It is invalid to say because $Cov(X,Y)=0$, $X$ and $Y$ are independent. For example, if $X \\sim f(x-\\theta)$ and $Y$ is an indicator function $Y = I(|X-\\theta|<2)$, then $Y$ and $X$ are not independent but $E(XY)$ ends up equaling $EXEY$ so $Cov(X,Y) = 0$.\n",
    "\n",
    "**Properties:**\n",
    "\n",
    "For any rv $X$ and $Y$, \n",
    "\n",
    "a. $-1 \\leq \\rho_{XY} \\leq 1$\n",
    "\n",
    "b. $|\\rho_{XY}| = 1$ iff there exists $a \\neq 0$ and $b$ st $P(Y=aX+b) = 1$. If $\\rho_{XY} = 1$, then $a > 0$, and if $\\rho_{XY} = -1$, then $a<0$.\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Multivariate variance\n",
    "\n",
    "If $X$ and $Y$ are any two rv and $a$ and $b$ are any two constants, then\n",
    "\n",
    "$$Var(aX + bY) = a^2 Var X + b^2 Var Y + 2ab Cov(X,Y)$$\n",
    "\n",
    "**Note:** if $X$ and $Y$ are independent rv then \n",
    "\n",
    "$$Var(aX + bY) = a^2 Var X + b^2 Var Y$$\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Multivariate distributions\n",
    "\n",
    "With $\\mathbf{X} = (X_1, ..., X_n)$ representing a sample space that is a subset of $\\mathbf{R}^n$\n",
    "\n",
    "$$P(\\mathbf{X} \\in A) = \\int ... \\int_A f(\\mathbf{x}) d\\mathbf{x}$$\n",
    "\n",
    "and its expectation\n",
    "\n",
    "$$E g(\\mathbf{X}) = \\int_{-\\infty}^\\infty ... \\int_{-\\infty}^\\infty g(\\mathbf{x}) f(\\mathbf{x}) d\\mathbf{x}$$\n",
    "\n",
    "The marginal pdf of any subset of the coordinates of $(X_1, ..., X_n)$ can be computed by integrating the joint pdf over all possible values of the coordinates\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Multinomial distribution\n",
    "\n",
    "Let $n$ and $m$ be positive integers and let $p_1,..., p_n$ be numbers satisfying $0 \\leq p_i \\leq 1$, $i=1, ..., n$ and $\\sum_{i=1}^n p_i = 1$. Then the rv (X_1, ..., X_n)$ has a multinomial distribution with $m$ trials and cell probabilities $p_1, ..., p_n$ if the join pmf of $(X_1, ..., X_n)$ is \n",
    "\n",
    "$$f(x_1, ..., x_n) = \\frac{m!}{x_1! \\times ... \\times x_n!} p_1^{x_1} \\times ... \\times p_n^{x_n} = m! \\prod_{i=1}^n \\frac{p_i^{x_i}}{x_i!}$$\n",
    "\n",
    "on the set of $(x_1, ..., x_n)$ st $x_i$ is a nonnegative integer and $\\sum_{i=1}^n x_i = m$.\n",
    "\n",
    "This follows the following experiment: the experiment consists of $m$ independent trials. each trial results in one of $n$ distinct possible outcomes. The probability of $i$th outcome is $p_i$ on every trial. And, $X_i$ is the count of the number of times the $i$th outcome occurred in the $m$ trials. For $n=2$, this is just the binomial experiment in which each trial has $n=2$ possible oucomes and $X_i$ counts the number of \"successes\" and $X_2 = m - X_1$ counts the number of fials in $m$ tirlas. In a general multinomial experiment, there are $n$ possible outcomes to count.\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Multinomial properties (similar to univariate)\n",
    "\n",
    "$$Cov(X_i, X_j) = E[(X_i - p_i)(X_j - p_j)] = -m p_i p_j$$\n",
    "\n",
    "$$E(g_1(X_1) \\times ... \\times g_n(X_n)) = (g_1(X_1))\\times ... \\times (g_n(X_n))$$\n",
    "\n",
    "$$M_Z(t) = M_{X_1}(t) \\times ... \\times M_{X_n}(t)$$\n",
    "\n",
    "In particular, if $X_1, ..., X_n$ share the same distribution with mgf $M_X(t)$, then \n",
    "\n",
    "$$M_Z(t) = (M_X(t))^n$$\n",
    "\n",
    "***\n",
    "\n",
    "**Corollary:** Linear combination of independent distributions form the same (but multivariate) distribution\n",
    "\n",
    "Let $X_1, ..., X_n$ be mutually independent rv with mgfs $M_{X_1}(t), ..., M_{X_n}(t)$. Let $a_1, ..., a_n$ and $b_1, ..., b_n$ be fixed constants. Let $Z = (a_1X_1 +b_1) + ... + (a_n X_n + b_n)$. Then, the mgf of $Z$ is \n",
    "\n",
    "$$\\begin{align*}\n",
    "M_Z(t) &= E e^{tZ}\\\\\n",
    "&= Ee^{t\\sum (a_i X_i + b_i)}\\\\\n",
    "&= (e^{t(\\sum b_i)}) E(e^{t(\\sum a_i)} \\times ... \\times e^{t(\\sum a_n)})\\\\\n",
    "&= (e^{t(\\sum b_i)}) M_{X_1}(a_1 t) \\times ... \\times M_{X_n}(a_n t)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "From this, we can conclude (for instance) that a linear combination of independent (say) normal rv is normally distributed.\n",
    "\n",
    "$$Z = \\sum_{i=1}^n (a_i X_i + b_i) \\sim Normal(\\sum_{i=1}^n (a_i \\mu_i + b_i), \\sum_{i=1}^n a_i^2 \\sigma_i^2)$$\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Inequalities\n",
    "\n",
    "Per *Holder's Inequality*, if $\\frac{1}{p} + \\frac{1}{q} = 1$, then $|EXY| \\leq E|XY| \\leq (E|X|^p)^{1/p}(E|X|^q)^{1/q}$\n",
    "\n",
    "*Cauchy-Schwarz's* inequality is a special case of Holder's Inequality where $p=q=2$: \n",
    "\n",
    "$$|EXY| \\leq E|XY| \\leq (E|X|^2)^{1/2}(E|X|^2)^{1/2}$$\n",
    "\n",
    "The *covariance inequality* states that if $X$ and $Y$ have means $\\mu_X$ and $\\mu_Y$ and variances $\\sigma_X^2$ and $\\sigma_Y^2$, we can apply Cauch-Schwarz's Inequality to get \n",
    "\n",
    "$$E|(X-\\mu_X)(Y-\\mu_Y)| \\leq \\{ E(X-\\mu_X)^2 \\}^{1/2} \\{ E(Y-\\mu_Y)^2 \\}^{1/2}$$\n",
    "\n",
    "By squaring both sides, we get a useful property:\n",
    "\n",
    "$$(Cov(X,Y))^2 \\leq \\sigma_X^2 \\sigma_Y^2$$\n",
    "\n",
    "This can be modified (by setting $Y \\equiv 1$) to state $E|X| \\leq \\{ E|X|^p \\}^{1/p}$ (with $1<p<\\infty$).\n",
    "\n",
    "Additionally, *Liapounov's Inequality* takes this a step further by, for $1<r<p$, if we replace $|X|$ by $|X|^r$ we obtain\n",
    "\n",
    "$$E|X|^r \\leq \\{ E(|X|^{pr})^{1/p} \\}$$\n",
    "\n",
    "and then we set $s=pr$ (where $s>r$) and rearrange:\n",
    "\n",
    "$$\\{E|X|^r\\}^{1/r} \\leq \\{ E(|X|^{s})^{1/s} \\}$$\n",
    "\n",
    "Also, *Minkowski's Inequality* states that for two rvs $X$ and $Y$ and for $1 \\leq p < \\infty$, \n",
    "\n",
    "$$[E|X+Y|^p]^{1/p} \\leq [E|X|^p]^{1/p} + [E|Y|^p]^{1/p}$$\n",
    "\n",
    "This just uses the normal triangle inequality property.\n",
    "\n",
    "Lastly, *Jensen's Inequality* says that for a rv $X$, if $g(x)$ is a convex function, then $$Eg(X) \\geq g(EX)$$\n",
    "\n",
    "Note: This only holds true iff, for every line $a+bx$ that is tangent to $g(x)$ at $x=EX$, $P(g(X) = a+bX)=1$.\n",
    "\n",
    "***\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Random Sample\n",
    "\n",
    "**Definition:** A statistic $Y = T(X_1, ..., X_n)$ cannot be a function of a parameter of the distribution.\n",
    "\n",
    "The *sample mean* is $$\\bar{X} = \\frac{X_1 + ... + X_n}{n} = \\frac{1}{n} \\sum_{i=1}^n X_i$$\n",
    "\n",
    "The *sample variance* is $$S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2$$\n",
    "\n",
    "**Theorem:** With the definition defined like above, we know a few things:\n",
    "\n",
    "a. $\\min_a \\sum_{i=1}^n (x_i - a)^2 = \\sum_{i=1}^n (x_i - \\bar{x})^2$\n",
    "\n",
    "b. $(n-1)s^2 = \\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i=1}^n x_i^2 - n \\bar{x}^2$\n",
    "\n",
    "***\n",
    "\n",
    "**Lemma:** Let $X_1, ..., X_n$ be a random sample from a population and let $g(x)$ be a function such that $Eg(X_1)$ and $\\hbox{Var} g(X_1)$ exist. Then,\n",
    "\n",
    "$$E(\\sum_{i=1}^n g(X_i)) = n(E g(X_1))$$\n",
    "\n",
    "$$\\hbox{Var}(\\sum_{i=1}^n g(X_i)) = n(\\hbox{Var} g(X_1))$$\n",
    "\n",
    "***\n",
    "\n",
    "**Theorem:** With $X_1, ..., X_n$ as a random sample and population with mean $\\mu$ and variance $\\sigma^2<\\infty$,\n",
    "\n",
    "a. $E \\bar{X} = \\mu$\n",
    "\n",
    "b. $\\hbox{Var} \\bar{X} = \\frac{\\sigma^2}{n}$\n",
    "\n",
    "c. $E S^2 = \\sigma^2$\n",
    "\n",
    "In relationship (c), we see why $S^2$ requires a $\\frac{1}{n-1}$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "ES^2 &= E(\\frac{1}{n-1} [\\sum_{i=1}^n X_i^2 - n \\bar{X}^2])\\\\\n",
    "&= \\frac{1}{n-1} (n E X_1^2 - n E \\bar{X}^2)\\\\\n",
    "&= \\frac{1}{n-1} (n(\\sigma^2 + \\mu^2) - n(\\frac{\\sigma^2}{n} + \\mu^2))\\\\\n",
    "&= \\sigma^2\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "**Example:** since $\\bar{X} = \\frac{1}{n}(X_1 + ... + X_n)$, if $f(y)$ is the pdf of $Y = (X_1 + ... + X_n)$, then $f_{\\bar{X}}(x) = nf(nx)$ is the pdf of $\\bar{X}$.\n",
    "We can prove this using the transformation equation from chapter 4.3.2:\n",
    "\n",
    "We say that $\\bar{X} = g(Y) = (1/n)Y$. Therefore, $g^{-1}(\\bar{X}) = n\\bar{X}$\n",
    "\n",
    "$f_{\\bar{X}}(x) = f_{Y}(n\\bar{X}) |n|$\n",
    "\n",
    "Additionally, this can be conducted for mgfs:\n",
    "\n",
    "$$M_{\\bar{X}}(t) = Ee^{t\\bar{X}} = Ee^{t(X_1 + ... + X_n)/n} = Ee^{(t/n)Y} = M_Y(t/n)$$\n",
    "\n",
    "Since $X_1, ..., X_n$ are iid, then this is true. \n",
    "\n",
    "**Theorem:** However, if they are just from a random sample from a population with mgf $M_X(t)$, then the mgf of the sample mean is:\n",
    "\n",
    "$$M_{\\bar{X}}(t) = [M_X(t/n)]^n$$\n",
    "\n",
    "This can be useful in cases where $M_{\\bar{X}}(t)$ is a familiar mgf, for instance:\n",
    "\n",
    "**Example:** Distribution of the mean\n",
    "\n",
    "Let $X_1, ..., X_n$ be a random sample from a $n(\\mu, \\sigma^2)$ population. Then, the mgf of the sample mean is\n",
    "\n",
    "$$\\begin{align*}\n",
    "M_{\\bar{X}}(t) &= [ \\exp(\\mu \\frac{t}{n} + \\frac{\\sigma^2(t/n)^2}{2}) ]^n\\\\\n",
    "&= \\exp(n(\\mu\\frac{t}{n} + \\frac{\\sigma^2(t/n)^2}{2}))\\\\\n",
    "&= \\exp(\\mu t + \\frac{(\\sigma^2/n)t^2}{2})\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "This also works for a random sample of $\\gamma(\\alpha,\\beta)$ (see Example 5.2.8)\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Check if pdf is member of exponential family\n",
    "\n",
    "Suppose $X_1, ..., X_n$ is a random sample from a pdf or pmf $f(x|\\theta)$, where\n",
    "\n",
    "$$f(x|\\theta) = h(x)c(\\theta) \\exp(\\sum_{i=1}^k w_i(\\theta) t_i(x))$$\n",
    "\n",
    "is a member of an exponential family. Define statistics $T_1, ..., T_k$ by\n",
    "\n",
    "$$T_i(X_1, ..., X_n) = \\sum_{j=1}^n t_i(X_j)$$\n",
    "\n",
    "where $i=1,...,k$.\n",
    "\n",
    "If the set $\\{(w_1(\\theta) w_2(\\theta), ..., w_k(\\theta)), \\theta \\in \\Theta\\}$ contains an open subset of $\\mathbf{R}^k$, then the distribution of $(T_1,...,T_k)$ is an exponential family of the form\n",
    "\n",
    "$$f_{T}(u_1,...,u_k|\\theta) = H(u_1,...,u_k)[c(\\theta)]^n \\exp(\\sum_{i=1}^k w_i(\\theta)u_i)$$\n",
    "\n",
    "The open set condition eliminates a density such as the $n(\\theta, \\theta^2)$ and, in general, eliminates curved exponential families.\n",
    "\n",
    "**Example:** Sum of bernuolli rvs\n",
    "\n",
    "Suppose $X_1, ..., X_n$ is a random sample from a $Bernuolli(p)$. We know that\n",
    "\n",
    "$$\\begin{align*}\n",
    "f(x|p) &= {n \\choose x} p^x (1-p)^{n-x}\\\\\n",
    "&= {n \\choose x} (1 - p)^n (\\frac{p}{1-p})^x\\\\\n",
    "&= {n \\choose x} (1 - p)^n \\exp(\\log(\\frac{p}{1-p})x)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "We can see that this is a exponential family where (if $n=1$, because $Bernuolli(p) \\sim Binomial(1,p)$)\n",
    "\n",
    "$$h(x) =\\begin{cases}\n",
    "{n \\choose x} & x=0,...,n\\\\\n",
    "0 & \\hbox{o.w.}\n",
    "\\end{cases}$$\n",
    "\n",
    "$c(p) = (1-p)^n, 0<p<1$\n",
    "\n",
    "$w_1(p) = \\log(\\frac{p}{1-p}), 0<p<1$\n",
    "\n",
    "$t_1(x) = x$\n",
    "\n",
    "Thus, from the previous theorem, $T_1(X_1,...,X_n) = X_1 + ... + X_n$. From the definition of a binomial distribution, we know that $T_1$ has a $binomial(n,p)$ distribution, which we have already shown is an exponential family. This verifies the theorem shown above.\n",
    "\n",
    "***\n",
    "\n",
    "**Properties:** of sample mean and variance\n",
    "\n",
    "a. $\\bar{X}$ and $S^2$ are independent random variables\n",
    "\n",
    "b. $\\bar{X}$ has a $n(\\mu, \\sigma^2/n)$ distribution\n",
    "\n",
    "c. $(n-1)S^2/\\sigma^2 \\sim \\chi_{n-1}^2$\n",
    "\n",
    "**Facts:** about $\\chi_p^2$ distribution with $p$ dof\n",
    "\n",
    "a. If $Z$ is a $n(0,1)$ rv, then $Z^2 \\sim \\chi_1^2$\n",
    "\n",
    "b. If $X_1, ..., X_n$ are independent and $X_i \\sim \\chi_{p_i}^2$ then $X_1 + ... X_n \\sim \\chi^2_{p_1 + ... p_n}$; so, independent chi-squared variables add to a chi-squared variable AND the dof also add.\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Student's t-distribution\n",
    "\n",
    "Instead of looking at the $n(\\mu, \\sigma^2)$,\n",
    "\n",
    "$$\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}}$$\n",
    "\n",
    "where we can use our knowledge of $\\sigma$ and our measurement of $\\bar{X}$ as a basis to determine $\\mu$,\n",
    "we can look at a distribution where $\\mu$ and $\\sigma$ are unknown:\n",
    "\n",
    "$$\\frac{\\bar{X} - \\mu}{S /\\sqrt{n}}$$\n",
    "\n",
    "**Properties:** of t-distribution\n",
    "\n",
    "a. Has no mgf becasue it does not have moments of all orders. If it has $p$ degrees of freedom, it only has $p-1$ moments. For instance, $t_1$ has no mean and $t_2$ has no variance.\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** F-distribution\n",
    "\n",
    "Built by a ratio of variances. See Definition 5.3.6\n",
    "\n",
    "This distribution has a few important corollaries:\n",
    "\n",
    "a. If $X \\sim F_{p,q}$, then $\\frac{1}{X} \\sim F_{q,p}$\n",
    "\n",
    "b. If $X \\sim t_q$, then $X^2 \\sim F_{1,q}$\n",
    "\n",
    "c. If $X \\sim F_{p,q}$, then $\\frac{p}{q} \\frac{X}{1 + (p/q)X} \\sim beta(p/2, q/2)$\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Order statistics\n",
    "\n",
    "Organize the random variables by size: $X_{(1)} \\leq ... \\leq X_{(n)}$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c28e26739430500fec97d508cbac2e5d4a112deb445b412c4e69aa96f605479"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
