{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Statistics\n",
    "\n",
    "Notes based off Casella and Berger edition 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Theory\n",
    "\n",
    "To Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations and Expectations\n",
    "\n",
    "**Theorem 2.1.5:** Let $X$ have pdf $f_X(x)$ and let $Y = g(X)$ where $g$ is a monotone function. Let $X$ and $Y$ be defined by $\\mathbf{X} = \\{ x: f_X(x) > 0 \\}$ and $\\mathbf{Y} = \\{y: y = g(x)$ for some $x \\in \\mathbf{X} \\}$. Suppose that $f_X(x)$ is continuous on $\\mathbf{X}$ and that $g^{-1}(y)$ has a continuous derivative on $\\mathbf{Y}$. Then, the pdf of $Y$ is given by \n",
    "\n",
    "$$f_Y(y) = \\begin{cases}\n",
    "f_X(g^{-1}(y)) |\\frac{d}{dy} g^{-1}(y) | & y \\in \\mathbf{Y}\\\\\n",
    "0 & o.w.\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "**Procedure:**\n",
    "\n",
    "1. Check if $g$ is monotonic\n",
    "\n",
    "2. Split up into regions where monotonic and then evaluate formula above\n",
    "\n",
    "**See:** Example 2.1.6 (monotonic) and Example 2.1.7 (multiple regions)\n",
    "\n",
    "***\n",
    "\n",
    "**Theorem 2.1.10:** $$F^{-1}_X(y) = x \\Leftrightarrow F_X(x) = y$$\n",
    "\n",
    "**See:** Proof on pg. 54\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Expected values\n",
    "\n",
    "$$E g(X) = \\begin{cases}\n",
    "\\int_{-\\infty}^\\infty g(x) f_X(x) dx& \\hbox{ if $X$ is continuous}\\\\\n",
    "\\sum_{x \\in \\mathbf{X}} g(x) f_X(x) = \\sum_{x \\in \\mathbf{X}} g(x) P(X=x)& \\hbox{ if $X$ is discrete}\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "**See:** Example 2.2.2 for continuous and Example 2.2.3 for discrete\n",
    "\n",
    "**Properties:**\n",
    "\n",
    "a. $E(a g_1(X) + b g_2(X) + c) = a E g_1(X) + b E g_2(X) + c$.\n",
    "\n",
    "b. If $g_1(x) \\geq 0$ for all $x$ then $E g_1(X) \\geq 0$.\n",
    "\n",
    "c. If $g_1(x) \\geq g_2(x)$ for all $x$ then $E g_1(X) \\geq E g_2(X)$.\n",
    "\n",
    "d. If $a \\leq g_1(x) \\leq b$ for all $x$ then $a \\leq E g_1(X) \\leq b$.\n",
    "\n",
    "**Example:** Minimize distance\n",
    "\n",
    "$$\\begin{align*}\n",
    "& \\textcolor{red}{\\hbox{Add } \\pm E X}\\\\\n",
    "E(X-b)^2 &= E(X - E X  + E X + b^2)\\\\\n",
    " & \\textcolor{red}{\\hbox{Group terms}}\\\\\n",
    " &= E((X - EX) + (EX - b))^2\\\\\n",
    " &= E(X-EX)^2 + (EX - b)^2 + 2E((X-EX)(EX - b))\\\\\n",
    " & \\textcolor{red}{\\hbox{We know } E((X-EX)(EX - b)) = (EX-b)E(X-EX) = 0}\\\\\n",
    " & \\textcolor{red}{\\hbox{because (EX-b) is constant and } E(X-EX) = EX-EX=0}\\\\\n",
    " &= E(X-EX)^2 + (EX - b)^2\\\\\n",
    "\\min_b E(X-b)^2 &= E(X-EX)^2\\\\\n",
    " & \\textcolor{red}{\\hbox{If choosing } b=EX}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "This result happens to be the definition of variance.\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Moments\n",
    "\n",
    "The $n$th central moment of $X$ is \n",
    "\n",
    "$$\\mu_n = E(X-\\mu)^n$$\n",
    "\n",
    "where $\\mu = E X$. \n",
    "\n",
    "From this, we know the variance is \n",
    "\n",
    "$$Var X = E(X - EX)^2$$\n",
    "\n",
    "**See:** Example 2.3.3 for the variance of a parameter\n",
    "\n",
    "**Properties**:\n",
    "\n",
    "a. $Var(aX + b) = a^2 Var X$\n",
    "\n",
    "b. $Var X = E(X - EX)^2 = EX^2 - (EX)^2$\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Moment Generating Function (mgf)\n",
    "\n",
    "Let $X$ be a R.V. with cdf $F_X$. The mgf of $X$ (or $F_X$) is\n",
    "\n",
    "$$M_X(t) = E e^{tX}$$\n",
    "\n",
    "With our knowledge of expected values, \n",
    "\n",
    "$$M_X(t) = \\begin{cases}\n",
    "\\int_{-\\infty}^\\infty e^{tX} f_X(x) dx & \\hbox{if } X \\hbox{ is continuous}\\\\\n",
    "\\sum_{x} e^{tx} P(X=x) & \\hbox{if } X \\hbox{ is discrete}\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "**Theorem:** The $n$th moment is equal to the $n$th derivative of $M_X(t)$ evaluated at $t=0$.\n",
    "\n",
    "$$M_X^{(n)}(0) = \\frac{d^n}{dt^n} M_X(t) \\rvert_{t=0}$$\n",
    "\n",
    "Assuming we can differentiate under the integral sign (see Leibnitz Rule below), \n",
    "\n",
    "$$\\frac{d}{dt} M_X(t) = E X e^{tX}$$\n",
    "\n",
    "Evaluating this at $t=0$, we have\n",
    "\n",
    "$$\\frac{d^n}{dt^n} M_X(t)\\rvert_{t=0} = E X^N e^{tX} \\rvert_{t=0} = E X^n$$\n",
    "\n",
    "**See:** Example 2.3.8 for continuous case and Example 2.3.9 for discrete.\n",
    "\n",
    "**Properties:**\n",
    "\n",
    "a. $M_{aX + b}(t) = e^{bt} M_X(at)$\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Convergence of mgfs\n",
    "\n",
    "Suppose $\\{X_i, i = 1, 2, ... \\}$ is a sequence of RVs, each with mgf $M_{X_i}(t)$. Furthermore suppose that $$\\lim_{i\\xrightarrow{} \\infty} M_{X_i}(t) = M_X(t)$$ for all $t$ in a neighborhood of 0 and $M_X(t)$ is an mgf. Then, there is a unique cdf $F_X$ whose moments are determined by $M_X(t)$ and, for all $x$ where $F_X(x)$ is continuous we have $$\\lim_{t\\xrightarrow{}\\infty}F_{X_i}(x) = F_X(x)$$. That is, convergence, for $|t| < h$, of mgfs to an mgf implies convergence of cdfs.\n",
    "\n",
    "This relies on Laplace transforms, which defines\n",
    "\n",
    "$$M_X(t) = \\int_{-\\infty}^\\infty e^{tX} f_X(x) dx$$\n",
    "\n",
    "***\n",
    "\n",
    "**Proof:** Poisson approximation of Binomial\n",
    "\n",
    "We know that the poisson approximation is valid when $n$ is large and $np$ is small.\n",
    "\n",
    "Recall that the moment of binomial is $M_X(t) = [p e^t + (1 - p)]^n$.\n",
    "\n",
    "From the rule above (and just from txtbk), the MGF of poisson is $M_Y(t) = e^{\\lambda (e^t - 1)}$.\n",
    "\n",
    "If we define $p = \\lambda / n$ then $M_X(t) \\xrightarrow{} M_Y(t)$ as $n \\xrightarrow{} \\infty$.\n",
    "\n",
    "***\n",
    "\n",
    "**Lemma:** If $\\lim_{n\\xrightarrow{} \\infty} a_n = a$, then $$\\lim_{n\\xrightarrow{} \\infty} (1 + \\frac{a_n}{n})^n = e^a$$\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Leibnitz Rule\n",
    "\n",
    "If $f(x,\\theta)$, $a(\\theta)$, and $b(\\theta)$ are differentiable with respect to $\\theta$, then \n",
    "\n",
    "See page 69.\n",
    "\n",
    "If $a(\\theta)$ and $b(\\theta)$ are constant, then \n",
    "\n",
    "$$\\frac{d}{d\\theta} \\int_a^b f(x,\\theta) dx = \\int_a^b \\frac{\\delta}{\\delta \\theta} f(x,\\theta) dx$$\n",
    "\n",
    "**Definition:** Lebesgue's Dominated Convergence Theorem\n",
    "\n",
    "See page 69 & 70. Basically, if the integral is not too badly behaved, then we can say it's good enough to bring a limit inside an integral.\n",
    "\n",
    "**Definition:** Lipschitz Continuous\n",
    "\n",
    "Impose smoothness on a function by bounding its first derivative by a function with finite integral. It leads to interchangeability of integration and differentiation.\n",
    "\n",
    "**See:** Theorem 2.4.3 (pg 70), Corollary 2.4.4 and Examples 2.4.5 and 2.4.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Families of Distributions\n",
    "\n",
    "**Definition:** Exponential families\n",
    "\n",
    "$$f(x|\\mathbf{\\theta}) = h(x) c(\\mathbf{\\theta}) \\exp (\\sum_{i=1}^k w_i(\\mathbf{\\theta}) t_i(x))$$\n",
    "\n",
    "where $h(x) \\geq 0$, $t_i(x)$ are real-valued functions of $x$, $c(\\mathbf{\\theta}) \\geq 0$ and $w_i(\\mathbf{\\theta})$ are real-valued functions of the possibly vector-valued parameter $\\mathbf{\\theta}$ which is independent of $x$.\n",
    "\n",
    "Here are some common exponential families:\n",
    "\n",
    "a. Continuous: normal, gamma, beta\n",
    "\n",
    "b. Discrete: binomial, poisson, negative binomial\n",
    "\n",
    "A distribution which is a member of the exponential family has nice properties. For instance,\n",
    "\n",
    "**Theorem:** If $X$ is a RV with pdf or pmf which is member of exponential family, \n",
    "\n",
    "$$E(\\sum_{i=1}^k \\frac{\\delta w_i(\\mathbf{\\theta})}{d \\theta_j} t_i(X)) = - \\frac{\\delta}{\\delta \\theta_j} \\log c(\\mathbf{\\theta})$$\n",
    "\n",
    "$$Var(\\sum_{i=1}^k \\frac{\\delta w_i(\\mathbf{\\theta})}{d \\theta_j} t_i(X)) = - \\frac{\\delta^2}{\\delta \\theta_j^2} \\log c(\\mathbf{\\theta}) - E(\\sum_{i=1}^k \\frac{\\delta^2 w_i(\\mathbf{\\theta})}{d \\theta^2_j} t_i(X))$$\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** The indicator function of a set $A$ \n",
    "\n",
    "$$I_A(x) = \\begin{cases}\n",
    "1 & x \\in A\\\\\n",
    "0 & x \\not\\in A\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "So, we can write the normal pdf (example 3.4.4) as \n",
    "\n",
    "$$f(x|\\mu, \\sigma^2) = h(x) c(\\mu, \\sigma) \\exp [w_1(\\mu, \\sigma) t_1(x) + w_2(\\mu, \\sigma) t_2(x)] I_{(-\\infty, \\infty)}(x)$$\n",
    "\n",
    "Since the indicator function is only a function of $x$, it can be incorporated into the function $h(x)$, showing that this pdf is of the exponential family form. \n",
    "\n",
    "Another example is of $f(x|\\theta) = \\theta^{-1} \\exp(1 - \\frac{x}{\\theta})$ on $0 < \\theta < x < \\infty$. Although this expression can fit the exponential family definition, the indicator function is dependent, $I_{[\\theta, \\infty)}(x)$.\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Chebychev's inequality\n",
    "\n",
    "Let $X$ be a RV and let $g(x)$ be a nonnegative function. Then, for any $r>0$,\n",
    "\n",
    "$$P(g(X) \\geq r) \\leq \\frac{E g(X)}{r}$$\n",
    "\n",
    "We usually set $r = t^2$.\n",
    "\n",
    "**See:** Example 3.6.2 and 3.6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Random Variables\n",
    "\n",
    "Joint probability $f_{X,Y}(x,y)$\n",
    "\n",
    "**Definition:** Discrete\n",
    "\n",
    "Marginal probability is $f_X(x) = \\sum_{y \\in \\mathbf{R}} f_{X,Y}(x,y)$\n",
    "\n",
    "**Definition:** Continuous \n",
    "\n",
    "$$Eg(X,Y) = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty g(x,y) f(x,y) dx dy$$\n",
    "\n",
    "$$f_X(x) \\int_{-\\infty}^\\infty f(x,y)dy$$\n",
    "\n",
    "where $-\\infty <x < \\infty$\n",
    "\n",
    "$$\\frac{\\delta^2 F(x,y)}{\\delta x \\delta y} = f(x,y)$$\n",
    "\n",
    "**Definition:** Conditional\n",
    "\n",
    "$$f(y|x) = P(Y=y|X=x) = \\frac{f(x,y)}{f_X(x)}$$\n",
    "\n",
    "where $\\sum_y f(y|x) = 1$.\n",
    "\n",
    "$$E(g(Y) | x) = \\int_{-\\infty}^\\infty g(y) f(y|x) dy$$\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Independence properties\n",
    "\n",
    "$$f(x,y) = f_X(x) f_Y(y)$$\n",
    "\n",
    "$$f(y|x) = \\frac{f(x,y)}{f_X(x)} = \\frac{f_X(x) f_Y(y)}{f_X(x)} = f_Y(y)$$\n",
    "\n",
    "$$E(g(X)h(Y)) = (Eg(X))(Eh(Y))$$\n",
    "\n",
    "$$M_Z(t) = M_X(t) M_Y(t)$$\n",
    "\n",
    "With $U=g(X)$ and $V=h(Y)$ where $X$ and $Y$ are independent and $A_u = \\{ x: g(x) \\leq u\\}$ and $B_v = \\{ y: h(y) \\leq v\\}. Then, \n",
    "\n",
    "$$f_{U,V}(u, v) = \\frac{\\delta^2}{\\delta u \\delta v} F_{U,V}(u,v) = (\\frac{d}{du} P(X \\in A_u)) (\\frac{d}{dv} P(Y \\in B_v))$$\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Conditional expectation $EX = E(E(X|Y))$\n",
    "\n",
    "Rewritten, we say $E_X X = E_Y (E_{X|Y} (X|Y))$ because $E(X|Y)$ is a rv (random in $Y$), \n",
    "\n",
    "$$E(X|Y=y) = \\int x f_{X|Y}(x|Y=y) dx$$\n",
    "is a constant and $$E_Y E(X|Y=y) = \\int \\{ \\int x f_{X|Y}(x|y)dx \\} f_Y(y) dy$$\n",
    "\n",
    "**See:** Example 4.4.5\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Conditional variance identity\n",
    "\n",
    "For any two rv $X$ and $Y$,\n",
    "$$Var X = E(Var(X|Y)) + Var(E(X|Y))$$\n",
    "\n",
    "**See:** Example 4.4.8\n",
    "\n",
    "***\n",
    "\n",
    "**Definition:** Covariance and correlation\n",
    "\n",
    "Covariance and correlation measure the strength of a relationship between two rv.\n",
    "\n",
    "Covariance of $X$ and $Y$ is $$Cov(X,Y) = E((X - \\mu_X)(Y - \\mu_Y))$$\n",
    "\n",
    "This gives information regarding the relationship of $X$ and $Y$. Large positive values mean $X$ and $Y$ both go up together or down together. This value, however, struggles because, by itself, it is domain-specific. We can normalize by the variance to ensure the range of the metric... this is what correlation does.\n",
    "\n",
    "Correlation of $Y$ and $Y$ is $$\\rho_{XY} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}$$\n",
    "\n",
    "$\\rho_{XY}$ is also known as the *correlation coefficient*.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
