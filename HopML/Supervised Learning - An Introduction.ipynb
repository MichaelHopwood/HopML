{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: An Introduction\r\n",
    "\r\n",
    "## Least-squares\r\n",
    "\r\n",
    "The assumption is that $f(X) = E(Y|X)$ is linear.\r\n",
    "\r\n",
    "We assume that $ Y = E(Y|X) + \\epsilon$\r\n",
    "\r\n",
    "$$\\hat{Y} = \\hat{\\beta_0} + \\sum_{i=1}^n X_{i} \\hat{\\beta_j} = X^T \\hat{\\beta}$$\r\n",
    "\r\n",
    "Where the $\\beta_0$ term is the model *bias*. The gradient $f^\\prime(X) = \\beta$ is a vector in input space that points in the steepest uphill direction. To fit the model, a (simple) method is *least squares*. Here, we pick coefficients $\\beta$ to minimize the residual sum of squares\r\n",
    "\r\n",
    "$$RSS(\\beta) = \\sum_{i=1}^n (y_i - x_i^T \\beta)^2$$\r\n",
    "\r\n",
    "which shows a quadratic function of the parameters. Therefore, a minimum always exists but may not be unique. In matrix notation, \r\n",
    "\r\n",
    "$$RSS(\\beta) = (y - X \\beta)^T (y - X \\beta)$$\r\n",
    "\r\n",
    "where X is an $N \\times p$ matrix with each row a sample, and y is an N-vector of the outputs in the training set. Differentiating w.r.t. $\\beta$ we get the normal equations\r\n",
    "\r\n",
    "$$X^T (y - X \\beta) = 0$$\r\n",
    "\r\n",
    "If $X^T X$ is nonsingular (i.e. invertible, $AB = BA = I$), then the unique solution is given by\r\n",
    "\r\n",
    "$$\\hat{\\beta} = (X^T X)^{-1} X^T y$$\r\n",
    "\r\n",
    "Therefore, a solution for the best $\\beta$ can be found without iteration.\r\n",
    "\r\n",
    "A \"poor man's\" classifier can use linear regression and predict $1(\\hat{Y} > 0.5)$. Ideally, we would like to estimate $P(Y=1|X=x)$\r\n",
    "\r\n",
    "## Nearest neighbors\r\n",
    "\r\n",
    "For regression, calculates average values of the $k$ nearest neighbors. For classification, a majority vote is conducted.\r\n",
    "\r\n",
    "$$\\hat{y} = \\frac{1}{k} \\sum_{x_i \\in N_k(x)} y_i$$\r\n",
    "\r\n",
    "If large number of variables, it'll require a larger number $k$.  If kept same, then smaller number of neighbors will be included (**Curse of dimensionality**). Increased number of features, the definition of the neighborhood will also have to expand.  The bias increases. This is because as you add another feature, it'll inherently make the points be further apart.\r\n",
    "\r\n",
    "Also, as you increase $k$, a smoother surface will be formed (i.e. reduced variance).\r\n",
    "\r\n",
    "The best $k$ can be found empirically.\r\n",
    "\r\n",
    "## Bias-variance tradeoff\r\n",
    "\r\n",
    "For a fixed $x_0$, \r\n",
    "\r\n",
    "$$E [\\hat{f}(x_0) - f(x_0)] ^2 = E[ \\hat{f}(x_0) - E\\hat{f}(x_0) + E\\hat{f}(x_0) - \\hat{f}(x_0)]^2$$\r\n",
    "\r\n",
    "$$= E[\\hat{f}(x) - E\\hat{f}(x_0)]^2 + [E\\hat{f}(x_0) - f(x_0)]^2 + 2 E\\hat{f}(x_0) - f(x_0) E[\\hat{f}(x_0) - E\\hat{f}(x_0)]$$\r\n",
    "\r\n",
    "We know that $E[\\hat{f}(x_0) - E\\hat{f}(x_0)] = 0$. Therefore, \r\n",
    "\r\n",
    "$$Var(\\hat{f}(x_0)) + bias(\\hat{f}(x_0)^2$$\r\n",
    "\r\n",
    "There is no bias if $k=1$ in nearest neighbor analysis. Small $k$ is small bias but high variance. Large $k$ is the summation over $n$ so benefiting from Variance (because for sample variance, there is a $\\frac{1}{n}$ term) will be low but bias will be high.\r\n",
    "\r\n",
    "## Linear regression vs. kNN\r\n",
    "\r\n",
    "Linear regression has high bias (linear assumption can be violated) but only needs to estimate p+1 parameters.\r\n",
    "\r\n",
    "kNN uses $\\frac{n}{k}$ parameters but is flexible and adaptive. It is small bias but large variance.\r\n",
    "\r\n",
    "\r\n",
    "# Linear Algebra Review\r\n",
    "\r\n",
    "Matrix transpose: $A_{ij}^T = A_{ji}$ and $(AB)^T = B^T A^T$\r\n",
    "\r\n",
    "Matrix (dot) product: $C = AB$\r\n",
    "\r\n",
    "Identity matrix $I$ has a diagonal of ones and the rest zero.\r\n",
    "\r\n",
    "Matrix inversion: $A^{-1} A = A A^{-1} =  I_n$\r\n",
    "\r\n",
    "$$Ax = b$$\r\n",
    "\r\n",
    "$$A^{-1} A x = A^{-1} b$$\r\n",
    "\r\n",
    "$$I_n x = A^{-1} b$$\r\n",
    "\r\n",
    "Invertability. We cannot invert a matrix if 1) more rows than columns or 2) more columns than rows, or 3) redundant rows (\"linear dependence\", \"low rank\")\r\n",
    "\r\n",
    "Norms L^p norm: \r\n",
    "\r\n",
    "L2 norm (p=2) is mos often used. It is a distance. \r\n",
    "\r\n",
    "Eigendecomposition: $A v = \\lambda v$\r\n",
    "\r\n",
    "If $\\lambda$ is eigenvalue of matrix $A$, there exists an eigenvector $V$ such that \r\n",
    "\r\n",
    "$$A = V diag(\\lambda) V^{-1}$$\r\n",
    "\r\n",
    "We can find $\\lambda$ by $\\lambda = \\frac{V^T A V}{V^T V}\r\n",
    "\r\n",
    "Every real symmetric matrix has a real, orthogonal eigendecomposition $A = Q \\Lambda Q^T$.\r\n",
    "\r\n",
    "This will take two vectors on an $x_1, x_2$ space. When you multiply the matrix, on the direction of v_1, you scale it by $\\lambda_1$. This stretches the space.\r\n",
    "\r\n",
    "\r\n",
    "Trace: $Tr(A) = \\sum_i A_{i,i}$\r\n",
    "\r\n",
    "We can switch this around in any way.\r\n",
    "\r\n",
    "$$Tr(ABC) = Tr(CAB) = Tr(BCA)$$\r\n",
    "\r\n",
    "# Probability and Information Theory\r\n",
    "\r\n",
    "A pdf must be contained s.t. $\\all x \\in x, p(x) \\geq 0$. Additionally, $\\sum_{x\\in x} p(x) = 1$ or $\\int p(x) dx = 1$.\r\n",
    "\r\n",
    "Computing a marginal probability with the **sum rule**\r\n",
    "\r\n",
    "$$p(x) = \\int p(x,y) dy$$\r\n",
    "\r\n",
    "\r\n",
    "Conditional probability: $P(y=y, x=x) = \\frac{P(y=y, x=x)}{P(x=x)}$\r\n",
    "\r\n",
    "Chain rule of probability: $P(x_1, ..., x_n) = P(x_1) \\pi_{i=1}^n P(x_i | x_1, ..., x_{(i-1)})$\r\n",
    "\r\n",
    "$P(x_1, x_2, x_3) = P(x_1) P(x_2, x_3 | x_1) = P(x_1) p(x_2) p(x_3 | x_1, x_2)$\r\n",
    "\r\n",
    "\r\n",
    "Independence: $p(x=x, y=y) = p(x=x)p(y=y)$\r\n",
    "\r\n",
    "Expectation: $E_{x\\sim P} [f(x)] = \\sum_x P(x) f(x)$\r\n",
    "\r\n",
    "Variance and covariance: $E(Z)^2 = Var(Z) + (E Z)^2$ where $Z=f(x) - E f(x)$\r\n",
    "\r\n",
    "$Cov(X,Y) = E(XY) - EX EY$\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "F distribution is chi-squared divided by chi-squared\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "name": "python368jvsc74a57bd07c28e26739430500fec97d508cbac2e5d4a112deb445b412c4e69aa96f605479"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}