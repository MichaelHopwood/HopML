{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: An Introduction\r\n",
    "\r\n",
    "## Least-squares\r\n",
    "\r\n",
    "$$\\hat{Y} = \\hat{\\beta_0} + \\sum_{i=1}^n X_{i} \\hat{\\beta_j} = X^T \\hat{\\beta}$$\r\n",
    "\r\n",
    "Where the $\\beta_0$ term is the model *bias*. The gradient $f^\\prime(X) = \\beta$ is a vector in input space that points in the steepest uphill direction. To fit the model, a (simple) method is *least squares*. Here, we pick coefficients $\\beta$ to minimize the residual sum of squares\r\n",
    "\r\n",
    "$$RSS(\\beta) = \\sum_{i=1}^n (y_i - x_i^T \\beta)^2$$\r\n",
    "\r\n",
    "which shows a quadratic function of the parameters. Therefore, a minimum always exists but may not be unique. In matrix notation, \r\n",
    "\r\n",
    "$$RSS(\\beta) = (y - X \\beta)^T (y - X \\beta)$$\r\n",
    "\r\n",
    "where X is an $N \\times p$ matrix with each row a sample, and y is an N-vector of the outputs in the training set. Differentiating w.r.t. $\\beta$ we get the normal equations\r\n",
    "\r\n",
    "$$X^T (y - X \\beta) = 0$$\r\n",
    "\r\n",
    "If $X^T X$ is nonsingular (i.e. invertible, $AB = BA = I$), then the unique solution is given by\r\n",
    "\r\n",
    "$$\\hat{\\beta} = (X^T X)^{-1} X^T y$$\r\n",
    "\r\n",
    "## Nearest-neighbor\r\n",
    "\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "name": "python368jvsc74a57bd07c28e26739430500fec97d508cbac2e5d4a112deb445b412c4e69aa96f605479"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}