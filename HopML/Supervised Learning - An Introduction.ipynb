{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: An Introduction\r\n",
    "\r\n",
    "## Least-squares\r\n",
    "\r\n",
    "The assumption is that $f(X) = E(Y|X)$ is linear.\r\n",
    "\r\n",
    "We assume that $Y = E(Y|X) + \\epsilon$\r\n",
    "\r\n",
    "$$\\hat{Y} = \\hat{\\beta_0} + \\sum_{i=1}^n X_{i} \\hat{\\beta_j} = X^T \\hat{\\beta}$$\r\n",
    "\r\n",
    "Where the $\\beta_0$ term is the model *bias*. The gradient $f^\\prime(X) = \\beta$ is a vector in input space that points in the steepest uphill direction. To fit the model, a (simple) method is *least squares*. Here, we pick coefficients $\\beta$ to minimize the residual sum of squares\r\n",
    "\r\n",
    "$$RSS(\\beta) = \\sum_{i=1}^n (y_i - x_i^T \\beta)^2$$\r\n",
    "\r\n",
    "which shows a quadratic function of the parameters. Therefore, a minimum always exists but may not be unique. In matrix notation, \r\n",
    "\r\n",
    "$$RSS(\\beta) = (y - X \\beta)^T (y - X \\beta)$$\r\n",
    "\r\n",
    "where X is an $N \\times p$ matrix with each row a sample, and y is an N-vector of the outputs in the training set. Differentiating w.r.t. $\\beta$ we get the normal equations\r\n",
    "\r\n",
    "$$X^T (y - X \\beta) = 0$$\r\n",
    "\r\n",
    "If $X^T X$ is nonsingular (i.e. invertible, $AB = BA = I$), then the unique solution is given by\r\n",
    "\r\n",
    "$$\\hat{\\beta} = (X^T X)^{-1} X^T y$$\r\n",
    "\r\n",
    "Therefore, a solution for the best $\\beta$ can be found without iteration.\r\n",
    "\r\n",
    "A \"poor man's\" classifier can use linear regression and predict $1(\\hat{Y} > 0.5)$. Ideally, we would like to estimate $P(Y=1|X=x)$\r\n",
    "\r\n",
    "## Nearest neighbors\r\n",
    "\r\n",
    "For regression, calculates average values of the $k$ nearest neighbors. This replaces the expected value (in normal regression) with the sample average. For classification, a majority vote is conducted.\r\n",
    "\r\n",
    "$$\\hat{y} = \\frac{1}{k} \\sum_{x_i \\in N_k(x)} y_i$$\r\n",
    "\r\n",
    "If large number of variables, it'll require a larger number $k$.  If kept same, then smaller number of neighbors will be included (**Curse of dimensionality**). Increased number of features, the definition of the neighborhood will also have to expand.  The bias increases. This is because as you add another feature, it'll inherently make the points be further apart.\r\n",
    "\r\n",
    "Also, as you increase $k$, a smoother surface will be formed (i.e. reduced variance).\r\n",
    "\r\n",
    "The best $k$ can be found empirically.\r\n",
    "\r\n",
    "\r\n",
    "## Bias-variance tradeoff\r\n",
    "\r\n",
    "For a fixed $x_0$, \r\n",
    "\r\n",
    "$$E [\\hat{f}(x_0) - f(x_0)] ^2 = E[ \\hat{f}(x_0) - E\\hat{f}(x_0) + E\\hat{f}(x_0) - \\hat{f}(x_0)]^2$$\r\n",
    "\r\n",
    "$$= E[\\hat{f}(x) - E\\hat{f}(x_0)]^2 + [E\\hat{f}(x_0) - f(x_0)]^2 + 2 E\\hat{f}(x_0) - f(x_0) E[\\hat{f}(x_0) - E\\hat{f}(x_0)]$$\r\n",
    "\r\n",
    "We know that $E[\\hat{f}(x_0) - E\\hat{f}(x_0)] = 0$. Therefore, \r\n",
    "\r\n",
    "$$Var(\\hat{f}(x_0)) + bias(\\hat{f}(x_0)^2$$\r\n",
    "\r\n",
    "There is no bias if $k=1$ in nearest neighbor analysis. Small $k$ is small bias but high variance. Large $k$ is the summation over $n$ so benefiting from Variance (because for sample variance, there is a $\\frac{1}{n}$ term) will be low but bias will be high.\r\n",
    "\r\n",
    "## Linear regression vs. kNN\r\n",
    "\r\n",
    "Linear regression has high bias (linear assumption can be violated) but only needs to estimate p+1 parameters.\r\n",
    "\r\n",
    "kNN uses $\\frac{n}{k}$ parameters but is flexible and adaptive. It is small bias but large variance.\r\n",
    "\r\n",
    "\r\n",
    "# Linear Algebra Review\r\n",
    "\r\n",
    "Matrix transpose: $A_{ij}^T = A_{ji}$ and $(AB)^T = B^T A^T$\r\n",
    "\r\n",
    "Matrix (dot) product: $C = AB$\r\n",
    "\r\n",
    "Identity matrix $I$ has a diagonal of ones and the rest zero.\r\n",
    "\r\n",
    "Matrix inversion: $A^{-1} A = A A^{-1} =  I_n$\r\n",
    "\r\n",
    "$$Ax = b$$\r\n",
    "\r\n",
    "$$A^{-1} A x = A^{-1} b$$\r\n",
    "\r\n",
    "$$I_n x = A^{-1} b$$\r\n",
    "\r\n",
    "Invertability. We cannot invert a matrix if 1) more rows than columns or 2) more columns than rows, or 3) redundant rows (\"linear dependence\", \"low rank\")\r\n",
    "\r\n",
    "Norms L^p norm: \r\n",
    "\r\n",
    "L2 norm (p=2) is mos often used. It is a distance. \r\n",
    "\r\n",
    "Eigendecomposition: $A v = \\lambda v$\r\n",
    "\r\n",
    "If $\\lambda$ is eigenvalue of matrix $A$, there exists an eigenvector $V$ such that \r\n",
    "\r\n",
    "$$A = V diag(\\lambda) V^{-1}$$\r\n",
    "\r\n",
    "We can find $\\lambda$ by $\\lambda = \\frac{V^T A V}{V^T V}$\r\n",
    "\r\n",
    "Every real symmetric matrix has a real, orthogonal eigendecomposition $A = Q \\Lambda Q^T$.\r\n",
    "\r\n",
    "This will take two vectors on an $x_1, x_2$ space. When you multiply the matrix, on the direction of v_1, you scale it by $\\lambda_1$. This stretches the space.\r\n",
    "\r\n",
    "\r\n",
    "Trace: $Tr(A) = \\sum_i A_{i,i}$\r\n",
    "\r\n",
    "We can switch this around in any way.\r\n",
    "\r\n",
    "$$Tr(ABC) = Tr(CAB) = Tr(BCA)$$\r\n",
    "\r\n",
    "# Probability and Information Theory\r\n",
    "\r\n",
    "A pdf must be contained s.t. $\\forall x \\in x, p(x) \\geq 0$. Additionally, $\\sum_{x\\in x} p(x) = 1$ or $\\int p(x) dx = 1$.\r\n",
    "\r\n",
    "Computing a marginal probability with the **sum rule**\r\n",
    "\r\n",
    "$$p(x) = \\int p(x,y) dy$$\r\n",
    "\r\n",
    "\r\n",
    "Conditional probability: $P(y=y, x=x) = \\frac{P(y=y, x=x)}{P(x=x)}$\r\n",
    "\r\n",
    "Chain rule of probability: $P(x_1, ..., x_n) = P(x_1) \\pi_{i=1}^n P(x_i | x_1, ..., x_{(i-1)})$\r\n",
    "\r\n",
    "$P(x_1, x_2, x_3) = P(x_1) P(x_2, x_3 | x_1) = P(x_1) p(x_2) p(x_3 | x_1, x_2)$\r\n",
    "\r\n",
    "\r\n",
    "Independence: $p(x=x, y=y) = p(x=x)p(y=y)$\r\n",
    "\r\n",
    "Expectation: $E_{x\\sim P} [f(x)] = \\sum_x P(x) f(x)$\r\n",
    "\r\n",
    "Variance and covariance: $E(Z)^2 = Var(Z) + (E Z)^2$ where $Z=f(x) - E f(x)$\r\n",
    "\r\n",
    "$Cov(X,Y) = E(XY) - EX EY$\r\n",
    "\r\n",
    "F distribution is chi-squared divided by chi-squared\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Conditional Expectation\r\n",
    "\r\n",
    "\r\n",
    "The conditional expected value is just the expectation when X is specified. \r\n",
    "\r\n",
    "$$E(Y|X=x) = \\int_{y} y dF(y|X=x) = \\int_y y f_{y|X}(y|x) dy$$\r\n",
    "\r\n",
    "Conditional expectation is a random variable. Without specificing $X=x$, $E(Y|X)$ is a function of $X$. Because $X$ is a RV, then $E(Y|X)$ is also RV.\r\n",
    "\r\n",
    "Tower property: $E(Y) = E[E(Y|X)]$.\r\n",
    "\r\n",
    "We say that $X$ takes a fixed value such as $x_0 = 0$, then $g(x_0)$ is **deterministic** (i.e. not random). Its form may be unknown, or involves unknown parameters, e.g.\r\n",
    "\r\n",
    "**Example**\r\n",
    "\r\n",
    "$Y = a X^2 + \\epsilon$, $\\epsilon$ ind $X$, $\\epsilon \\sim N(0,1)$\r\n",
    "\r\n",
    "$E(Y|X) = E(c + X^2 + \\epsilon | X) = c + X^2 + E(\\epsilon|X) = c + X^2$ where $E(\\epsilon|X)=0$\r\n",
    "\r\n",
    "**Example**\r\n",
    "\r\n",
    "$Y = X^2 + 10X + 20 + \\epsilon$\r\n",
    "\r\n",
    "where $\\epsilon \\sim N(0, 3)$ and $X \\sim N(30,10)$\r\n",
    "\r\n",
    "$$\r\n",
    "\\begin{bmatrix}\r\n",
    "X\\\\\r\n",
    "Y\\\\\r\n",
    "\\end{bmatrix} = N(\r\n",
    "\\begin{bmatrix}\r\n",
    "\\mu_x & \\sigma_x^2 + \\sigma_{xy}^2\\\\\r\n",
    "\\mu_y & \\sigma_{xy} + \\sigma_x^2\r\n",
    "\\end{bmatrix}\r\n",
    ")\r\n",
    "$$\r\n",
    "\r\n",
    "In this case, we know the underlying probability model.\r\n",
    "\r\n",
    "The joint distribution gives a lot of information!\r\n",
    "\r\n",
    "We can evalaute for the best model $f$ by minimizing a loss function (i.e. $L(Y, f(X)) = Y - f(X))^2$)\r\n",
    "\r\n",
    "$$EPE(f) = E L(Y, f(X)) = E(Y - f(X))^2 = \\int [y - f(x)]^2 Pr(dx, dy)$$\r\n",
    "\r\n",
    "Because we have assumed that we know the joint distribution (and it's all continuous), then we evaluate an integral.\r\n",
    "\r\n",
    "**The best $f$ is E(Y|X=x)**\r\n",
    "\r\n",
    "^ This depends on your loss function! (using squared loss!) If you use L1 then your best $f$ will be at the median. Squared loss is better because can take derivative of it. However, it can be influenced by extreme values. \r\n",
    "\r\n",
    "$$EPE(f) = E(Y - f(X))^2 = E[E[(Y - f(x)]^2 | X)]$$\r\n",
    "\r\n",
    "Minimize $E[(Y - f(x)]^2 | X)$ for every X. This can be decomposed\r\n",
    "\r\n",
    "$$E[(Y - f(x)]^2 | X) = E[[Y - E(Y|X) + E(Y|X) - f(X)]^2 | X]$$\r\n",
    "\r\n",
    "With $A = Y - E(Y|X)$ and $B = E(Y|X) - f(X)$\r\n",
    "\r\n",
    "$$= E(A^2 | X) + E(B^2 | X) + 2 E(A \\times B | X)$$\r\n",
    "\r\n",
    "We know that at a given $X$, $A \\times B$ is a constant. \r\n",
    "\r\n",
    "$$ = [E[Y|X] - f(X)] [E(Y|X) E[Y|X]]$$\r\n",
    "\r\n",
    "Therefore, $EPE(f) = EPE(E(Y|X)) + B$\r\n",
    "\r\n",
    "If the population is known, then $f(x) = \\int y f_{Y|X} (y|x) dy$ simply. This is the ideal case where you have population. However, this is rare.\r\n",
    "\r\n",
    "For Example, if $Y$ is a known funtion of $X$ (with some error), then you know the conditional distribution. From this, you can estimate $f$ as the mean of that conditional distribution.\r\n",
    "\r\n",
    "### Categorical classification\r\n",
    "\r\n",
    "Loss matrix can be used to penalize categories heavier. \r\n",
    "\r\n",
    "For example, in stock market prediction, we may place a heavier scaler on the loss function for when the stock market \r\n",
    "\r\n",
    "Popular choice: $L = 1_{K \\times K} - I_K$ forms a matrix of ones except for zeros in the diagonal (because no update should be made if it is correct). This can also be expressed as $L(G, \\hat{G}(X)) = I(G \\neq \\hat{G}(X))$. \r\n",
    "\r\n",
    "The solution that minimizes the EPE is $\\hat{G}(x) = arg max_g Pr_{G|X} (g|x)$. The group that maximizes the conditional probability $Pr_{G|X}(g|x)$. This is called the bayes classifier. Its error is called the bayes rate. The group has a prior (original) distribution. For example, increasing and decreasing is equally likely.  According to yesterday's information, update and calculate posterior probability $Pr_{G|X}(g|x)$.\r\n",
    "\r\n",
    "**Example**\r\n",
    "\r\n",
    "Generate $X|G \\sim N(\\mu_G, I_2)$ where two centers are defined: $\\mu_1 = (0,1)^T, \\mu_2 = (1,0)^T$\r\n",
    "\r\n",
    "Because this was generated, we know the labels: $G_{1}, ..., G_{100} = 1$ and $G_{101}, ..., G_{200} = 2$. \r\n",
    "\r\n",
    "The bayes classifier is found by assuming the joint distribution $X|G \\sim N(\\mu_G, I_2)$. Therefore, each group is equally likely. The boundary between these two groups is found by \r\n",
    "\r\n",
    "$E(1(G|X)) = P(G=1|x_0) $ versus $P(G=2|X)$ and the larger one is chosen for the point.\r\n",
    "\r\n",
    "At the beginning, $P(G=1) = P(G=2) = 0.5$. \r\n",
    "\r\n",
    "At a sample located at $\\vec{X} = (10,9)$, the expectation can be evaluated by $P(G_j = 1 | x_0 = (10,9)) = f(x_0 = (10,9)) = \\frac{f(x_0 = (10,9) | G=1)}{f_x( (10, 9) )}$\r\n",
    "\r\n",
    "$f_{N(0,1})(x_0, x_1)}$ is the double normal distribution (a function of X2 and X1).\r\n",
    "\r\n",
    "So plug in the likelihood of observing the X multiplied by the given distribution (per bayesian rule). Bayes rule finds the ratio of the joint probability \r\n",
    "\r\n",
    "$ = \\frac{f(x_0 | \\mu_1) \\times P(G=1 | x_0)}{f(x_0 | \\mu_1) \\times P(G=1 | x_0) + f(x_0 | \\mu_2) \\times P(G=1 | x_0)}$\r\n",
    "\r\n",
    "### Linear regression\r\n",
    "\r\n",
    "$$RSS(\\beta) = \\sum_{i=1}^N (y_i - f(x_i))^2 = \\sum_{i=1}^N (y_i - \\beta_0 - \\sum)^2$$\r\n",
    "\r\n",
    "With a feature $p=1$, what is the estimated $\\beta$?\r\n",
    "\r\n",
    "Solution: Take the derivative and then set equal to zero. RSS will have a minimum.\r\n",
    "\r\n",
    "$RSS(\\beta_0, \\beta_1) = y - X\\beta$\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Suppose each of $K$-classes has an associated target $ùë°_ùëò$, which is a vector of\r\n",
    "all zeros, except a one in the $k$th position. Show that classifying to the largest element of $\\hat{y}$\r\n",
    "amounts to choosing the closest target, $min_{k} ||t_k - \\hat{y}||$, if the elements of $\\hat{y}$ sum to one.\r\n",
    "\r\n",
    "**Proof:** \r\n",
    "\r\n",
    "$$\r\n",
    "T = \\begin{bmatrix}\r\n",
    "\r\n",
    "\\end{bmatrix}\r\n",
    "$$\r\n",
    "\r\n",
    "where $t_k \\in T$.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal set of points per class\n",
      " 1    100\n",
      "0    100\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<Figure size 576x576 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "array([-12.52801629,  -3.56239247,  -2.90412878, -12.28710829,\n         0.03638171,  -3.69897114, -15.68068774,  -5.29109045,\n         1.73279073,  -9.56389529,  -3.91261051,  -7.77714045,\n       -20.36512245, -13.86835129, -10.46041684, -11.53642131,\n         2.28581322, -20.0824698 , -11.23063806, -13.23706341,\n        -8.63109328, -24.75292263,  -7.20376479,  -2.32533403,\n        -2.84058561,   2.9898612 , -11.33712585,  -5.31854411,\n       -11.70858866,  -3.23172379,  -8.21273938, -11.05267593,\n       -14.85659008, -10.79956758, -15.82075792,  -2.82563244,\n         6.92284119,  -7.18854815, -14.80028888, -11.24504322,\n       -10.03305471,   4.32640523,  -4.02833559,  -2.50864522,\n        -4.06321913, -15.79993956,  -1.97854182, -11.94400958,\n        -2.94197991,  -2.9554109 ,  -7.84252512,   4.78158277,\n        -5.06642196,   0.7620665 ,  -6.38174487, -15.87146272,\n        -1.83985641, -16.05808142, -11.64422887,  -1.73712784,\n       -11.55502809,   0.49368146,  -8.9141197 ,  -4.47018294,\n       -10.30966402,  -7.6337805 ,   5.62325467,  -9.45582604,\n        -4.78650073,  -6.41868595, -10.20721903, -10.2212615 ,\n       -21.96381068, -13.01965375, -12.80463012, -16.25059092,\n       -14.69429141,  -4.15792703, -14.03396786, -13.87873843,\n       -18.92948065, -14.23676922,   3.5449348 ,  -9.69359908,\n        -4.93056337, -17.00099233, -11.10485984, -11.10226636,\n        -7.10986679,  -7.48052026, -11.41565348,  -4.77402352,\n         0.88247658,  -9.17038483,  -6.23842245, -15.41621936,\n       -16.98743418,  -0.60445564, -16.59286506,  -8.14512002,\n       -12.4882279 , -12.39555375,  -0.16106315, -12.2436581 ,\n         3.19703127,  -1.05266318, -13.65454362, -10.44047927,\n        -2.73884785, -19.77044463, -18.86364727, -10.81880578,\n        -7.51619948,  -8.56680739, -12.72410352,  -7.62775356,\n         4.99411888, -20.04496827,  -6.33118156, -12.5174306 ,\n        -6.84381857, -11.25007254, -11.98360433,  -8.33626392,\n       -10.10050716,   0.9997224 ,  -1.0412171 , -18.94775759,\n         0.51359441,  -8.72423253,  -4.79577627, -15.71136135,\n       -14.91665909, -14.60698374,  -4.76202533, -13.49562241,\n        -0.68062381,  -6.13801293,  -4.75302514,   4.33465074,\n        -2.73418264, -12.40742452, -21.71647259, -17.36768084,\n       -11.18922619,  -6.90756339,  -5.11906518, -12.66818122,\n         5.36757851, -14.56656309,  -9.71107651,   1.56668862,\n         0.08755093, -17.91634951, -14.53232612, -18.12952106,\n        -5.67417619, -18.10593449,  -8.0772865 , -14.41981503,\n       -10.55277247, -12.59537577, -29.68881794,  -8.7915628 ,\n        -6.12546341,   2.31008532,  -2.14707948,  -2.66213343,\n        -4.58600765,  -9.34665372, -14.6084582 ,   0.50974943,\n       -22.37778705, -18.89048663, -23.901368  ,  -8.09278773,\n       -12.32596697, -12.40549646,  -3.04245236,  -2.76780497,\n       -12.75180485, -22.05980659,   1.76566269,   1.03665045,\n        -2.192392  ,  -9.33296738,   0.12737452, -22.37050241,\n       -18.89008875,  -8.93790605,  -2.05934405, -13.52670648,\n        -9.60304952, -14.68956417, -14.39437982,  -9.52689858,\n       -13.6913479 , -12.00936192, -12.83355541,   7.04190877])"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "np.random.seed(34)\r\n",
    "\r\n",
    "X, y = make_blobs(n_samples=200, centers=2, n_features=2, cluster_std=1.)\r\n",
    "\r\n",
    "print(\"Equal set of points per class\\n\", pd.Series(y).value_counts())\r\n",
    "\r\n",
    "unique_classes = np.unique(y)\r\n",
    "\r\n",
    "fig = plt.figure(figsize=(8,8))\r\n",
    "\r\n",
    "for c in unique_classes:\r\n",
    "    mask = y == c\r\n",
    "    plt.scatter(X[mask,0], X[mask,1], label=f\"Class: {c}\")\r\n",
    "plt.legend()\r\n",
    "plt.xlabel(\"X0\")\r\n",
    "plt.ylabel(\"X1\")\r\n",
    "plt.rcParams.update({'font.size': 20})\r\n",
    "plt.show()\r\n",
    "\r\n",
    "# Add column of 1s to a version of X for classification\r\n",
    "X_proc = np.append(np.ones([len(X), 1]), X, axis=1)\r\n",
    "\r\n",
    "def f(X, B):\r\n",
    "    return np.dot(X, B)\r\n",
    "\r\n",
    "f(X_proc, [1,3,4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "name": "python368jvsc74a57bd07c28e26739430500fec97d508cbac2e5d4a112deb445b412c4e69aa96f605479"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}