{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 1 0 1 0 0]\n",
      "ODDS 0.8\n",
      "pseudo_residuals 0   -0.444444\n",
      "1    0.555556\n",
      "2   -0.444444\n",
      "3    0.555556\n",
      "4    0.555556\n",
      "5   -0.444444\n",
      "6    0.555556\n",
      "7   -0.444444\n",
      "8   -0.444444\n",
      "dtype: float64\n",
      "input 0   -0.444444\n",
      "1    0.555556\n",
      "2   -0.444444\n",
      "3    0.555556\n",
      "4    0.555556\n",
      "5   -0.444444\n",
      "6    0.555556\n",
      "7   -0.444444\n",
      "8   -0.444444\n",
      "dtype: float64\n",
      "preds [-0.22314355 -0.22314355 -0.22314355 -0.22314355 -0.22314355 -0.22314355\n",
      " -0.22314355 -0.22314355 -0.22314355]\n",
      "[-0.22314355 -0.22314355 -0.22314355 -0.22314355 -0.22314355 -0.22314355\n",
      " -0.22314355 -0.22314355 -0.22314355]\n",
      "p [0.44444444 0.44444444 0.44444444 0.44444444 0.44444444 0.44444444\n",
      " 0.44444444 0.44444444 0.44444444]\n",
      "val 9.992007221626408e-17\n",
      "boosting_tree.predict(X) [9.99200722e-17 9.99200722e-17 9.99200722e-17 9.99200722e-17\n",
      " 9.99200722e-17 9.99200722e-17 9.99200722e-17 9.99200722e-17\n",
      " 9.99200722e-17]\n",
      "pseudo_residuals 0   -0.444444\n",
      "1    0.555556\n",
      "2   -0.444444\n",
      "3    0.555556\n",
      "4    0.555556\n",
      "5   -0.444444\n",
      "6    0.555556\n",
      "7   -0.444444\n",
      "8   -0.444444\n",
      "dtype: float64\n",
      "input 0   -0.444444\n",
      "1    0.555556\n",
      "2   -0.444444\n",
      "3    0.555556\n",
      "4    0.555556\n",
      "5   -0.444444\n",
      "6    0.555556\n",
      "7   -0.444444\n",
      "8   -0.444444\n",
      "dtype: float64\n",
      "preds [-0.22314355 -0.22314355 -0.22314355 -0.22314355 -0.22314355 -0.22314355\n",
      " -0.22314355 -0.22314355 -0.22314355]\n",
      "[-0.22314355 -0.22314355 -0.22314355 -0.22314355 -0.22314355 -0.22314355\n",
      " -0.22314355 -0.22314355 -0.22314355]\n",
      "p [0.44444444 0.44444444 0.44444444 0.44444444 0.44444444 0.44444444\n",
      " 0.44444444 0.44444444 0.44444444]\n",
      "val 9.992007221626408e-17\n",
      "boosting_tree.predict(X) [9.99200722e-17 9.99200722e-17 9.99200722e-17 9.99200722e-17\n",
      " 9.99200722e-17 9.99200722e-17 9.99200722e-17 9.99200722e-17\n",
      " 9.99200722e-17]\n",
      "estimator.predict(X) [9.99200722e-17 9.99200722e-17 9.99200722e-17 9.99200722e-17\n",
      " 9.99200722e-17 9.99200722e-17 9.99200722e-17 9.99200722e-17\n",
      " 9.99200722e-17]\n",
      "estimator.predict(X) [9.99200722e-17 9.99200722e-17 9.99200722e-17 9.99200722e-17\n",
      " 9.99200722e-17 9.99200722e-17 9.99200722e-17 9.99200722e-17\n",
      " 9.99200722e-17]\n",
      "[0 1 0 1 1 0 1 0 0]\n",
      "ODDS 0.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.22314355, -0.22314355, -0.22314355, -0.22314355, -0.22314355,\n",
       "       -0.22314355, -0.22314355, -0.22314355, -0.22314355])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import e\n",
    "\n",
    "class Node:\n",
    "    '''\n",
    "    This class defines a node which creates a tree structure by recursively calling itself \n",
    "    whilst checking a number of ending parameters such as depth and min_leaf. It uses an exact greedy method\n",
    "    to exhaustively scan every possible split point. The gain metric of choice is conservation of varience.\n",
    "    This is a Naive solution and does not comapre to Frieman's 2001 Gradient Boosting Machines\n",
    "    \n",
    "    Input\n",
    "    ------------------------------------------------------------------------------------------\n",
    "    X: Pandas dataframe\n",
    "    y: Pandas Series\n",
    "    idxs: indices of values used to keep track of splits points in tree\n",
    "    predictions: are the predictions of a gradient boosting algorthim thus far used to calculate the leaf values\n",
    "    min_leaf: minimum number of samples needed to be classified as a node\n",
    "    depth: sets the maximum depth allowed\n",
    "    classification: a flag that indicates if the problem is regression or binary classification\n",
    "    \n",
    "    Output\n",
    "    ---------------------------------------------------------------------------------------------\n",
    "    Regression tree that can either be used for classification or regression\n",
    "    '''\n",
    "\n",
    "    def __init__(self, x, y, idxs, classification, predictions, min_leaf=5, depth = 10):\n",
    "        self.x, self.y = x, y\n",
    "        self.idxs = idxs \n",
    "        self.depth = depth\n",
    "        self.min_leaf = min_leaf\n",
    "        self.row_count = len(idxs)\n",
    "        self.col_count = x.shape[1]\n",
    "        self.classification = classification\n",
    "        self.predictions = predictions\n",
    "        \n",
    "        if classification:\n",
    "            print('input', y)\n",
    "            print('preds', predictions)\n",
    "            self.val = self.compute_leaf_value(y[idxs], predictions[idxs])\n",
    "            print('val', self.val)\n",
    "        else:\n",
    "            self.val = np.mean(y[idxs])\n",
    "          \n",
    "        self.score = float('inf')\n",
    "        self.find_varsplit()\n",
    "            \n",
    "    def find_varsplit(self):\n",
    "        '''\n",
    "        Scans through every column and calcuates the best split point.\n",
    "        The node is then split at this point and two new nodes are created.\n",
    "        Depth is only parameter to change as we have added a new layer to tre structure.\n",
    "        If no split is better than the score initalised at the begining then no splits further splits are made\n",
    "        \n",
    "        '''\n",
    "        for c in range(self.col_count): self.find_better_split(c)\n",
    "        if self.is_leaf: return\n",
    "        x = self.split_col\n",
    "        lhs = np.nonzero(x <= self.split)[0]\n",
    "        rhs = np.nonzero(x > self.split)[0]\n",
    "        self.lhs = Node(self.x, self.y, self.idxs[lhs], self.classification, self.predictions, self.min_leaf, depth = self.depth-1)\n",
    "        self.rhs = Node(self.x, self.y, self.idxs[rhs], self.classification, self.predictions, self.min_leaf, depth = self.depth-1)\n",
    "        \n",
    "    def find_better_split(self, var_idx):\n",
    "        '''\n",
    "        For a given feature calculates the gain at each split.\n",
    "        Globally updates the best score if a better split point is found\n",
    "        '''\n",
    "        x = self.x.values[self.idxs, var_idx]\n",
    "\n",
    "        for r in range(self.row_count):\n",
    "            lhs = x <= x[r]\n",
    "            rhs = x > x[r]\n",
    "            if rhs.sum() < self.min_leaf or lhs.sum() < self.min_leaf: continue\n",
    "\n",
    "            curr_score = self.find_score(lhs, rhs)\n",
    "            if curr_score < self.score: \n",
    "                self.var_idx = var_idx\n",
    "                self.score = curr_score\n",
    "                self.split = x[r]\n",
    "                \n",
    "    def find_score(self, lhs, rhs):\n",
    "        '''\n",
    "        Calculates or metric for evaluating split use standard deviation chooses splits where standard \n",
    "        deviation is low as it means these points are similar and can be grouped together.\n",
    "        '''\n",
    "        y = self.y[self.idxs]\n",
    "        lhs_std = y[lhs].std()\n",
    "        rhs_std = y[rhs].std()\n",
    "        return lhs_std * lhs.sum() + rhs_std * rhs.sum()\n",
    "\n",
    "    def compute_leaf_value(self, leaf_values, predictions):\n",
    "        '''\n",
    "        if we are constructing a GBM classifier this is the optimal leaf node \n",
    "        value.\n",
    "        '''\n",
    "        p = self.sigmoid(predictions)\n",
    "        denominator = p*(1- p)\n",
    "        print(predictions)\n",
    "        print('p',p)\n",
    "        return(np.sum(leaf_values)/np.sum(denominator))\n",
    "    \n",
    "    @staticmethod  \n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @property\n",
    "    def split_col(self):\n",
    "        return self.x.values[self.idxs,self.var_idx]\n",
    "                \n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.score == float('inf') or self.depth <= 0                 \n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "\n",
    "    def predict_row(self, xi):\n",
    "        if self.is_leaf: return self.val\n",
    "        node = self.lhs if xi[self.var_idx] <= self.split else self.rhs\n",
    "        return node.predict_row(xi)\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "    '''\n",
    "    Wrapper class that provides a scikit learn interface to the recursive regression tree above\n",
    "    \n",
    "    Input\n",
    "    ------------------------------------------------------------------------------------------\n",
    "    X: Pandas dataframe\n",
    "    y: Pandas Series\n",
    "    idxs: indices of values used to keep track of splits points in tree\n",
    "    predictions: are the predictions of a gradient boosting algorthim thus far used to calculate the leaf values\n",
    "    min_leaf: minimum number of samples needed to be classified as a node\n",
    "    depth: sets the maximum depth allowed\n",
    "    classification: a flag that indicates if the problem is regression or binary classification\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def fit(self, X, y, classification ,min_leaf = 5, depth = 5, predictions = []):\n",
    "        self.dtree = Node(x = X, y = y, idxs = np.array(np.arange(len(y))), predictions = predictions, min_leaf = min_leaf, depth = depth, classification = classification)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.dtree.predict(X.values)\n",
    "        \n",
    "class GradientBoostingClassification:\n",
    "    '''\n",
    "    Applies the methododlgy of gradeint boosting for binary classification only.\n",
    "    Uses the Binary logistic loss function to calculate the negative derivate.\n",
    "    \n",
    "    Input\n",
    "    ------------------------------------------------------------------------------------------\n",
    "    X: Pandas dataframe\n",
    "    y: Pandas Series\n",
    "    min_leaf: minimum number of samples needed to be classified as a node\n",
    "    depth: sets the maximum depth allowed\n",
    "    Boosting_Rounds: number of boosting rounds or iterations\n",
    "    \n",
    "    Output\n",
    "    ---------------------------------------------------------------------------------------------\n",
    "    Gradient boosting machine that can be used for binary classification.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.estimators = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "    def negativeDerivitiveLogloss(self, y, log_odds):\n",
    "        p = self.sigmoid(log_odds)\n",
    "        return(y - p)\n",
    "        \n",
    "    @staticmethod\n",
    "    def log_odds(column):\n",
    "        '''\n",
    "        Calculates the initial log odds prediction of the taget variable\n",
    "        '''\n",
    "        if isinstance(column, pd.Series):\n",
    "            binary_yes = np.count_nonzero(column.values == 1)\n",
    "            binary_no  = np.count_nonzero(column.values == 0)\n",
    "        elif isinstance(column, list):\n",
    "            column = np.array(column)\n",
    "            binary_yes = np.count_nonzero(column == 1)\n",
    "            binary_no  = np.count_nonzero(column == 0) \n",
    "        else:\n",
    "            binary_yes = np.count_nonzero(column == 1)\n",
    "            binary_no  = np.count_nonzero(column == 0)\n",
    "        \n",
    "        print(column.values)\n",
    "        print(\"ODDS\", binary_yes/binary_no)\n",
    "        value = np.log(binary_yes/binary_no)\n",
    "        return(np.full((len(column), 1), value).flatten())\n",
    "    \n",
    "    def fit(self, X, y, depth = 5, min_leaf = 5, learning_rate = 0.1, boosting_rounds = 5):\n",
    "        \n",
    "        # use the log odds value of the target variable as our inital prediction\n",
    "        self.learning_rate = learning_rate\n",
    "        self.base_pred = self.log_odds(y)\n",
    "        \n",
    "        for booster in range(boosting_rounds):\n",
    "            # Calculate the initial Pseudo Residuals using Base Prediction.\n",
    "            pseudo_residuals = self.negativeDerivitiveLogloss(y, self.base_pred)\n",
    "            print(\"pseudo_residuals\", pseudo_residuals)\n",
    "            # Approximate the residuals\n",
    "            boosting_tree = DecisionTreeRegressor().fit(X = X, y = pseudo_residuals, depth = depth, min_leaf = min_leaf, classification = True , predictions = self.base_pred)\n",
    "            self.base_pred += self.learning_rate * boosting_tree.predict(X)\n",
    "            print(\"boosting_tree.predict(X)\", boosting_tree.predict(X))\n",
    "            # store predictors for later\n",
    "            self.estimators.append(boosting_tree)\n",
    "   \n",
    "    def predict(self, X):\n",
    "        \n",
    "        pred = np.zeros(X.shape[0])\n",
    "        for estimator in self.estimators:\n",
    "            pred += self.learning_rate * estimator.predict(X)\n",
    "            print(\"estimator.predict(X)\", estimator.predict(X))\n",
    "        return self.log_odds(y) + pred\n",
    "        \n",
    "        \n",
    "class GradientBoostingRegressor:\n",
    "    '''\n",
    "    Applies the methododlgy of gradeint boosting for regression.\n",
    "    Uses the mean squared loss function to calculate the negative derivate.\n",
    "    \n",
    "    Input\n",
    "    ------------------------------------------------------------------------------------------\n",
    "    X: Pandas dataframe\n",
    "    y: Pandas Series\n",
    "    min_leaf: minimum number of samples needed to be classified as a node\n",
    "    depth: sets the maximum depth allowed\n",
    "    Boosting_Rounds: number of boosting rounds or iterations\n",
    "    \n",
    "    Output\n",
    "    ---------------------------------------------------------------------------------------------\n",
    "    Gradient boosting machine that can be used for regression.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.estimators = []\n",
    "      \n",
    "    @staticmethod\n",
    "    def negativeMeanSquaredErrorDerivitive(y, y_pred):\n",
    "        return(2*(y-y_pred))\n",
    "        \n",
    "    def fit(self, X, y, depth = 5, min_leaf = 5, learning_rate = 0.1, boosting_rounds = 5):\n",
    "        \n",
    "        # Start with the mean y value as our initial prediciton\n",
    "        self.learning_rate = learning_rate\n",
    "        self.base_pred = np.full((X.shape[0], 1), np.mean(y)).flatten()\n",
    "        \n",
    "        for booster in range(boosting_rounds):\n",
    "            # Calculate the initial Pseudo Residuals using Base Prediction.\n",
    "            pseudo_residuals = self.negativeMeanSquaredErrorDerivitive(y, self.base_pred)\n",
    "            # Approximate the residuals\n",
    "            boosting_tree = DecisionTreeRegressor().fit(X = X, y = pseudo_residuals, depth = 5, min_leaf = 5, classification = False , predictions = None)\n",
    "            self.base_pred += self.learning_rate * boosting_tree.predict(X)\n",
    "            # store predictors for later\n",
    "            self.estimators.append(boosting_tree)\n",
    "   \n",
    "    def predict(self, X):\n",
    "        \n",
    "        pred = np.zeros(X.shape[0])\n",
    "        for estimator in self.estimators:\n",
    "            pred += self.learning_rate * estimator.predict(X)\n",
    "        \n",
    "        return np.full((X.shape[0], 1), np.mean(y)).flatten() + pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = np.array([\n",
    "    ['excellent', '3 yrs', 'high', 'safe'],\n",
    "    ['fair', '5 yrs', 'low', 'risky'],\n",
    "    ['fair', '3 yrs', 'high', 'safe'],\n",
    "    ['poor', '5 yrs', 'high', 'risky'],\n",
    "    ['excellent', '3 yrs', 'low', 'risky'],\n",
    "    ['fair', '5 yrs', 'low', 'safe'],\n",
    "    ['poor', '3 yrs', 'high', 'risky'],\n",
    "    ['poor', '5 yrs', 'low', 'safe'],\n",
    "    ['fair', '3 yrs', 'high', 'safe']\n",
    "])\n",
    "\n",
    "# Separate the features and target\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# Convert the categorical features to numerical\n",
    "X[X == 'excellent'] = 0\n",
    "X[X == 'fair'] = 1\n",
    "X[X == 'poor'] = 2\n",
    "X[X == '3 yrs'] = 0\n",
    "X[X == '5 yrs'] = 1\n",
    "X[X == 'low'] = 0\n",
    "X[X == 'high'] = 1\n",
    "y[y == 'safe'] = 0\n",
    "y[y == 'risky'] = 1\n",
    "y = y.astype(int)\n",
    "\n",
    "X = pd.DataFrame(X, columns=['credit', 'term', 'income'])\n",
    "y = pd.Series(y)\n",
    "\n",
    "# Train the Gradient Boosting regressor with decision trees as base learners\n",
    "reg = GradientBoostingClassification()\n",
    "reg.fit(X, y, depth=1, boosting_rounds=2)\n",
    "reg.predict(X)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c28e26739430500fec97d508cbac2e5d4a112deb445b412c4e69aa96f605479"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
