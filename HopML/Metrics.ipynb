{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "## Linear Models\n",
    "\n",
    "We need to check the assumptions of the linear model before using the model. This process is called regression diagnostics. We divide the potential problems into the following categories.\n",
    "\n",
    "**Error term:** $\\epsilon \\sim N(0, \\sigma_\\epsilon^2)$, iid.\n",
    "\n",
    "**Linear assumption:** $E(Y|X) = X\\beta$\n",
    "\n",
    "**Unusual observations:** these few observations might change the choice and fit of the model (example, large values can effect which model we deem as the \"best\"... but should these large values be included?)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "$$\\hbox{Accuracy} = \\frac{\\hbox{Number of correct predictions}}{\\hbox{Total number of predictions made}}$$\n",
    "\n",
    "### Confusion matrix\n",
    "\n",
    "\n",
    "$n = 165$\n",
    "\n",
    "$$\\begin{matrix}\n",
    "& \\hbox{Predicted No (-1)} & \\hbox{Predicted Yes (+1)}\\\\\n",
    "\\hbox{Actual No (-1)} & 50 \\hbox{(TN)} & 10 \\hbox{(FP)}\\\\\n",
    "\\hbox{Actual Yes (+1)} & 5 \\hbox{(FN)}& 100 \\hbox{(TP)}\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "**True positive (TP)**: cases in which we predicted YES and the actual output was YES.\n",
    "\n",
    "**True negative (TN)**: cases in which we predicted NO and the actual output was NO.\n",
    "\n",
    "**False positive (FP)**: cases in which we predicted YES and the actual output was NO.\n",
    "\n",
    "**False negative (FN)**: cases in which we predicted NO and the actual output was YES.\n",
    "\n",
    "$$Accuracy = \\frac{TP + TN}{\\hbox{Total sample}} = \\frac{100 + 50}{165} = 0.91$$\n",
    "\n",
    "**True positive rate (sensitivity)**\n",
    "\n",
    "$$TPR = \\frac{TP}{FN + TP}$$\n",
    "\n",
    "**True negative rate (specificity)**\n",
    "\n",
    "$$TNR = \\frac{TN}{FP + TN}$$\n",
    "\n",
    "**False positive rate (1-specificity)**\n",
    "\n",
    "$$FPR = \\frac{FP}{FP + TN}$$\n",
    "\n",
    "**Precision**\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "**Recall**\n",
    "\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "### Mean absolute error (MAE)\n",
    "\n",
    "$$MAE = \\frac{1}{N} \\sum_{i=1}^N |y_i - \\hat{y}_i|$$\n",
    "\n",
    "where $y_i$ are the actual value and $\\hat{y}_i$ are the predicted values.\n",
    "\n",
    "### Mean squared error (MSE)\n",
    "\n",
    "$$MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "It is easier to compute the gradient of $MSE$ than the $MAE$, so it is often preferred. Additionally, $MAE$ requires complicated linear programming tools to compute the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collinearity\n",
    "\n",
    "$(X^T X)_{2,2}$ where $X$ is centered by the means is equal to the $(n-1) \\times Var(X_1)$.\n",
    "\n",
    "$(X^T X)_{2,3}$ where $X$ is centered by the means is equal to the $(n-1) \\times Cov(X_1, X_2)$, which generates a 2x2 matrix. This 2x2 matrix is not full rank (it is of rank 1 in this case). It is also not invertible. It is ill-conditioned matrix. \n",
    "\n",
    "When inverse the matrix, $n$ should go to the denominator. \n",
    "\n",
    "The power is 1 - type2 error. The prob that a non-zero beta is detected to be non-zero.\n",
    "\n",
    "We use collinearity $VIF(\\hat{\\beta}_j)$\n",
    "\n",
    "Model selection uses regularized solution.\n",
    "\n",
    "Full rank when no collinearity.\n",
    "\n",
    "$X^T X$. if p > n then will not have a full rank matrix; we cannot define x s.t. \n",
    "\n",
    "\n",
    "$Var(\\hat{\\beta}) = (X^T X)^{-1} \\sigma_\\epsilon^2$\n",
    "\n",
    "## Multicollinearity\n",
    "\n",
    "Majority of variability in $X_p$ can be explained by the other predictors.\n",
    "\n",
    "So, build regression of predictor combinations (e.g. X1 vs X2 and X3,  X2 vs X1 and X3,  X3 vs X1 and X2) to find an $R^2$ score for each. If high $R^2$, then potential multicollinearity. This is captured in the variance inflation factor (VIF) score\n",
    "\n",
    "$$VIF(\\hat{\\beta}_j) = \\frac{1}{1 - R_j^2} \\geq 1$$\n",
    "\n",
    "$R_{ij} = 0.8$ means 80% of the variability has been explained by the others.\n",
    "\n",
    "## Metrics from the training dataset\n",
    "\n",
    "For $d$ features. As you increase the $d$, RSS will increase. But, $\\hat{\\sigma}_\\epsilon^2$ will decrease.\n",
    "\n",
    "Mallow's $C_p$: \n",
    "\n",
    "$$C_p = \\frac{1}{n} (RSS + 2 d \\hat{\\sigma}_\\epsilon^2)$$\n",
    "\n",
    "**AIC (smaller is better)**\n",
    "\n",
    "$$AIC = \\frac{1}{n \\hat{\\sigma}_\\epsilon^2} (RSS + 2 d \\hat{\\sigma}_\\epsilon^2)$$\n",
    "\n",
    "**BIC (smaller is better)**\n",
    "\n",
    "$$AIC = \\frac{1}{n \\hat{\\sigma}_\\epsilon^2} (RSS + 2 \\log(n) d \\hat{\\sigma}_\\epsilon^2)$$\n",
    "\n",
    "BIC tends to select the smaller model (in number of features), compared to AIC. \n",
    "\n",
    "### Model selection\n",
    "\n",
    "Forward selection may not always be able to find the best model. \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c28e26739430500fec97d508cbac2e5d4a112deb445b412c4e69aa96f605479"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
