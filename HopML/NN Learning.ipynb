{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Learning\n",
    "\n",
    "We can solve for gradients in a vectorized manner. \n",
    "\n",
    "A function $\\mathbf{f}: R^n \\xrightarrow{} R^m$ exists such that $\\mathbf{f}(\\mathbf{x}) = [f_1(x_1, ..., x_n), f_2(x_1, ..., x_n), ..., f_m(x_1, ..., x_n)]$. Its jacobian can be calculated as\n",
    "\n",
    "$$\\frac{d\\mathbf{f}}{d\\mathbf{x}} = \n",
    "    \\begin{bmatrix} \n",
    "    \\frac{df_1}{dx_1} & \\dots & \\frac{df_1}{dx_n} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{df_m}{dx_1} & \\dots  & \\frac{df_m}{dx_b} \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In other words, $(\\frac{d\\mathbf{f}}{d\\mathbf{x}})_{ij} = \\frac{df_i}{dx_j}$. This jacobian is useful for the application of chain rule to a vector-valued function just by multiplying jacobians.\n",
    "\n",
    "For example, suppose we have a function $\\mathbf{f}(x) = [f_1(x), f_2(x)]$ taking a scaler to a vector of size 2 and a function $\\mathbf{g}(\\mathbf{y}) = [g_1(y_1, y_2), g_2(y_1, y_2)]$ taking a vector of size two to a vector of size two. Now let's compose them to get $\\mathbf{g}(x) = [g_1(f_1(x), f_2(x)), g_2(f_1(x), f_2(x))]$. Using the regular chain rule, we can compute the derivative of $\\mathbf{g}$ as the Jacobian\n",
    "\n",
    "$$\\frac{d\\mathbf{g}}{dx} = \\begin{bmatrix}\n",
    "\\frac{d}{dx} g_1(f_1(x), f_2(x)) \\\\\n",
    "\\frac{d}{dx} g_2(f_1(x), f_2(x)) \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\frac{d g_1}{d f_1} \\frac{d f_1}{dx} + \\frac{d g_1}{d f_2} \\frac{d f_2}{dx} \\\\\n",
    "\\frac{d g_2}{d f_1} \\frac{d f_1}{dx} + \\frac{d g_2}{d f_2} \\frac{d f_2}{dx} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is the same as multiplying the two jacobians.\n",
    "\n",
    "$$\\frac{d\\mathbf{g}}{dx} = \\frac{d\\mathbf{g}}{d\\mathbf{f}} \\frac{d\\mathbf{f}}{dx} = \\begin{bmatrix}\n",
    "\\frac{d g_1}{d f_1} & \\frac{d g_1}{d f_2}\\\\\n",
    "\\frac{d g_2}{d f_1} & \\frac{d g_2}{d f_2}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{d f_1}{dx}\\\\\n",
    "\\frac{d f_2}{dx}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Useful Linear Algebra Identities\n",
    "\n",
    "**(1) Matrix times column vector wrt the column vector**\n",
    "\n",
    "$\\mathbf{z} = \\mathbf{W} \\mathbf{x}$, what is $\\frac{d\\mathbf{z}}{d\\mathbf{x}}$?\n",
    "\n",
    "Suppose $\\mathbf{W} \\in R^{n \\times m}$, then we can think of z as a function of \\mathbf{x} taking an $m$-dimensional vector to an $n$-dimensional vector. So, its Jacobian will be $n \\times m$. Note that\n",
    "\n",
    "$$z_i = \\sum_{k=1}^m W_{ik} x_{k}$$\n",
    "\n",
    "An entry $(\\frac{d\\mathbf{f}}{d\\mathbf{x}})_{ij}$ of the Jacobian will be\n",
    "\n",
    "$$(\\frac{d\\mathbf{z}}{d\\mathbf{x}})_{ij} = \\frac{d z_i}{d x_j} \\frac{d}{d x_j} \\sum_{k=1}^m W_{ik} x_k = \\sum_{k=1}^m W_{ik} \\frac{d}{d x_j} x_k = W_{ij}$$\n",
    "\n",
    "because $\\frac{d}{d x_j} x_k = 1$ if $k = j$ and 0 if otherwise. So, we see that $\\frac{d \\mathbf{z}}{d \\mathbf{x}} = \\mathbf{W}$.\n",
    "\n",
    "**(2) Row vector times matrix wrt the row vector**\n",
    "\n",
    "$\\mathbf{z} = \\mathbf{x} \\mathbf{W}$, what is $\\frac{d \\mathbf{z}}{d \\mathbf{x}}$?\n",
    "\n",
    "A computation similar to (1) shows that $\\frac{d \\mathbf{z}}{d \\mathbf{x}} = \\mathbf{W}^T$.\n",
    "\n",
    "**(3) A vector with itself**\n",
    "\n",
    "$\\mathbf{z} = \\mathbf{x}$, what is $\\frac{d \\mathbf{z}}{d \\mathbf{x}}$?\n",
    "\n",
    "We have $z_i = x_i$, so\n",
    "\n",
    "$$(\\frac{d\\mathbf{z}}{d\\mathbf{x}})_{ij} = \\frac{d z_i}{d x_j} = \\frac{d}{d x_j} x_i = \\begin{cases}\n",
    "  1 & i=j\\\\\n",
    "  0 & otherwise\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "So we see that the Jacobian $\\frac{d\\mathbf{z}}{d \\mathbf{x}}$ is a diagonal matrix where the entry at $(i, i)$ is 1. This is just the identity matrix: $\\frac{d\\mathbf{z}}{d \\mathbf{x}} = \\mathbf{I}$. When applying the chain rule, this term will disappear because a matrix or vector multiplied by the identity matrix does not change.\n",
    "\n",
    "**(4) An elementwise function applied a vector**\n",
    "\n",
    "$\\mathbf{z} = f(\\mathbf{x})$, what is $\\frac{d \\mathbf{z}}{d \\mathbf{x}}$?\n",
    "\n",
    "Since $f$ is being applied elementwise, we have $z_i = f(x_i)$. So, \n",
    "\n",
    "$$(\\frac{d\\mathbf{z}}{d\\mathbf{x}})_{ij} = \\frac{d z_i}{d x_j} = \\frac{d}{d x_j} f(x_i) = \\begin{cases}\n",
    "  f^\\prime (x_i) & i=j\\\\\n",
    "  0 & otherwise\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "So we see that the jacobian $\\frac{d\\mathbf{z}}{d\\mathbf{x}}$ is a diagonal matrix where the entry at $(i,i)$ is the derivative of $f$ applied to $x_i$. We can write this as $\\frac{d \\mathbf{z}}{d \\mathbf{x}} = diag( f^\\prime (\\mathbf{x}))$. Since multiplication by a diagonal matrix is the same as doing elementwise multiplication by the diagonal, we could also write $\\circ f^\\prime (\\mathbf{x})$ when applying the chain rule.\n",
    "\n",
    "**(5) Matrix times column vector wrt the matrix**\n",
    "\n",
    "$\\mathbf{z} = \\mathbf{W} \\mathbf{x}$, $\\mathbf{\\delta} = \\frac{d J}{d z}$ what is $\\frac{dJ}{d\\mathbf{W}} = \\frac{d J}{d \\mathbf{z}} \\frac{d \\mathbf{z}}{d \\mathbf{W}} = \\delta \\frac{d \\mathbf{z}}{d \\mathbf{W}}$?\n",
    "\n",
    "Suppose we ahve a loss function $J$ (a scalar) and are computing its gradient wrt a matrix $\\mathbf{W} \\in R^{n \\times m}$. Then we could think of $J$ as a function of $\\mathbf{W}$ taking $n m$ inputs (the entries of $\\mathbf{W}$) to a single output ($J$). This means the Jacobian $\\frac{d J}{d \\mathbf{W}}$ would be a $1 \\times nm$ vector. But in practice this is not a very useful way of arranging the gradient. It would be much nicer if hte derivatives were in a $n \\times m$ matrix like this: \n",
    "\n",
    "$$\\frac{d\\mathbf{J}}{d\\mathbf{W}} = \n",
    "\\begin{bmatrix} \n",
    "    \\frac{d J}{d W_{11}} & \\dots & \\frac{d J}{d W_{1 m}} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{d J}{d W_{n1}} & \\dots  & \\frac{d J}{d W_{nm}} \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Since this matrix has the same shape as $\\mathbf{W}$, we could just subtract it (times the learning rate) from $\\mathbf{W}$ when doing gradient descent. So (in a slight abuse of notation) let's find this matrix as $\\frac{d J}{d \\mathbf{W}}$ instead.\n",
    "\n",
    "This way of arranging the gradients becomes complicated when computing $\\frac{d \\mathbf{z}}{d \\mathbf{W}}$. Unlike $J$, $\\mathbf{z}$ is a vector. So if we are trying to rearrange the gradients like with $\\frac{d J}{d \\mathbf{W}}$, $\\frac{d \\mathbf{z}}{d \\mathbf{W}}$ would be an $n \\times m \\times n$ tensor! Luckily, we can avoid the issue by taking the gradient wrt a single weight $W_{ij}$ instead. $\\frac{d \\mathbf{z}}{d W_{ij}}$ is just a vector, which is much easier to deal with. We have \n",
    "\n",
    "$$z_k = \\sum_{l=1}^m W_{kl} x_l$$\n",
    "\n",
    "$$\\frac{d z_k}{d W_{ij}} = \\sum_{l=1}^m x_l \\frac{d}{d W_{ij}} W_{kl}$$\n",
    "\n",
    "Note that $\\frac{d}{d W_{ij}} W_{kl} = 1$ if $i=k$ and $j=l$ and 0 if otherwise.  So if $k \\neq i$ everything in the sum is zero and the gradient is zero. Otherwise, the only nonzero element of hte sum is when $l=j$, so we just get $x_j$. Thus we find $\\frac{d z_k}{d W_{ij}} = x_j$ if $k = i$ and 0 if otherwise. Another way of writing this is\n",
    "\n",
    "$$\\frac{d \\mathbf{z}}{d \\mathbf{W}_{ij}} = \\begin{bmatrix}\n",
    "0\\\\\n",
    "\\vdots \\\\\n",
    "0\\\\\n",
    "x_j\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Here, the $x_j$ is located in the $i$th element.\n",
    "\n",
    "Now let's compute $\\frac{d J}{d W_{ij}}$\n",
    "\n",
    "$$\\frac{d J}{d W_{ij}} = \\frac{d J}{d z} \\frac{dz}{d W_{ij}} = \\mathbf{\\delta} \\frac{d z}{d W_{ij}} = \\sum_{k=1}^m d_k \\frac{d z_k}{d W_{ij}} = \\delta_{i} x_j$$\n",
    "\n",
    "The only nonzero term in the sum is $\\delta_i \\frac{d z_i}{d W_{ij}}$.  To get $\\frac{d J}{d \\mathbf{W}}$ we want a matrix where entry $(i, j)$ is $\\delta_i x_j$. This matrix is equal to the outer product $\\frac{d J}{d \\mathbf{W}} = \\delta^T x^T$.\n",
    "\n",
    "**(6) Row vector time matrix wrt the matrix**\n",
    "\n",
    "$\\mathbf{z} = \\mathbf{x} \\mathbf{W}$, $\\mathbf{\\delta} = \\frac{d J}{d z}$ what is $\\mathbf{d J}{d \\mathbf{W}} = \\delta \\frac{d \\mathbf{z}}{d \\mathbf{W}}$?\n",
    "\n",
    "A similar computation to the one above shows $\\frac{d J}{d \\mathbf{W}} = \\mathbf{x}^T \\delta$.\n",
    "\n",
    "**(7) Cross-entropy loss wrt logits**\n",
    "\n",
    "$\\mathbf{\\hat{y}} = softmax(\\mathbf{\\theta}), J = CE(\\mathbf{y}, \\mathbf{\\hat{y}})$, what is $\\frac{d J}{d \\mathbf{\\theta}}$?\n",
    "\n",
    "The gradient is $\\frac{d J}{d \\mathbf{\\theta}} = \\mathbf{\\hat{y}} - \\mathbf{y}$, or $(\\mathbf{\\hat{y}} - \\mathbf{y})^T$ if $\\mathbf{y}$ is a column vector.\n",
    "\n",
    "These identities will be enough to let you quickly compute the gradients for many neural networks. \n",
    "\n",
    "## Gradient layout\n",
    "\n",
    "Jacobean formulation is great for applying the chain rule: simply multiply the Jacobians. However, when doing SGD it's more convenient to follow the convention \"the shape of the gradient equals the shape of the parameter\" (as done when computing $\\frac{d J}{d \\mathbf{W}}$) That way subtracting the gradient times the learning rate from the parameters is easy.\n",
    "\n",
    "## Example on 1-layer NN\n",
    "\n",
    "We compute the gradients of a full neural network with one-layer and cross-entropy (CE) loss.\n",
    "\n",
    "The forward pass is as follows:\n",
    "\n",
    "$$\\mathbf{x} = input$$\n",
    "\n",
    "$$\\mathbf{z} = \\mathbf{W} \\mathbf{x} + \\mathbf{b_1}$$\n",
    "\n",
    "$$\\mathbf{h} = ReLU(\\mathbf{z})$$\n",
    "\n",
    "$$\\mathbf{\\theta} = \\mathbf{U} \\mathbf{h} + \\mathbf{b_2}$$\n",
    "\n",
    "$$\\mathbf{\\hat{y}} = softmax(\\mathbf{\\theta})$$\n",
    "\n",
    "$$J = CE(\\mathbf{y}, \\mathbf{\\hat{y}})$$\n",
    "\n",
    "It helps to break up the model into the simplest parts possible, so note that we defined $\\mathbf{z}$ and $\\mathbf{\\theta}$ to split up the activation functions from the linear transformations in the network's layers. The dimensions of the model's parameters are\n",
    "\n",
    "$$\\mathbf{x} \\in R^{D_x \\times 1}$$\n",
    "\n",
    "$$\\mathbf{b_1} \\in R^{D_h \\times 1}$$\n",
    "\n",
    "$$\\mathbf{W} \\in R^{D_h \\times D_x}$$\n",
    "\n",
    "$$\\mathbf{b_2} \\in R^{N_c \\times 1}$$\n",
    "\n",
    "$$\\mathbf{U} \\in R^{N_c \\times D_h}$$\n",
    "\n",
    "where $D_x$ is the size of our input, $D_h$ is the size of our hidden layer, and $N_c$ is the number of classes.\n",
    "\n",
    "In this example, we will compute all of the network's gradients: $\\frac{d J}{d \\mathbf{U}}$, $\\frac{d J}{d \\mathbf{b_2}}$, $\\frac{d J}{d \\mathbf{W}}$, $\\frac{d J}{d \\mathbf{b_1}}$, $\\frac{d J}{d \\mathbf{x}}$\n",
    "\n",
    "To start with, recall that ReLU($x$) = max($x, 0$). This means\n",
    "\n",
    "$$ReLU^\\prime (x) = \\begin{cases}\n",
    "1 & x > 0\\\\\n",
    "0 & otherwise \\\\\n",
    "\\end{cases} = sgn(ReLU(x))$$\n",
    "\n",
    "where sgn is the signum function. Note that we are able to write the derivative of the activation in terms of the activation itself.\n",
    "\n",
    "Now let's write out the chain rule for $\\frac{d J}{d \\mathbf{U}}$ and $\\frac{d J}{d \\mathbf{b_2}}$:\n",
    "\n",
    "$$\\frac{d J}{d \\mathbf{U}} = \\frac{d J}{d \\mathbf{\\hat{y}}} \\frac{d \\mathbf{\\hat{y}}}{d \\mathbf{\\theta}} \\frac{d \\mathbf{\\theta}}{d \\mathbf{U}}$$\n",
    "\n",
    "$$\\frac{d J}{d \\mathbf{b_2}} = \\frac{d J}{d \\mathbf{\\hat{y}}} \\frac{d \\mathbf{\\hat{y}}}{d \\mathbf{\\theta}} \\frac{d \\mathbf{\\theta}}{d \\mathbf{b_2}}$$\n",
    "\n",
    "Notice that $\\frac{d J}{d \\mathbf{\\hat{y}}} \\frac{d \\mathbf{\\hat{y}}}{d \\mathbf{\\theta}} = \\frac{d J}{d \\mathbf{\\theta}}$ is present in both gradients. This makes the math a bit cumbersome. Even worse, if we're implementing the model without automatic differentiation, computing $\\frac{d J}{d \\mathbf{\\theta}}$ twice will be inefficient. So it will help us to define some variables to represent the intermediate derivatives:\n",
    "\n",
    "$$\\mathbf{\\delta_1} = \\frac{d J}{d \\mathbf{\\theta}}$$\n",
    "\n",
    "$$\\mathbf{\\delta_2} = \\frac{d J}{d \\mathbf{z}}$$\n",
    "\n",
    "These are the error signals passed down to $\\mathbf{\\theta}$ and $\\mathbf{z}$ when doing backpropagation. We can compute them as follows:\n",
    "\n",
    "Per cross-entropy loss wrt logits, \n",
    "\n",
    "$$\\mathbf{\\delta_1} = \\frac{d J}{d \\mathbf{\\theta}} = (\\mathbf{\\hat{y}} - \\mathbf{y})^T$$\n",
    "\n",
    "Using the chain rule, \n",
    "\n",
    "$$\\mathbf{\\delta_2} = \\frac{d J}{d \\mathbf{z}} = \\frac{d J}{d \\mathbf{\\theta}} \\frac{d \\mathbf{\\theta}}{d \\mathbf{h}} \\frac{d \\mathbf{h}}{d \\mathbf{z}}$$\n",
    "\n",
    "Substituting in $\\mathbf{\\delta_1}$, \n",
    "\n",
    "$$= \\mathbf{\\delta_1} \\frac{d \\mathbf{\\theta}}{d \\mathbf{h}} \\frac{d \\mathbf{h}}{d \\mathbf{z}}$$\n",
    "\n",
    "Using matrix times column vector wrt column vector,\n",
    "\n",
    "$$= \\mathbf{\\delta_1} \\mathbf{U} \\frac{d \\mathbf{h}}{d \\mathbf{z}}$$\n",
    "\n",
    "Using elementwise function applied to a vector,\n",
    "\n",
    "$$= \\mathbf{\\delta_1} \\mathbf{U} \\circ ReLU^\\prime (\\mathbf{z})$$\n",
    "\n",
    "$$= \\mathbf{\\delta_1} \\mathbf{U} \\circ sgn(\\mathbf{h})$$\n",
    "\n",
    "These final objects have the following sizes: $\\frac{d J}{d \\mathbf{z}}$ is ($1 \\times D_h$), $\\mathbf{\\delta_1}$ is ($1 \\times N_c$), $\\mathbf{U}$ is ($N_c \\times D_h$), and $sgn(\\mathbf{h})$ is ($D_h$, ).\n",
    "\n",
    "The error terms can be utilized to compute the gradients. \n",
    "\n",
    "Using the property that the matrix times column vector wrt matrix,\n",
    "\n",
    "$$\\frac{d J}{d \\mathbf{U}} = \\frac{d J}{d \\mathbf{\\theta}} \\frac{d \\mathbf{\\theta}}{d \\mathbf{U}} = \\mathbf{\\delta_1} \\frac{d \\mathbf{\\theta}}{d \\mathbf{U}} = \\mathbf{\\delta_1}^T \\mathbf{h}^T$$\n",
    "\n",
    "Using identity matrix and transposing\n",
    "\n",
    "$$\\frac{d J}{d \\mathbf{b_2}} = \\frac{d J}{d \\mathbf{\\theta}} \\frac{d \\mathbf{\\theta}}{d \\mathbf{b_2}} = \\mathbf{\\delta_1} \\frac{d \\mathbf{\\theta}}{d \\mathbf{b_2}} = \\mathbf{\\delta_1}^T$$\n",
    "\n",
    "Using the property that the matrix times column vector wrt matrix,\n",
    "\n",
    "$$\\frac{d J}{d \\mathbf{W}} = \\frac{d J}{d \\mathbf{\\theta}} \\frac{d \\mathbf{z}}{d \\mathbf{W}} = \\mathbf{\\delta_2} \\frac{d \\mathbf{z}}{d \\mathbf{W}} = \\mathbf{\\delta_2}^T \\mathbf{x}^T$$\n",
    "\n",
    "Using identity matrix and transposing\n",
    "\n",
    "$$\\frac{d J}{d \\mathbf{b_1}} = \\frac{d J}{d \\mathbf{\\theta}} \\frac{d \\mathbf{z}}{d \\mathbf{b_1}} = \\mathbf{\\delta_2} \\frac{d \\mathbf{z}}{d \\mathbf{b_1}} = \\mathbf{\\delta_2}^T$$\n",
    "\n",
    "Using matrix times column vector wrt column vector and transposing\n",
    "\n",
    "$$\\frac{d J}{d \\mathbf{x}} = \\frac{d J}{d \\mathbf{\\theta}} \\frac{d \\mathbf{z}}{d \\mathbf{x}} = ( \\mathbf{\\delta_2}^T \\mathbf{W})^T$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The below example shows the use of backpropagation on a linear regression problem. As a note, this method can be solved (easier, perhaps) with least squares linear algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, m = 0, error = 2.059703523518794\n",
      "Running...\n",
      "After 1000 iterations b = -3.2643663545390273, m = -1.3386872077957057, error = 0.11444715957515363\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy import *\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "\n",
    "        # Calculate gradient with partial derivatives\n",
    "        m_gradient += -x * (y - (m_current * x + b_current))\n",
    "        b_gradient += -(y - (m_current * x + b_current))\n",
    "\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m]\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "    return [b, m]\n",
    "\n",
    "def gen_data():\n",
    "    \n",
    "    n_samples = 1000\n",
    "    random_state = 170\n",
    "    X, y = make_blobs(n_samples=n_samples, random_state=random_state, centers=1)\n",
    "\n",
    "    # Anisotropicly distributed data to stretch the blobs\n",
    "    transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "    X_aniso = dot(X, transformation)\n",
    "    return X_aniso\n",
    "\n",
    "def run():\n",
    "    points = gen_data()\n",
    "\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_m = 0 # initial slope guess\n",
    "    num_iterations = 1000\n",
    "    print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))\n",
    "\n",
    "    plt.scatter(points[:,0], points[:,1])\n",
    "    plt.plot(points[:,0], m*points[:,0] + b)\n",
    "    plt.show()\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules of thumb for neural networks\n",
    "\n",
    "1. Batch normalization standardizes the inputs (or activations of a prior layer or inputs directly), offering some regularization effect and reducing generalization error, perhaps no longer requiring the use of dropout for regularization. It can halve the epochs or better. Do not use dropout and batch normalization at the same time.\n",
    "2. The number of hidden layers in a neural network $N_h = \\frac{N_s}{\\alpha \\times (N_i + N_o)}$ where $\\alpha$ is an arbitrary scaling factor (usually 2 through 10) which symbolizes the number of nonzero weights for each neuron (which is smaller when including dropout layers), $N_i$ is the number of input neurons, $N_o$ is the number of output neurons, and $N_s$ number of samples in a training data set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 209.95831298828125\n",
      "199 144.6602020263672\n",
      "299 100.70250701904297\n",
      "399 71.03519439697266\n",
      "499 50.97850799560547\n",
      "599 37.403133392333984\n",
      "699 28.206867218017578\n",
      "799 21.97317886352539\n",
      "899 17.745729446411133\n",
      "999 14.877889633178711\n",
      "1099 12.93176555633545\n",
      "1199 11.610918045043945\n",
      "1299 10.714245796203613\n",
      "1399 10.105476379394531\n",
      "1499 9.69210433959961\n",
      "1599 9.411376953125\n",
      "1699 9.220744132995605\n",
      "1799 9.091286659240723\n",
      "1899 9.003360748291016\n",
      "1999 8.943641662597656\n",
      "Result: y = 4.691534383205465e-10 + -2.208526849746704 * P3(2.9543581470115043e-10 + 0.2554861009120941 x)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For this example, we need\n",
    "# 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n",
    "# not too far from the correct result to ensure convergence.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # P3 using our custom autograd operation.\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:57:54) [MSC v.1924 64 bit (AMD64)]"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "8180df55a6c751d640a512e9acccb76e32a247f5c207f09e4b8101856c6a1078"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
