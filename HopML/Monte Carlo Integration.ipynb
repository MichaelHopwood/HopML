{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Monte Carlo Integration\r\n",
    "\r\n",
    "Ref: Chapter 6, Rizzo\r\n",
    "\r\n",
    "## Basic Monte Carlo Integration\r\n",
    "\r\n",
    "Sampling and integartion are important techniques of statistical inference problems. But, often, the target distributions are too complicated and/or the integrands are too complex or high dimensional for these problems to be solved using basic methods.\r\n",
    "\r\n",
    "Let $p(x)$ be a pdf defined on a sample space $X$. Let $h$ be a function mapping $X$ to $\\mathbf{R}$.\r\n",
    "\r\n",
    "The goal is to estimate $E[ h(X) ]$. Normally this is an integral computed by hand ($\\mu_p(h) = E[ h(x) ] = \\int h(x) p(x) dx$). However, we assume that this case is too complicated to solve that way. It depends on the structure of $h$ and $p$ which can be very difficult to calculate.\r\n",
    "\r\n",
    "In general, we will only assume that $\\mu_p (|h|) < \\infty$. In other words, we assume a finite first moment. \r\n",
    "\r\n",
    "The **monte carlo method (MCM)** is based on 2 of the most important results in probability theory:\r\n",
    "\r\n",
    "1. Central limit theorem, CLT ($X_n \\xrightarrow{} N(\\mu, \\sigma)$ as $n \\xrightarrow{} \\infty$)\r\n",
    "\r\n",
    "2. Law of large numbers, LLN ($\\bar{x}_n \\xrightarrow{} E [ x ]$ as $n \\xrightarrow{} \\infty$)\r\n",
    "\r\n",
    "The monte carlo method appeals to the LLN and estimates $ E[ h(X) ]$ by the sample mean of $h(x)$: \r\n",
    "\r\n",
    "$$\\bar{h(x)} = \\frac{1}{N} \\sum_{i=1}^N h(X_i)$$\r\n",
    "\r\n",
    "and $X_1, X_2, ...$ are iid with pdf $p$. This is simply the average.\r\n",
    "\r\n",
    "Recall LLN ensures that $\\bar{h(x)} \\xrightarrow{} E [ h(x) ]$ as $N \\xrightarrow{} \\infty$\r\n",
    "\r\n",
    "We can generate RV from $p$ using inverse method (see **Inverse methods** page). This will generate a set of values $h(x_i)$. With these, we can estimate\r\n",
    "\r\n",
    "$\\bar{h(x)} = \\frac{h(x_1) + h(x_2) + ... + h(x_n)}{N}$\r\n",
    "\r\n",
    "by the LLN. If the estimate is bad, we need a larger $N$.\r\n",
    "\r\n",
    "The confidence inteval can be defined for $E[ h(x)]$ by appealing to the CLT. The sample variance of $\\bar{h(x)}$ is\r\n",
    "\r\n",
    "$$Var(\\bar{h(x)}) = \\frac{\\hat{Var}(h(x))}{N} = \\frac{1}{N^2} \\sum_{i=1}^N (h(x_i) - \\bar{h(x)})^2$$\r\n",
    "\r\n",
    "Because (for instance) if $X \\sim (\\mu, \\sigma^2)$, $E[ \\bar{x} ] = \\mu$ and $Var(\\bar{x}) = \\frac{\\sigma^2}{n}$.\r\n",
    "\r\n",
    "The CLT tells us that the approximate distribution of $\\bar{h(x)}$ is approximately normal ($ N(E(h(x)), \\frac{var(h(x))}{N})$)\r\n",
    "\r\n",
    "Therefore, the ($1 - \\alpha$) CI for $E[ h(x)]$ is approximately \r\n",
    "\r\n",
    "$$\\bar{h(x)} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{Var}(h(x))}{N}}$$\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Example:** Estimate $E[ h(x)] = \\int_{-\\infty}^\\infty \\frac{1}{1 + \\exp (-x)} \\frac{1}{\\sqrt{ (2\\pi) (1.43)}} \\exp (\\frac{-(x - 1.5)^2}{2.86}) dx$ and give a 95% CI for $E[h(x)]$.\r\n",
    "\r\n",
    "Using $\\int h(x) p(x) dx$\r\n",
    "\r\n",
    "Note that $h(x) = \\frac{1}{1 + e^{-x}}$ and is the inverse of the logit function.\r\n",
    "\r\n",
    "$$p(x) = \\frac{1}{\\sqrt{(2\\pi)(1.43)}} \\exp (\\frac{-(x - 1.5)^2}{2.36})$$\r\n",
    "\r\n",
    "is the pdf of the $N(1.5, 1.43)$. To estimate this integral, \r\n",
    "\r\n",
    "Step 1: Generate $Z_1, Z_2, ..., Z_{n} \\sim N(1.5, 1.43)$ where $n$ is just a large number, say 10000.\r\n",
    "\r\n",
    "Step 2: Calculate $\\hat{I} = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{1 + e^{-Z_i}}$ as $n \\xrightarrow{} \\infty$, $\\hat{I} \\xrightarrow{} E[h(x)]$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "n = 1000\r\n",
    "\r\n",
    "# We can also estimate this using base numpy. See `inversion methods` page.\r\n",
    "g = np.random.normal(1.5,1.43,n)\r\n",
    "\r\n",
    "integral = (1 / n) * np.sum(1 / (1 + np.exp(g)))\r\n",
    "integral"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.2511457485713463"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the 95\\% CI for $E[h(X)]$, we will need an estimate of the variance. The corresponding variance is \r\n",
    "\r\n",
    "$$Var(\\hat{I}) = \\frac{1}{n} \\sum_{i=1}^n (\\frac{1}{1 + e^{-Z_i}} - \\hat{I})^2$$\r\n",
    "\r\n",
    "So, the 95\\% CI for $E[h(x)]$ is $\\hat{I} \\mp 1.96 \\sqrt{\\frac{Var(\\hat{I})}{n}}$\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculating arbitrary integrals\r\n",
    "\r\n",
    "This is a special case also known as \"sample mean\". or \"crude\" method.\r\n",
    "\r\n",
    "To calculate more general integrals, we generate a random variable from [a,b] and find summation.\r\n",
    "\r\n",
    "$$\\int_a^b h(x) dx = \\int_a^b \\frac{h(x)p(x)}{p(x)} dx = \\int_a^b f(x) p(x) dx$$\r\n",
    "\r\n",
    "where $f(x) = \\frac{h(x)}{p(x)}$. This re-writes the integral as an expectation against some density $f$, generates from that density, and looks at the sample mean as before.\r\n",
    "\r\n",
    "For definite integrals over a finite interval, the uniform distribution will suffice (but you may not get a good accuracy).\r\n",
    "\r\n",
    "To calculate $I = \\int_a^b h(x)dx$, we can proceed as\r\n",
    "\r\n",
    "$$I = \\int_a^b h(x) dx = (b-a)\\int_a^b h(x) \\frac{1}{b-a}dx = (b-a)\\int_a^b h(x) f_{unif}(a,b)dx $$\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Example:** Estimate $\\theta = \\int_0^2 e^{-x^2} dx$\r\n",
    "\r\n",
    "$\\theta$ can be rewritten as\r\n",
    "\r\n",
    "$$\\theta = (2 - 0) \\int_0^2 e^{-x^2} \\frac{1}{2 - 0} dx = 2 \\int_0^2 e^{-x^2} \\frac{1}{2} dx$$\r\n",
    "\r\n",
    "Step 1: Sample $U_1, ... U_n \\sim Unif(0,2)$\r\n",
    "\r\n",
    "Step 2: Estimate $\\theta$ by $\\hat{\\theta} = \\frac{2}{n} \\sum_{i=1}^n e^{-U_i^2}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import numpy as np\r\n",
    "n = 1000\r\n",
    "u = np.random.uniform(0,2,n)\r\n",
    "\r\n",
    "theta = (2/n) * np.sum(np.exp(-1 * u**2))\r\n",
    "theta"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8659644587536548"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " **Example** Estimate $\\theta = \\int_0^{2\\pi} \\sin(x \\cos(x)) dx$\r\n",
    " \r\n",
    "  $$\\theta = (2\\pi - 0) \\int_0^{2\\pi} \\sin(x \\cos(x)) \\frac{1}{2\\pi - 0} dx = 2\\pi \\int_0^{2\\pi} \\sin(x \\cos(x)) \\frac{1}{2 \\pi} dx$$\r\n",
    "\r\n",
    "  Step 1: Generate $unif(0, 2 \\pi)$\r\n",
    "\r\n",
    "  Step 2: Build $\\frac{2\\pi}{n} \\sum_{i=1}^n sin(U_i cos(U_i))$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import numpy as np\r\n",
    "n = 1000000\r\n",
    "u = np.random.uniform(0,2*np.pi,n)\r\n",
    "theta = (2*np.pi/n) * np.sum(np.sin(u * np.cos(u)))\r\n",
    "theta"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-1.0438478105216198"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Example:** What about $\\int_{-\\infty}^\\infty \\sin(x \\cos(x))dx$?\r\n",
    "\r\n",
    "Here, the uniform approx won't work because we cannot depend on an evaluation of $b - a$.\r\n",
    "\r\n",
    "The way to solve this is discussed below.\r\n",
    "\r\n",
    "## Infinite integrals\r\n",
    "\r\n",
    "Use any distribution defined on the real line and do a similar approximation.\r\n",
    "So, rewrite the integral as \r\n",
    "\r\n",
    "$$\\int_{-\\infty}^{\\infty} h(x) dx = \\int_{-\\infty}^{\\infty} \\frac{h(x) p(x)}{p(x)} dx$$\r\n",
    "\r\n",
    "where $p(x)$ is the pdf of a distribution defined over $\\mathbf{R}$, and $h$ in any function.\r\n",
    "\r\n",
    "Then, generate $X_i$ from $p(x)$ and estimate the integral $\\int_{-\\infty}^\\infty h(x) dx$ by using \r\n",
    "\r\n",
    "$$\\frac{1}{n} \\sum_{i=1}^n \\frac{h(x_i)}{p(x_i)} $$\r\n",
    "\r\n",
    "per the problem earlier.\r\n",
    "\r\n",
    "**Example** Back to the problem from earlier: compute $\\int_{-\\infty}^\\infty \\sin(x \\cos(x))dx$?\r\n",
    "\r\n",
    "The standard normal is supported over $\\mathbf{R}$. So, use the standard normal pdf for the estimation.\r\n",
    "\r\n",
    "Step 1: Generate $X_1, ..., X_n \\sim N(0,1)$\r\n",
    "\r\n",
    "Step 2: Calcualte the average, $\\frac{1}{n} \\sum_{i=1}^n \\frac{\\sin (x_i \\cos(x_i))}{\\phi(x_i)}$\r\n",
    "\r\n",
    "where $\\phi(.)$ is the pdf of the standard normal."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "n=1000\r\n",
    "g = np.random.normal(0,1,n)\r\n",
    "\r\n",
    "def gaussian_pdf(x):\r\n",
    "    sigm = 1\r\n",
    "    mu = 0\r\n",
    "    return (1 / np.sqrt(2 * np.pi * sigm**2)) * np.exp(-1 * (x - mu)**2 / (2 * sigm**2))\r\n",
    "\r\n",
    "np.sum( np.sin(g * np.cos(g)) / gaussian_pdf(g))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-20.41176776122647"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Higher-dimensional integrals\r\n",
    "\r\n",
    "Monte carlo methods become particularly attractive when we consider integration in higher dimensions\r\n",
    "\r\n",
    "$$I = \\int_0^1 \\int_0^1 \\int_0^1 f(x, y, z) dx dy dz$$\r\n",
    "\r\n",
    "$I$ can be estimated by\r\n",
    "\r\n",
    "$\\hat{I} = \\frac{1}{n} \\sum_{i=1}^n f(x_i, y_i, z_i)$ where $(x_i, y_i, z_i)$ is a random sequence of points in the unit cube $[0, 1] \\times [0, 1] \\times [0, 1]$.\r\n",
    "For example,  $X_i \\sim Unif(0,1), Y_i \\sim Unif(0,1), Z_i \\sim Unif(0,1)$.\r\n",
    "\r\n",
    "In total, we need $3 n$ random numbers in order to generate the $N$ random points.\r\n",
    "\r\n",
    "**Example:** $I = \\int_0^1 \\int_0^1 x y \\exp (- x^2 y) dx dy$\r\n",
    "\r\n",
    "Here, $f(x,y) = x y \\exp (- x^2 y)$. So, \r\n",
    "\r\n",
    "$$\\hat{I} = \\frac{1}{n} \\sum_{i=1}^n x_i y_i \\exp (- x_i^2 y_i)$$\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "n = 100000\r\n",
    "u = np.random.uniform(0,1,n)\r\n",
    "u2 = np.random.uniform(0,1,n)\r\n",
    "\r\n",
    "approx = (1 / n) * np.sum(u * u2 * np.exp(- u * u2**2))\r\n",
    "real = 1 / (2 * np.e)\r\n",
    "\r\n",
    "approx, real"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.18458428310422237, 0.18393972058572117)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recall that the exact value of $I$ is $I = \\frac{1}{2e}$ We see a close approximation."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "d68cb6e2fa6c6235e1c177245b5b978609c8de5c8598db0334f33922eb56994c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}