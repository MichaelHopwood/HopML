{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Summary\n",
    "\n",
    "A logistic regression model tries to model $P(Y|X)$. We use a logit transformation to make sure the $P(Y|X)$ is between 0 and 1. The linear function (used inside logit) is trained via maximum likelihood estimation.\n",
    "\n",
    "\n",
    "## Detailed summary\n",
    "\n",
    "$$\\log(\\frac{p(X)}{1 - p(X)}) = \\beta_0 + \\beta_1 X$$\n",
    "\n",
    "As $p(X)$ increases, the $\\frac{p(X)}{1 - p(X)}$ will increase monotonically. \n",
    "As $p(x)$ increases, the $\\beta_0 + \\beta_1 x$ increases monotonically.\n",
    "$p(x)$ is always between 0 and 1.\n",
    "\n",
    "We train this via MLE: describe the distribution of observing $Y | X$ where $$\\begin{cases} 0 & \\hbox{with probability } 1 - p(x)\\\\ 1 & \\hbox{with probability } p(x) \\\\\\end{cases}$$\n",
    "\n",
    "where $P(Y=1) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}} = p(x)$.\n",
    "\n",
    "If $Y_i \\sim Bernuoilli(p(X_i))$, then $p(Y=k) = p^k(x) (1 - p(x))^{1 - k}$\n",
    "\n",
    "Given each sample is independent, then the likelihood is defined as the product (for all samples) of $p(Y=k)$\n",
    "\n",
    "$$L(\\hat{\\beta}) = \\Pi_{i=1}^n p(y_i = k) = \\Pi_{i=1}^n \\{ p(x_i)^y_i [1 - p(x_i)]^{1 - y_i} \\} = \\Pi_{i: y_i = 1} p(x_i) \\Pi_{i: y_i = 0} [1 - p(x_i)]$$\n",
    "\n",
    "From here, we can solve normally by taking log-likelihood and derivative equal to zero.\n",
    "\n",
    "The maximum likelihood is an optimization which allows us to solve $\\vec{\\beta}$.\n",
    "\n",
    "The coefficient describes the effect of parameter $X$ on the log-odds of the class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_k(X) = Pr(Y=k|X) = \\frac{e^{\\beta_k^T X}}{\\sum_{j=1}^K e^{\\beta_k^T X}}$\n",
    "\n",
    "Let $p_k(x) = e^{\\beta_k^T x}$\n",
    "\n",
    "The log odds of class 2 and 3 can be found by $\\beta_k^\\star = \\beta_k - \\beta_K$. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(X):\n",
    "    return np.exp(X) / np.sum(np.exp(X), axis=0)\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.01, num_iters=100):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        pass\n",
    "\n",
    "\n",
    "def logistic_regression():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Reweighted Least Squares (IRLS)\n",
    "\n",
    "### Background Ideas\n",
    "\n",
    "In a generalized linear model (glm), the idea is to relate a general response variable $y_i$ with a set of covariates, in order to get a predictive model similar to the one provided by simple regression.\n",
    "\n",
    "Assume $n$ observations of a response variable $y_1, y_2, ..., y_n$ and $k$ explanatory variables $x_1, x_2, ..., x_k$. with unknown parameters $\\beta_0, \\beta_1, ..., \\beta_k$. \n",
    "\n",
    "3 parts for the GLM:\n",
    "\n",
    "1. Random component ($y_i$) -> distribution of $y_i$\n",
    "\n",
    "2. Systematic component - explanatory variable form a linear predictor $\\beta_0 + \\beta_1 x_1 + ... + \\beta_k x_k$\n",
    "\n",
    "3. Link between random component and systematic components.\n",
    "\n",
    "**Example 1:** In linear regression, $y_i \\sim N(\\mu, \\sigma^2)$, where $\\mu = E[Y_i]$ (which is the random component), and we have systematic component which is $\\beta_0 + \\beta_1 x_1 + ... + \\beta_k x_k$. In linear regression, we set the random component and systematic component equal. Therefore, the link function is the identity function.\n",
    "\n",
    "**Example 2:** Let's say $y_i \\sim Ber(\\pi_i)$, so $E[y_i] = \\pi_i$ and we know $\\pi_i \\in [0, 1]$. This is the random component. The systematic component again is $\\beta_0 + \\beta_1 x_1 + ... + \\beta_k x_k$. However, we cannot set these equal (aka, use identity as link function) because linear regression does not constrict the domain to $[0, 1]$. Instead, we use the logit link function where\n",
    "\n",
    "$$logit(\\pi) = \\log(\\frac{\\pi}{1 - \\pi})$$\n",
    "\n",
    "This is the log odds of success. Now, assume that $y_i | z_i \\sim_{iid} Ber(\\pi_i), i=1, ..., n$ where $\\pi_i$ is related to the set of covariates $z_i$ by the logit link function, i.e.\n",
    "\n",
    "$$logit (\\pi_i) = \\log(\\frac{\\pi_i}{1 - \\pi_i}) = \\vec{Z}_i^\\prime \\vec{\\beta}$$\n",
    "\n",
    "For simplicity, we may assume that  $\\vec{z}_i = (1, z_i)^\\prime$ and $\\vec{\\beta} = (\\beta_0, \\beta_1)^\\prime$. So, the link function relates the natural parameter of the Bernuolli as a member of the exponential family with the set of covariates.\n",
    "\n",
    "The log likelihood is $$l(\\vec{\\beta}) = \\vec{y}^\\prime \\vec{z} \\vec{\\beta} - \\vec{b}^\\prime \\vec{1}$$ where $\\vec{1}$ is a vector of ones, $\\vec{y} = (y_1, y_2, ..., y_n)^\\prime$, $\\vec{z}$ is the $n \\times 2$ matrix whose $i$th row is $z_i^\\prime$ and $b = \\{ -\\log(1 - \\pi_i) \\}^n_{i=1}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROVE THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use the Newton's method to find the MLE $\\hat{\\beta}$ which maximizes the likelihood.\n",
    "\n",
    "To do this, we need to find the first and second derivatives.\n",
    "\n",
    "The score function is $$l^\\prime(\\beta) = z^\\prime(y - \\vec{\\pi})$$ where $\\vec{\\pi}$ is the column vector of the Bernuolli probability $\\pi_i$.\n",
    "\n",
    "The Hessian matrix is given by $$l^{\\prime \\prime}(\\beta) = \\frac{d}{d \\beta}( z^\\prime (y - \\pi)) = - \\vec{z}^\\prime \\vec{w} \\vec{z}$$ where $\\vec{w}$ is the diagonal matrix with $i$th diagonal entry $\\pi_i (1 - \\pi_i)$. \n",
    "\n",
    "With these, we apply Newton-Raphson method (see root-finding page).\n",
    "\n",
    "The Newton's update is given by:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\beta^{(t+1)} &= \\beta^{(t)} - [l^{\\prime \\prime} (\\beta^{(t)}) ]^{-1} l^\\prime(\\beta^{(t)})\\\\\n",
    "&= \\beta^{(t)} + (Z^\\prime w^{(t)} Z)^{-1} (Z^\\prime (y - \\pi^{(t)}))\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\pi^{(t)}$ is the value of $\\pi$ corresponding to $\\beta^{(t)}$ and $w^{(t)}$ is evaluated at $\\pi^{(t)}$.\n",
    "\n",
    "As a reminder, OLS would find that $\\hat{\\beta} = (Z^\\prime Z))^{-1} Z^\\prime y$. This shares the same structure as our Newton raphson update.\n",
    "\n",
    "**Remarks**\n",
    "\n",
    "1. From the update formula, it follows that the problem of finding MLE in a GLM framework reduces to a repeated weighted least squares applications in which the inverse of the diagonal values of $W$ are the appropriate weights.\n",
    "\n",
    "2. Since the Hessian does not depend on the data, the expected Fisher information matrix is equal to the observed Fisher information matrix. (TODO: verify whether this is true)\n",
    "\n",
    "3. IRLS can be slow and unreliable unless the model fits the data well.\n",
    "\n",
    "**Example:** Example 2.5 on page 37 (G + H)\n",
    "\n",
    "irls.R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do exercise."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c28e26739430500fec97d508cbac2e5d4a112deb445b412c4e69aa96f605479"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
