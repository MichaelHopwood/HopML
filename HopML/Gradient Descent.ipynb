{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Suppose we want to solve $\\min F(w)$ or $\\min_w E_{in} (w) \\quad (F(w) = \\frac{1}{n} \\sum_{i=1}^n f_i (w) )$. $E_{in}(w)$ represents the in-sample loss (or training loss). In general $E_{in}(w) = \\frac{1}{N} \\sum_{i=1}^N f_i(w)$. In machine learning, we want to find the hypothesis that minimizes $E_{in}(w). \n",
    "\n",
    "<u>Convex functions</u>: \n",
    "\n",
    "$\\nabla F(w^\\star) = 0 \\Longleftrightarrow w^\\star$ is a global minimum. A function is convex if $\\nabla^2 F[w]$ is positive definite. Ex. linear regression, logistic regression, ...\n",
    "\n",
    "<u>Non-convex functions</u>: \n",
    "\n",
    "$\\nabla F(w^\\star) = 0 \\Longleftrightarrow w^\\star$ is global minimum, local minimum, OR saddle point.\n",
    "\n",
    "Most algorithms only converge to gradient = 0\n",
    "\n",
    "For example, **Neural Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=835x344 at 0x1540909BDC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import disp\n",
    "disp('nn_gradient_basic.drawio.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent update rule can be written in terms of the gradient. The original GD update is the same as the one of the steepest descent. \n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\alpha^{(t)} \\nabla F (w^{(t)})$$\n",
    "\n",
    "where $\\alpha^{(t)}$ is called the step size.\n",
    "\n",
    "<u>Step size</u>: several methods are available to choose the step size.\n",
    "\n",
    "i). Fixed step size: $\\alpha^{(t)} = \\alpha, \\forall t$\n",
    "\n",
    "ii). Exact line search: $\\alpha^{(t)} = arg\\min_{\\alpha} F(w^{(t)} + \\alpha d^{(t)})$ where $d^{(t)} = - \\nabla F(w^{(t)})$.\n",
    "\n",
    "iii). Inexact line search - (See Amijo/Wolfe conditions)\n",
    "\n",
    "In machine learning, we use option (i). So, the GD update rule becomes $w^{t+1} = w^{(t)} - \\alpha F( w^{(t)} )$ where $\\alpha > 0$ is the step size (or learning rate). In general, $\\alpha$ is one of the most important hyperparameter of a learning algorithm. It's very important to tune it. $\\alpha$ too large can cause instability, causing divergence. $\\alpha$ too small can slow progress, making it too slow to converge. Most importantly, one tries a bunch of values and pick the one which works the best.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Initialize $w \\xrightarrow{} w^{(0)}$\n",
    "\n",
    "2. For $t=1, 2, ...$ do\n",
    "\n",
    "    i. Compute $\\nabla F(w^{(t)})$\n",
    "\n",
    "    ii. Update $w^{(t+1)} = w^{(t)} - \\alpha \\nabla F ( w^{(t)})$\n",
    "\n",
    "3. return $w^{(t)}$\n",
    "\n",
    "\n",
    "### Convergence\n",
    "\n",
    "Let $X^\\star$ be the global minimizer. Assume the following:\n",
    "\n",
    "i. $f$ is twice differentiable so that $\\nabla^2 f$ exists\n",
    "\n",
    "ii. $0 \\leq \\lambda \\min I \\leq \\nabla^2 f(x) \\leq \\lambda \\max I$ for all $X \\in \\mathcal{R}^n$.\n",
    "\n",
    "iii. Run GD with exact line search\n",
    "\n",
    "Then, $f(X^{(t)}) \\xrightarrow{} f(X^\\star)$ as $t \\xrightarrow{} \\infty$\n",
    "\n",
    "Proof is in Nocedal-Wright, Ch.3, Thm 3.3.\n",
    "\n",
    "### Advice on Gradient Descent\n",
    "\n",
    "GD is useful because 1) it is simple to implement, and 2) low computational cost per iteration (no matrix inversion). It requires only first order derivative (i.e. no hessian).\n",
    "\n",
    "Most machine learning has built-in (stochastic) gradient descent. When building one, be aware of following:\n",
    "\n",
    "1) Convex non-differentiable problem (L1-norm)\n",
    "\n",
    "2) Non-convex problem, e.g. ReLU in deep network\n",
    "\n",
    "3) Trap by local minima\n",
    "\n",
    "4) Inappropriate step size\n",
    "\n",
    "**Example**: Consider the problem of minimizing $f(x,y) = 4x^2 - 4xy + 2y^2$ using the gradient descent method. \n",
    "\n",
    "**Solution**: We knew that the solution is $(0,0)$. First, let's compute the gradient\n",
    "\n",
    "$$\\nabla f(\\underset{\\sim}{X}) - \\nabla f(X, y) = \\begin{pmatrix}\n",
    "\\frac{df}{dx}\\\\\n",
    "\\frac{df}{dy}\\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "8x-4y\\\\\n",
    "-4x+4y\\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Starting from initial point,\n",
    "\n",
    "$$\\underset{\\sim}{X}^{(0)} = (X^{(0)}, y^{(0)}) = (2,3)$$\n",
    "\n",
    "We want to find the next point $\\underset{\\sim}{X}^{(1)} = \\underset{\\sim}{X}^{(0)} - \\alpha \\nabla f(\\underset{\\sim}{X}^{(0)})$. Assuming that $\\alpha = \\frac{1}{2}$, then \n",
    "\n",
    "$$\\underset{\\sim}{X}^{(1)} = (X^{(1)}, y^{(1)}) = \\begin{pmatrix}2\\\\3\\\\\\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix}4\\\\4\\\\\\end{pmatrix} = \\begin{pmatrix}0\\\\1\\\\\\end{pmatrix}$$\n",
    "\n",
    "*Check for convergence:* If not convergence, continue to $\\underset{\\sim}{X}^{(2)}$ and repeat the process until convergence.\n",
    "\n",
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "In machine learning, our cost loss function general consists of the average of costs or losses for individual training samples. This means that most loss functions in ML problems are separable\n",
    "\n",
    "$$J(\\underset{\\sim}{\\theta}) = \\frac{1}{N} \\sum_{n=1}^N \\mathcal{L} (g_\\theta (X^n), y^{(n)}) = \\frac{1}{N} \\sum_{n=1}^N J_n (\\underset{\\sim}{\\theta})$$\n",
    "\n",
    "**Example**: \n",
    "\n",
    "i. Square loss\n",
    "\n",
    "$$J(\\underset{\\sim}{\\theta}) = \\frac{1}{N} \\sum_{n=1}^N (g_\\theta (X^n) - y^{n})^2$$\n",
    "\n",
    "ii. Cross-entropy loss\n",
    "\n",
    "$$J(\\underset{\\sim}{\\theta}) = - \\sum_{n=1}^N \\Bigl\\{ y^n \\log g_\\theta (X^n) + (1 - y^n) \\log (1 - g_\\theta (X^n)) \\Bigr\\}$$\n",
    "\n",
    "iii. Logistic loss\n",
    "\n",
    "$$J(\\underset{\\sim}{\\theta}) = \\sum_{n=1}^N \\log (1 + \\exp (-y^{n \\theta^T X^n}) )$$\n",
    "\n",
    "Recall for GD, $\\underset{\\sim}{\\theta}^{(t+1)} = \\theta^{(t)} - \\alpha \\nabla J(\\theta^{(t)})$. The main computation is $\\nabla J(\\theta^{(t)})$. By linearity of derivatives, the gradient is the average of all the gradients for individual examples:\n",
    "\n",
    "$$\\nabla J(\\theta) = \\frac{1}{N} \\sum_{n=1}^N \\nabla J_n (\\theta) \\quad \\hbox{is the full gradient of the loss}$$\n",
    "\n",
    "This means that if we use this formula directly, we must visit every training example to compute the gradient. This is known as batch training since we are treating the entire training set as a batch But this can be very time-consuming and it is also unnecessary. We can get a stochatic estimate of the gradient from a single training sample. Instead of computing the sum of all gradients, SGD selects an observation uniformly at random, say $n$, and uses $\\nabla J_n (\\theta)$ as an estimator for $\\nabla J(\\theta) \\quad ( \\nabla J(\\theta) = \\frac{1}{N} \\sum_{n=1}^N \\nabla J_n (\\theta) )$.\n",
    "\n",
    "$$\\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\nabla J_n (\\theta^{(t)})$$\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "1. Initialize $\\theta \\xrightarrow{} \\theta^{(0)} = 0$ or randomly\n",
    "\n",
    "2. For $t = 1,2,3,...$, \n",
    "\n",
    "    i. Sample 1 observation $n$ uniformly at random\n",
    "\n",
    "    ii. Update $\\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\nabla J_n (\\theta^{(t)})$\n",
    "\n",
    "In practice, we don't compute the gradient on a single example, but rather compute an average overa a batch of $B$ training examples. Recall the full graident of hte loss\n",
    "\n",
    "$$\\nabla J(\\theta) = \\frac{1}{N} \\sum_{n=1}^N \\nabla J_n (\\theta)$$\n",
    "\n",
    "For SGD (mini-batch), $\\nabla J(\\theta) \\approx \\frac{1}{B} \\sum_{n\\in B} \\nabla J_n (\\theta)$ where $B \\leq \\{ 1, 2, ..., N \\}$ is a random subset and $|B|$ is the batch size.\n",
    "\n",
    "## Mini-batch SGD Algorithm\n",
    "\n",
    "1. Initialize $\\theta \\xrightarrow{} \\theta^{(0)} = 0$ or randomly\n",
    "\n",
    "2. For $t=1,2,3,...$,\n",
    "\n",
    "    i. Draw a random subset $B \\leq \\{ 1, 2, ..., N \\}$\n",
    "\n",
    "    ii. Update $\\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\frac{1}{|B|} \\sum_{n \\in B} \\nabla J_n (\\theta^{(t)})$\n",
    "\n",
    "\n",
    "**The approximate gradient is unbiased:**\n",
    "\n",
    "$$E[ \\frac{1}{|B|} \\sum_{n\\in B} \\nabla J_n (\\theta) ] = \\nabla J(\\theta)$$\n",
    "\n",
    "If all operations were equally expensive, one would always prefer to use $B=1$. We don't want to make $B$ too large, because then it takes too long to compute the gradients. In the extreme case where $B=N$, we get back the batch gradient descent.\n",
    "\n",
    "### Interpreting SGD\n",
    "\n",
    "Recall the SGD step is unbiased. Unbiased gradient implies that each update is gradient + zero-mean noise. \n",
    "\n",
    "Step size: SGD with constant size <u>does not converge</u>. If $\\theta^\\star$ is a minimizer, then $J(\\theta^\\star) = \\frac{1}{N} \\sum_{n=1}^N J_n (\\theta^\\star) = 0$.\n",
    "\n",
    "But, $\\frac{1}{|B|} \\sum_{n \\in B} J_n (\\theta^\\star) \\neq 0$, since $B$ is a subset. **Typical strategy:** start with a large step size and gradually decrease $\\alpha^t \\xrightarrow{} 0$ (e.g. $\\alpha^t = t^{-a}$) for some constant $a$.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Compared to GD in convex problems, SGD offers as trade-off between accuracy and efficiency. It involves more iterations, less gradient evaluation per iteration.\n",
    "\n",
    "**Recent studies** of SGD for non-convex problems found that: \n",
    "\n",
    "* SGD for training deep neural networks works\n",
    "\n",
    "* SGD finds solution faster\n",
    "\n",
    "* SGD finds a better local minima\n",
    "\n",
    "* Noise matters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Find the gradient for this loss function, $\\nabla J(\\theta)$ using following information\n",
    "\n",
    "$$h_\\theta (x) = \\theta^T X$$\n",
    "\n",
    "$$\\underset{\\sim}{\\theta} = (\\theta_0, \\theta_1, ..., \\theta_n) \\qquad \\underset{\\sim}{X} = (1, X_1, ..., X_n)$$\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2} \\sum_{i=1}^n (h_\\theta (X^{(i)} - y^{(i)}))^2 \\xrightarrow{} SSE$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8180df55a6c751d640a512e9acccb76e32a247f5c207f09e4b8101856c6a1078"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
