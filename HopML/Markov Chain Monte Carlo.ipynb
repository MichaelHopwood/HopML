{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Monte Carlo\n",
    "\n",
    "Used when you do not have independence in samples. The **markov property** looks at the most previous sample to help inform the next sample!\n",
    "\n",
    "This is a stochastic process. Other processes like this are q-theory, brownian motion, and poisson process.\n",
    "\n",
    "Monte carlo uses these simulated R.V.s to approximate integrals, etc. but the R.V. don't need to be independent in order to approximate integrals. MCMC constructs a dependent sequence of RV that can be used to approximate the integrals like the ordinary MC. The advantages of introducing this dependence is that very general \"black box\" algorithms (and corresponding theory) are available to perform the required simulations. This page will discuss some basics of Markov chains and MCMC but know that there are very important unanswered questions about how and when MCMC works.\n",
    "\n",
    "## Definition\n",
    "\n",
    "A markov chain is just a sequence of R.V. $\\{ x_1, x_2, ... \\}$ with a specific type of dependence structure. \n",
    "In particular, a Markov chain satisfies \n",
    "\n",
    "$$P(X_{n+1} \\in B | X_1, ..., X_{n-1}, X_n) = P(X_{n+1} \\in B | X_n)$$\n",
    "\n",
    "where $X_{n+1}$ is the cuture, $X_1, ..., X_{n-1}$ is the past, and $X_n$ is the present.\n",
    "Therefore, this property states that the future is only dependent on the present.\n",
    "This is called the *markov property*.\n",
    "\n",
    "Independence is a trivial Markov Chain.\n",
    "\n",
    "From the markov property, we can argue that the probabilistic properties of the chain are completely determined by \n",
    "\n",
    "i. initial distribution for $X_0$\n",
    "\n",
    "ii. the transition distribution, i.e. distribution of $X_{n+1}$ given $X_n$\n",
    "\n",
    "Note: Assume that the markov chain is homogeneous (aka, the transition distribution does not depend on $n$). \n",
    "\n",
    "Example: **simple random walk**\n",
    "Let $v_1, v_2, ...$ be iid $\\sim Unif(-1,1)$\n",
    "\n",
    "Set $x_0 = 0$ and $X_n = \\sum_{i=1}^n U_i = X_{n-1} + U_n$.\n",
    "The initial distribution is $P(X_0 = 0) = 1$.\n",
    "The transition distribution is determined by $$x_n = \\begin{cases} \n",
    "x_{n-1}-1 & prob.= 1/2\\\\\n",
    "x_{n-1}+1 & prob.= 1/2\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "While very simple, the random walk is an important example in probability theory, having connections to advanced things like Brownian Motion. In some conditions, random walk becomes brownian motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {1: 1.0},\n",
       " 1: {2: 1.0},\n",
       " 2: {3: 1.0},\n",
       " 3: {4: 1.0},\n",
       " 4: {5: 1.0},\n",
       " 5: {6: 1.0},\n",
       " 6: {7: 1.0},\n",
       " 7: {8: 1.0},\n",
       " 8: {9: 1.0},\n",
       " 9: {0: 0.99, 3: 0.01}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Task: predict next number\n",
    "\n",
    "# Sample set 1: \n",
    "X = np.array([0,1,2,3,4,5,6,7,8,9]*100 + [3])\n",
    "\n",
    "lag=1\n",
    "\n",
    "total_count_dict = {}\n",
    "dependent_count_dict = {}\n",
    "for i in range(len(X)-lag):\n",
    "\n",
    "    if X[i] not in total_count_dict:\n",
    "        total_count_dict[X[i]] = 0\n",
    "    total_count_dict[X[i]] += 1\n",
    "    \n",
    "    if X[i] not in dependent_count_dict:\n",
    "        dependent_count_dict[X[i]] = {}\n",
    "    if X[i+lag] not in dependent_count_dict[X[i]]:\n",
    "        dependent_count_dict[X[i]][X[i+lag]] = 0\n",
    "    dependent_count_dict[X[i]][X[i+lag]] += 1\n",
    "\n",
    "# Normalize each value in dependent_count_dict by total_count_dict\n",
    "for k,d in dependent_count_dict.items():\n",
    "    for key in d.keys():\n",
    "        d[key] /= total_count_dict[k]\n",
    "\n",
    "print(\"Transition Matrix:\")\n",
    "dependent_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brownian Motion\n",
    "\n",
    "Multiple plays - i.e. Gambler's ruin problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete time Markov Chain (DTMC)\n",
    "\n",
    "$P(X_{n+1}=j | X_n = i, X_{n-1}, ..., X_1, X_0) = P(X_{n+1}=j|X_n=i)$\n",
    "\n",
    "(Markov property for DTMC)\n",
    "\n",
    "If we have a homogeneous MC, then $P(X_{n+1}=j | X_n =i) = p_{ij} = P(X_1 = j|X_0 = i) = P(X_2=j | X_1 = i) = P(X_3=j|X_2=i) = ...$\n",
    "\n",
    "We can put the $P_{ij}$ in a matrix over the state space $S = \\{ 1, 2, ..., m \\}$ into a transition matrix: \n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "p_{11} & p_{12} & ... & p_{1m}\\\\\n",
    "p_{21} & p_{22} & ... & p_{2m}\\\\\n",
    "\\vdots &&&\\\\\n",
    "p_{m1} & p_{m2} & ... & p_{mm}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To describe completely a DTMC, we need $X_0$ (the initial state) and $P$ (transition probabilities).\n",
    "\n",
    "Example: Two-state DTMC\n",
    "\n",
    "$S = \\{ 1, 2 \\}$  and $P = \\begin{bmatrix} \\alpha&1-\\alpha\\\\ 1-\\beta&\\beta\\\\ \\end{bmatrix}$\n",
    "\n",
    "where $0 < \\alpha$ and $\\beta < 1$\n",
    "\n",
    "$p_{11} = \\alpha = P(X_1=1 | X_0 = 1)$\n",
    "\n",
    "The probability $P(X_1 = j | X_0 = i) = P_{ij} = P(X_2 = j | X_1 = i) = P(X_3 = j | X_2 = i) = ... = P(X_{n+1} = j | X_n = i)$ due to time-homogeneous property (if applies). Also, this $P_{ij}$ is a 1-step transition probability. \n",
    "\n",
    "$P(X_2 = j | X_0 = i) = P_{ij}^{(2)}$ is a 2-step transition probability.\n",
    "\n",
    "$P(X_n = j | X_0 = i) = P_{ij}^{(n)}$ is an n-step transition probability.\n",
    "\n",
    "$$\n",
    "P^{(n)} = \\begin{bmatrix}\n",
    "p_{11}^{(n)} & p_{12}^{(n)} & ... & p_{1m}^{(n)}\\\\\n",
    "\\vdots & & & \\\\\n",
    "p_{m1}^{(n)} & p_{m2}^{(n)} & ... & p_{mm}^{(n)}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "is the n-step transition probability matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is $\\lim_{n\\xrightarrow{} \\infty} P^{(n)}$ (limiting distribution)?\n",
    "\n",
    "For $i, j$, what is $\\lim_{n \\xrightarrow{} \\infty} p_{ij}^{(n)}$, $\\forall i,j$?\n",
    "\n",
    "What is $\\lim_{n \\xrightarrow{} \\infty} P(X_n = j | X_0 = i)$?\n",
    "\n",
    "Under some conditions, the DTMC $\\{ X_n, n\\geq 0 \\}$ has a stationary distribution $\\Pi$. For the 2-state DTMC example, \n",
    "\n",
    "$$\\Pi = (\\pi_1, \\pi_2) = ( P(X_n=1), P(X_n=2) )$$\n",
    "\n",
    "$P(X_n = j), \\forall j \\in S$. So, it will converge to some PMF.\n",
    "\n",
    "Moreover, under some conditions, the stationary distribution $\\Pi$ can also be the limiting distribution. We can use the same idea to construct a stationary distribution $f$ (in continuous case). $X_0$ and kernel transition.\n",
    "\n",
    "In statistical computing, this kernel transition is an algorithm.\n",
    "\n",
    "\n",
    "### Properties\n",
    "\n",
    "A state $i$ is _recurrent_ if a chain starting in $i$ will eventually return to $i$ with probability 1. If probability less than one, then it is _transient_. Also, the state is _positive recurrent_ if the expected time to return is finite.\n",
    "\n",
    "A markov chain is _irreducible_ if there is a positive probability that a chain starting in a state $i$ can reach any state $j$.\n",
    "\n",
    "A markov chain is _aperiodic_ if, for a starting state $i$, there is no constraint on the time at which the chain can return to state $i$. \n",
    "\n",
    "An irreducible, aperiodic Markov chain with all states being positive recurrent is called _ergodic_. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit Theory\n",
    "\n",
    "$f$ is a stationary distribution if $X_0 \\sim f$ up to $X_n \\sim f$ for all $n$. An irreducible and positive recurrent markov chain has at most one stationary distribution. Furthermore, if the chain is ergodic, $\\lim_{n\\xrightarrow{} \\infty} P(X_{m+n} \\in B | X_m \\in A) = \\int_B f(x) dx$ for all $A$, $B$, $m$. Even further, if $\\Omega(x)$ is integrable, then $\\frac{1}{n} \\sum_{t=1}^n \\Omega(X_t) \\xrightarrow{} \\int \\Omega(x) f(x) dx$ with probability 1. This is a version of the famous ergodic theorem. It is effectively the law of large numbers for the markov chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation: Why MCMC?\n",
    "\n",
    "In monte carlo applications, we want to generate random variables with distribution $f$. This could be difficult or impossible to do exactly (for example, if have dependent RVs).\n",
    "\n",
    "The markov chain monte carlo (MCMC) is designed to construct an ergodic monte carlo with $f$ as its stationary distribution. \n",
    "\n",
    "Asymptotically, the chain will resemble samples from $f$. In particular, by the ergodic theorem, expectations wrt $f$ can be approximated by averages. Somewhat surprising is that it is quite easy to construct and simulate a suitable MC, explaining why MCMC methods have become so popular. But, of course, there are practical and theoretical challenges. \n",
    "\n",
    "For instance, how do you choose a starting value? Also, the _. Also, when stop (i.e. when converge)?\n",
    "\n",
    "## Metropolis-Hastings Algorithm (MH)\n",
    "\n",
    "Let $f(x)$ denote the target distribution pdf. Let $q(x|y)$ denote a conditional pdf for $x$ given $Y=y$ (proposal distribution). This (proposal) pdf should be easy to sample from.\n",
    "\n",
    "Given $X_0$, the MH algorithm produces a sequence of RVs as follows:\n",
    "\n",
    "1. Sample $X^\\star_t \\sim q(X|X_{t-1})$\n",
    "\n",
    "2. Compute $R = \\min \\{ 1, \\frac{f(x^\\star_t) q(x_{t0-} | x^\\star_t)}{f(x_{t-1})q(x_t^\\star | x_{t-1})} \\}$\n",
    "\n",
    "3. Set $X_t = X_t^\\star$ with probability $R$; otherwise, $X_t = X_{t-1}$.\n",
    "\n",
    "This is different from accept-reject sampling because there is always a sample which is selected in each iteration (as one difference)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c28e26739430500fec97d508cbac2e5d4a112deb445b412c4e69aa96f605479"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
