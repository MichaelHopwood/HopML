{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Summaries\r\n",
    "\r\n",
    "## ML & DL models\r\n",
    "\r\n",
    "| Category          | Model | Description | When to use | Extending Concepts |\r\n",
    "|-------------------|-------|-------------|-------------|--------------------|\r\n",
    "| Ensemble Learning | Gradient Boosting | Builds tree one at a time! At each stage <br> $m$ ($1 \\leq m \\leq M$, of $M$ total stages) of <br> gradient boosting, suppose some imperfect model <br> $F_{m}$ (for low $m$, this model may simply return <br>$\\hat {y}_{i}=\\bar{y}$. In order to improve $F_m$, our algorithm <br>should add some new estimator, $h_{m}(x)$. Thus, <br> $F_{m+1}(x)=F_{m}(x)+h_{m}(x)=y$. Therefore, <br>$h_{m}(x)=y - F_m (x)$. Gradient boosting will fit $h$ <br>to the residual $y - F_m(x)$, attempting to correct the <br>errors of its predecessor. | Decrease the bias error. Used in classification (MSE)<br> and regression (log-Loss) |  A generalization of this idea to loss functions other than squared <br> error, and to classification and ranking problems, follows from the <br> observation that residuals $h_{m}(x)$ for a given model are the <br>negative gradients of the mean squared error (MSE) loss function <br> (with respect to $F(x)$: $L_{\\rm {MSE}}={\\frac {1}{2}}\\left(y-F(x)\\right)^{2}$ and <br> $h_{m}(x)=-{\\frac {\\partial L_{\\rm {MSE}}}{\\partial F}}=y-F(x)$, meaning the gradient boosting <br> could be specialized to a gradient descent algorithm, and <br> generalizing it entails \"plugging in\" a different loss and its gradient. |\r\n",
    "| Ensemble Learning | XGBoost | A form of gradient boosting. XGBoost delivers high <br>performance as compared to Gradient Boosting. Its <br>training is very fast and can be parallelized / <br> distributed across clusters. XGBoost computes <br> second-order gradients, i.e. second partial <br>derivatives of the loss function, which provides more <br> information about the direction of gradients <br> and how to get to the minimum of our loss <br>function. XGBoost also handles missing values in the<br> dataset. So, in data wrangling, you may or <br>may not do a separate treatment for the missing <br>values, because XGBoost is capable of handling <br> missing values internally. | A good model to try first. |  |\r\n",
    "| Ensemble Learning | Random Forest | RFs train each tree independently, using a <br> random sample of the data. This randomness helps to make <br> the model more robust than a single decision tree, <br> and less likely to overfit on the training data | multi-class because efficient |  |\r\n",
    "| Statistics | Beta regression | | |https://www.ime.usp.br/~sferrari/beta.pdf|\r\n",
    "| Statistics | Naive Bayes | Classifier $\\hat{y} = argmax_{k\\in\\{1,...,K\\}} p(C_k) \\Pi_{i=1}^n p(x_i \\| C_k)$ that often (in Gaussian NB) uses $p(X\\|C_k) \\sim N(\\mu_k, \\sigma_k^2)$. | Per the product in this equation, NB requires feature independent. Data scarcity causes probabilities to be close to zero which can cause numerical instabilities. Additionally, continuous features have to be transformed to discrete, throwing away a lot of information. | |\r\n",
    "| Statistics | Linear Discriminant Analysis| | Does not work if the classes are not balanced. Also, it's not applicable to non-linear problems. Also, sensitive to overfitting. ||"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sampling \r\n",
    "\r\n",
    "| Category          | Technique | Description | When to use | Extending Concepts |\r\n",
    "|-------------------|-----------|-------------|-------------|--------------------|\r\n",
    "\r\n",
    "\r\n",
    "Estimate integrals:\r\n",
    "\r\n",
    "Variational Bayesian http://www.orchid.ac.uk/eprints/40/1/fox_vbtut.pdf\r\n",
    "\r\n",
    "Monte carlo: see Monte Carlo Integration tab"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizers\r\n",
    "\r\n",
    "In stochastic gradient descent, $\\theta_{t-1} = \\theta_t - \\alpha \\delta L(\\theta_t)$, the $\\theta$ are getting changed according to the gradient of the loss wrt $\\theta$. $\\alpha$ is the learning rate. If it is small, convergence will be slow and has potential to get caught at local minima; large $\\alpha$ will lead to divergence. The gradient of the loss $L$ changes quickly after each iteration due to the diversity of each training example. In this base case, it is common to have \"zig-zag\" even though we slowly reach the loss minima.\r\n",
    "\r\n",
    "Momentum can overcome this \"zig-zag\". We introduce a new hyperparameter $\\mu$ where $v_{t+1} = \\mu v_t - \\alpha \\delta L(\\theta_t)$ and $\\theta_{t+1} = \\theta + v_{t+1}$. \r\n",
    "\r\n",
    "| Category          | Technique | Description | When to use | Extending Concepts |\r\n",
    "|-------------------|-----------|-------------|-------------|--------------------|\r\n",
    "|Adaptive learning rate| Adagrad| Scales $\\alpha$ for each parameter according to the history of gradients (previous steps) for that parameter which is basically done by dividing current gradient in updated rule by the sum of previous gradients. As a result, what happens is that when the gradient is very large, alpha is reduced and vice-versa.  $g_{t+1} = g_t + \\delta L(\\theta_t)^2$  and $\\theta_{t+1} = \\theta_t - \\frac{\\alpha \\delta L(\\theta)^2}{\\sqrt{g_{t+1} + \\epsilon}}$|||\r\n",
    "|Adaptive learning rate| RMSProp|$g_t$ term (as seen in Adagrad) is calculated by exponential decaying average and not the sum of gradients. ||https://www.quora.com/What-are-differences-between-update-rules-like-AdaDelta-RMSProp-AdaGrad-and-AdaM|\r\n",
    "|Adaptive learning rate| Adam| Uses both the first order moment $m_t$ and the second order moment $g_t$ but they are both decayed over time. Step size is approximately $\\pm \\alpha$. Step size will decrease as it approaches minimum.|||\r\n",
    "\r\n",
    "There is often value to using more than one method (an ensemble), because every method has a weakness."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "interpreter": {
   "hash": "7c28e26739430500fec97d508cbac2e5d4a112deb445b412c4e69aa96f605479"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}