{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ML & DL model summaries\r\n",
    "\r\n",
    "| Category          | Model | Description | When to use | Extending Concepts |\r\n",
    "|-------------------|-------|-------------|-------------|--------------------|\r\n",
    "| Ensemble Learning | Gradient Boosting | Builds tree one at a time! At each stage <br> $m$ ($1 \\leq m \\leq M$, of $M$ total stages) of <br> gradient boosting, suppose some imperfect model <br> $F_{m}$ (for low $m$, this model may simply return <br>$\\hat {y}_{i}=\\bar{y}$. In order to improve $F_m$, our algorithm <br>should add some new estimator, $h_{m}(x)$. Thus, <br> $F_{m+1}(x)=F_{m}(x)+h_{m}(x)=y$. Therefore, <br>$h_{m}(x)=y - F_m (x)$. Gradient boosting will fit $h$ <br>to the residual $y - F_m(x)$, attempting to correct the <br>errors of its predecessor. | Decrease the bias error. Used in classification (MSE)<br> and regression (log-Loss) |  A generalization of this idea to loss functions other than squared <br> error, and to classification and ranking problems, follows from the <br> observation that residuals $h_{m}(x)$ for a given model are the <br>negative gradients of the mean squared error (MSE) loss function <br> (with respect to $F(x)$: $L_{\\rm {MSE}}={\\frac {1}{2}}\\left(y-F(x)\\right)^{2}$ and <br> $h_{m}(x)=-{\\frac {\\partial L_{\\rm {MSE}}}{\\partial F}}=y-F(x)$, meaning the gradient boosting <br> could be specialized to a gradient descent algorithm, and <br> generalizing it entails \"plugging in\" a different loss and its gradient. |\r\n",
    "| Ensemble Learning | XGBoost | A form of gradient boosting. XGBoost delivers high <br>performance as compared to Gradient Boosting. Its <br>training is very fast and can be parallelized / <br> distributed across clusters. XGBoost computes <br> second-order gradients, i.e. second partial <br>derivatives of the loss function, which provides more <br> information about the direction of gradients <br> and how to get to the minimum of our loss <br>function. XGBoost also handles missing values in the<br> dataset. So, in data wrangling, you may or <br>may not do a separate treatment for the missing <br>values, because XGBoost is capable of handling <br> missing values internally. | A good model to try first. |  |\r\n",
    "| Ensemble Learning | Random Forest | RFs train each tree independently, using a <br> random sample of the data. This randomness helps to make <br> the model more robust than a single decision tree, <br> and less likely to overfit on the training data | multi-class because efficient |  |\r\n",
    "| Statistics | Beta regression | | |https://www.ime.usp.br/~sferrari/beta.pdf|\r\n",
    "| Statistics | Naive Bayes | Generates (usually) linear decision boundary $\\sigma_{\\alpha, c}$ | |https://www.ime.usp.br/~sferrari/beta.pdf|"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "name": "python368jvsc74a57bd07c28e26739430500fec97d508cbac2e5d4a112deb445b412c4e69aa96f605479"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}